{"meta":{"title":"Format's Notes","subtitle":"吃饭睡觉撸代码","description":"吃饭睡觉撸代码","author":"Format","url":"http://fangjian0423.github.io"},"pages":[{"title":"","date":"2015-07-17T03:05:48.000Z","updated":"2017-06-10T08:29:37.000Z","comments":true,"path":"about/index.html","permalink":"http://fangjian0423.github.io/about/index.html","excerpt":"","text":"1.关于我 12345678object Me &#123; def name = \"Format\" def email = \"fangjian0423@gmail.com\" def location = \"Hangzhou\" def wechat_code = \"format_coder\" def hobbies = List(\"Dota\", \"Gundam\", \"Eva\") def todoList = List(\"ukulele\", \"cooking\", \"learning\", \"....\")&#125; 2.其他资料 Github(https://github.com/fangjian0423/) 博客园，以前写博客的地方(http://www.cnblogs.com/fangjian0423/) 3.博客记录 2015-08-04 博客地址正式从博客园迁移到github pages，并复制了部分博客内容到新环境中 2017-05-16 文章数达到了100 2017-06-10 加上了leancloud统计功能","raw":null,"content":null},{"title":"tags","date":"2015-10-29T15:16:32.000Z","updated":"2016-01-17T13:58:00.000Z","comments":true,"path":"tags/index.html","permalink":"http://fangjian0423.github.io/tags/index.html","excerpt":"","text":"","raw":null,"content":null},{"title":"categories","date":"2015-10-29T15:15:59.000Z","updated":"2016-01-17T13:58:15.000Z","comments":true,"path":"categories/index.html","permalink":"http://fangjian0423.github.io/categories/index.html","excerpt":"","text":"","raw":null,"content":null}],"posts":[{"title":"SpringBoot应用程序的关闭","slug":"springboot-application-exit","date":"2017-06-28T01:15:30.000Z","updated":"2017-06-28T02:21:17.000Z","comments":true,"path":"2017/06/28/springboot-application-exit/","link":"","permalink":"http://fangjian0423.github.io/2017/06/28/springboot-application-exit/","excerpt":"SpringBoot应用程序的关闭目前总结起来有4种方式：\n\nRest接口：使用spring-boot-starter-actuator模块里的ShutdownEndpoint\nSpringApplication的exit静态方法：直接调用该静态方法即可\nJMX：使用SpringBoot内部提供的MXBean\n使用第三方进程管理工具\n","text":"SpringBoot应用程序的关闭目前总结起来有4种方式： Rest接口：使用spring-boot-starter-actuator模块里的ShutdownEndpoint SpringApplication的exit静态方法：直接调用该静态方法即可 JMX：使用SpringBoot内部提供的MXBean 使用第三方进程管理工具 Rest接口Rest接口使用Endpoint暴露出来，需要引入spring-boot-starter-actuator这个stater。 这个关闭应用程序对应的Endpoint是ShutdownEndpoint，直接调用ShutdownEndpoint提供的rest接口即可。得先开启ShutdownEndpoint(默认不开启)，以及不进行安全监测： 12endpoints.shutdown.enabled: trueendpoints.shutdown.sensitive: false 然后调用rest接口： 1curl -X POST http://localhost:8080/shutdown 可以使用spring-security进行安全监测： 1234endpoints.shutdown.sensitive: truesecurity.user.name: adminsecurity.user.password: adminmanagement.security.role: SUPERUSER 然后使用用户名和密码进行调用： 1curl -u admin:admin -X POST http://127.0.0.1:8080/shutdown 这个ShutdownEndpoint底层其实就是调用了Spring容器的close方法： 1234567891011121314151617181920212223242526272829@Overridepublic Map&lt;String, Object&gt; invoke() &#123; if (this.context == null) &#123; return Collections.&lt;String, Object&gt;singletonMap(\"message\", \"No context to shutdown.\"); &#125; try &#123; return Collections.&lt;String, Object&gt;singletonMap(\"message\", \"Shutting down, bye...\"); &#125; finally &#123; new Thread(new Runnable() &#123; @Override public void run() &#123; try &#123; Thread.sleep(500L); &#125; catch (InterruptedException ex) &#123; // Swallow exception and continue &#125; ShutdownEndpoint.this.context.close(); &#125; &#125;).start(); &#125;&#125; SpringApplication的exit静态方法SpringApplication提供了一个exit静态方法，用于关闭Spring容器，该方法还有一个参数exitCodeGenerators表示ExitCodeGenerator接口的数组。ExitCodeGenerator接口是一个生成退出码exitCode的生成器。 1234567891011121314151617181920212223242526272829303132333435public static int exit(ApplicationContext context, ExitCodeGenerator... exitCodeGenerators) &#123; Assert.notNull(context, \"Context must not be null\"); int exitCode = 0; // 默认的退出码是0 try &#123; try &#123; // 构造ExitCodeGenerator集合 ExitCodeGenerators generators = new ExitCodeGenerators(); // 获得Spring容器中所有的ExitCodeGenerator类型的bean Collection&lt;ExitCodeGenerator&gt; beans = context .getBeansOfType(ExitCodeGenerator.class).values(); // 集合加上参数中的ExitCodeGenerator数组 generators.addAll(exitCodeGenerators); // 集合加上Spring容器中的ExitCodeGenerator集合 generators.addAll(beans); // 遍历每个ExitCodeGenerator，得到最终的退出码exitCode // 这里每个ExitCodeGenerator生成的退出码如果比0大，那么取最大的 // 如果比0小，那么取最小的 exitCode = generators.getExitCode(); if (exitCode != 0) &#123; // 如果退出码exitCode不为0，发布ExitCodeEvent事件 context.publishEvent(new ExitCodeEvent(context, exitCode)); &#125; &#125; finally &#123; // 关闭Spring容器 close(context); &#125; &#125; catch (Exception ex) &#123; ex.printStackTrace(); exitCode = (exitCode == 0 ? 1 : exitCode); &#125; return exitCode;&#125; 我们写个Controller直接调用exit方法： 123456789@Autowiredprivate ApplicationContext applicationContext;@PostMapping(\"/stop\")public String stop() &#123; // 加上自己的权限验证 SpringApplication.exit(applicationContext); return \"ok\";&#125; JMXspring-boot-starter-actuator这个stater内部会构造ShutdownEndpointMBean。 使用jconsole可以看到这个MBean： SpringBoot内部也提供了一个SpringApplicationAdminMXBean，但是需要开启： 1spring.application.admin.enabled: true 使用第三方进程管理工具比如我们的应用程序部署在linux系统上，可以借助一些第三方的进程管理工具管理应用程序的运行，比如supervisor。 设置program： 12345678[program:stop-application]command=java -jar /yourjar.jarprocess_name=%(program_name)sstartsecs=10autostart=falseautorestart=falsestdout_logfile=/tmp/stop.logstderr_logfile=/tmp/stop-error.log 使用supervisorctl进入控制台操作应用程序： 12345678910supervisor&gt; statusstop-application STOPPED Jun 27 03:50 PMsupervisor&gt; start stop-applicationstop-application: startedsupervisor&gt; statusstop-application RUNNING pid 27918, uptime 0:00:11supervisor&gt; stop stop-applicationstop-application: stoppedsupervisor&gt; statusstop-application STOPPED Jun 27 03:50 PM","raw":null,"content":null,"categories":[{"name":"springboot","slug":"springboot","permalink":"http://fangjian0423.github.io/categories/springboot/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://fangjian0423.github.io/tags/spring/"},{"name":"java","slug":"java","permalink":"http://fangjian0423.github.io/tags/java/"},{"name":"springboot","slug":"springboot","permalink":"http://fangjian0423.github.io/tags/springboot/"}]},{"title":"Spring内置的BeanPostProcessor总结","slug":"spring-embedded-bean-post-processor","date":"2017-06-23T16:57:01.000Z","updated":"2017-06-23T17:23:46.000Z","comments":true,"path":"2017/06/24/spring-embedded-bean-post-processor/","link":"","permalink":"http://fangjian0423.github.io/2017/06/24/spring-embedded-bean-post-processor/","excerpt":"Spring内置了一些很有用的BeanPostProcessor接口实现类。比如有AutowiredAnnotationBeanPostProcessor、RequiredAnnotationBeanPostProcessor、CommonAnnotationBeanPostProcessor、EventListenerMethodProcessor等。这些Processor会处理各自的场景。\n正是有了这些processor，把bean的构造过程中的一部分功能分配给了这些processor处理，减轻了BeanFactory的负担。\n而且添加一些新的功能也很方便。比如Spring Scheduling模块，只需要添加个@EnableScheduling注解，然后加个@Scheduled注解修饰的方法即可，这个Processor内部会自行处理。","text":"Spring内置了一些很有用的BeanPostProcessor接口实现类。比如有AutowiredAnnotationBeanPostProcessor、RequiredAnnotationBeanPostProcessor、CommonAnnotationBeanPostProcessor、EventListenerMethodProcessor等。这些Processor会处理各自的场景。 正是有了这些processor，把bean的构造过程中的一部分功能分配给了这些processor处理，减轻了BeanFactory的负担。 而且添加一些新的功能也很方便。比如Spring Scheduling模块，只需要添加个@EnableScheduling注解，然后加个@Scheduled注解修饰的方法即可，这个Processor内部会自行处理。 ApplicationContextAwareProcessorApplicationContextAwareProcessor实现BeanPostProcessor接口。 Spring容器的refresh方法内部调用prepareBeanFactory方法，prepareBeanFactory方法会添加ApplicationContextAwareProcessor到BeanFactory中。这个Processor的作用在于为实现*Aware接口的bean调用该Aware接口定义的方法，并传入对应的参数。比如实现EnvironmentAware接口的bean在该Processor内部会调用EnvironmentAware接口的setEnvironment方法，并把Spring容器内部的ConfigurableEnvironment传递进去。 具体的代码： 1234567891011121314151617181920212223242526// AbstractApplicationContext.classbeanFactory.addBeanPostProcessor(new ApplicationContextAwareProcessor(this));// ApplicationContextAwareProcessor.classprivate void invokeAwareInterfaces(Object bean) &#123; if (bean instanceof Aware) &#123; if (bean instanceof EnvironmentAware) &#123; ((EnvironmentAware) bean).setEnvironment(this.applicationContext.getEnvironment()); &#125; if (bean instanceof EmbeddedValueResolverAware) &#123; ((EmbeddedValueResolverAware) bean).setEmbeddedValueResolver( new EmbeddedValueResolver(this.applicationContext.getBeanFactory())); &#125; if (bean instanceof ResourceLoaderAware) &#123; ((ResourceLoaderAware) bean).setResourceLoader(this.applicationContext); &#125; if (bean instanceof ApplicationEventPublisherAware) &#123; ((ApplicationEventPublisherAware) bean).setApplicationEventPublisher(this.applicationContext); &#125; if (bean instanceof MessageSourceAware) &#123; ((MessageSourceAware) bean).setMessageSource(this.applicationContext); &#125; if (bean instanceof ApplicationContextAware) &#123; ((ApplicationContextAware) bean).setApplicationContext(this.applicationContext); &#125; &#125;&#125; CommonAnnotationBeanPostProcessor在AnnotationConfigUtils类的registerAnnotationConfigProcessors方法中被封装成RootBeanDefinition并注册到Spring容器中。registerAnnotationConfigProcessors方法在一些比如扫描类的场景下注册。比如 context:component-scan 标签或 context:annotation-config 标签的使用，或ClassPathBeanDefinitionScanner扫描器的使用、AnnotatedBeanDefinitionReader读取器的使用。 主要处理@Resource、@PostConstruct和@PreDestroy注解的实现。 在postProcessPropertyValues过程中，该processor会找出bean中被@Resource注解修饰的属性(Field)和方法(Method)，找出以后注入到bean中。 123456789101112131415// CommonAnnotationBeanPostProcessor.class@Overridepublic PropertyValues postProcessPropertyValues( PropertyValues pvs, PropertyDescriptor[] pds, Object bean, String beanName) throws BeansException &#123; // 找出bean中被@Resource注解修饰的属性(Field)和方法(Method) InjectionMetadata metadata = findResourceMetadata(beanName, bean.getClass(), pvs); try &#123; // 注入到bean中 metadata.inject(bean, beanName, pvs); &#125; catch (Throwable ex) &#123; throw new BeanCreationException(beanName, \"Injection of resource dependencies failed\", ex); &#125; return pvs;&#125; CommonAnnotationBeanPostProcessor的父类InitDestroyAnnotationBeanPostProcessor类的postProcessMergedBeanDefinition过程会找出被@PostConstruct和@PreDestroy注解修饰的方法。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748// InitDestroyAnnotationBeanPostProcessor.class@Overridepublic void postProcessMergedBeanDefinition(RootBeanDefinition beanDefinition, Class&lt;?&gt; beanType, String beanName) &#123; if (beanType != null) &#123; // 找出被@PostConstruct和@PreDestroy注解修饰的方法 LifecycleMetadata metadata = findLifecycleMetadata(beanType); metadata.checkConfigMembers(beanDefinition); &#125;&#125;@Overridepublic Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException &#123; LifecycleMetadata metadata = findLifecycleMetadata(bean.getClass()); try &#123; // postProcessBeforeInitialization在实例初始化之前调用 // 这里调用了被@PostConstruct注解修饰的方法 metadata.invokeInitMethods(bean, beanName); &#125; catch (InvocationTargetException ex) &#123; throw new BeanCreationException(beanName, \"Invocation of init method failed\", ex.getTargetException()); &#125; catch (Throwable ex) &#123; throw new BeanCreationException(beanName, \"Couldn't invoke init method\", ex); &#125; return bean;&#125;@Overridepublic void postProcessBeforeDestruction(Object bean, String beanName) throws BeansException &#123; LifecycleMetadata metadata = findLifecycleMetadata(bean.getClass()); try &#123; // postProcessBeforeDestruction在实例销毁之前调用 // 这里调用了被@PreDestroy注解修饰的方法 metadata.invokeDestroyMethods(bean, beanName); &#125; catch (InvocationTargetException ex) &#123; String msg = \"Invocation of destroy method failed on bean with name '\" + beanName + \"'\"; if (logger.isDebugEnabled()) &#123; logger.warn(msg, ex.getTargetException()); &#125; else &#123; logger.warn(msg + \": \" + ex.getTargetException()); &#125; &#125; catch (Throwable ex) &#123; logger.error(\"Couldn't invoke destroy method on bean with name '\" + beanName + \"'\", ex); &#125;&#125; AutowiredAnnotationBeanPostProcessor跟CommonAnnotationBeanPostProcessor一样，在AnnotationConfigUtils类的registerAnnotationConfigProcessors方法被注册到Spring容器中。 主要处理@Autowired、@Value、@Lookup和@Inject注解的实现，处理逻辑跟CommonAnnotationBeanPostProcessor类似。 123456789101112131415// AutowiredAnnotationBeanPostProcessor.class@Overridepublic PropertyValues postProcessPropertyValues( PropertyValues pvs, PropertyDescriptor[] pds, Object bean, String beanName) throws BeansException &#123; // 找出被@Autowired、@Value以及@Inject注解修饰的属性和方法 InjectionMetadata metadata = findAutowiringMetadata(beanName, bean.getClass(), pvs); try &#123; // 注入到bean中 metadata.inject(bean, beanName, pvs); &#125; catch (Throwable ex) &#123; throw new BeanCreationException(beanName, \"Injection of autowired dependencies failed\", ex); &#125; return pvs;&#125; 由于@Autowired注解可以在构造器中使用，所以AutowiredAnnotationBeanPostProcessor实现了determineCandidateConstructors方法： 1234567891011121314151617181920212223@Overridepublic Constructor&lt;?&gt;[] determineCandidateConstructors(Class&lt;?&gt; beanClass, final String beanName) throws BeansException &#123; ... for (Constructor&lt;?&gt; candidate : rawCandidates) &#123; // 遍历所有的构造器 // 找出被@Autowired注解修饰的构造器 AnnotationAttributes ann = findAutowiredAnnotation(candidate); if (ann != null) &#123; ... candidates.add(candidate); &#125; else if (candidate.getParameterTypes().length == 0) &#123; defaultConstructor = candidate; &#125; &#125; if (!candidates.isEmpty()) &#123; // 有找到的话使用这些构造器 ... candidateConstructors = candidates.toArray(new Constructor&lt;?&gt;[candidates.size()]); &#125; else &#123; // 否则使用默认的构造器 candidateConstructors = new Constructor&lt;?&gt;[0]; &#125; ...&#125; RequiredAnnotationBeanPostProcessor跟CommonAnnotationBeanPostProcessor一样，在AnnotationConfigUtils类的registerAnnotationConfigProcessors方法被注册到Spring容器中。 主要处理@Required注解的实现(@Required注解只能修饰方法)，在postProcessPropertyValues过程中处理： 123456789101112131415161718192021@Overridepublic PropertyValues postProcessPropertyValues( PropertyValues pvs, PropertyDescriptor[] pds, Object bean, String beanName) throws BeansException &#123; if (!this.validatedBeanNames.contains(beanName)) &#123; // 查看是否已经验证过 if (!shouldSkip(this.beanFactory, beanName)) &#123; // 查看该bean是否不会被skip，如果在BeanDefinition中有个org.springframework.beans.factory.annotation.RequiredAnnotationBeanPostProcessor.skipRequiredCheck属性，且值为true。那么这里会skip，不做required验证 List&lt;String&gt; invalidProperties = new ArrayList&lt;String&gt;(); for (PropertyDescriptor pd : pds) &#123; if (isRequiredProperty(pd) &amp;&amp; !pvs.contains(pd.getName())) &#123; // 如果属性对应的set方法被@Required注解修饰并且该属性没有被设置值的话，添加到invalidProperties集合中 invalidProperties.add(pd.getName()); &#125; &#125; if (!invalidProperties.isEmpty()) &#123; // 如果存在被@Required注解修饰的方法对应的属性，抛出BeanInitializationException异常 throw new BeanInitializationException(buildExceptionMessage(invalidProperties, beanName)); &#125; &#125; this.validatedBeanNames.add(beanName); &#125; return pvs;&#125; BeanValidationPostProcessor默认不添加，需要手动添加。主要提供对JSR-303验证的支持，内部有个boolean类型的属性afterInitialization，默认是false。如果是false，在postProcessBeforeInitialization过程中对bean进行验证，否则在postProcessAfterInitialization过程对bean进行验证。 12345// 手动注册BeanValidationPostProcessor@Beanpublic BeanPostProcessor beanValidationPostProcessor() &#123; return new BeanValidationPostProcessor();&#125; 定义一个使用JSR-303的bean： 1234567@Componentpublic class BeanForBeanValidation &#123; @NotNull private String id; @Min(value = 10) private int age;&#125; 最后实例化BeanForBeanValidation的时候，BeanValidationPostProcessor起作用，在postProcessBeforeInitialization过程中发现validate不通过，抛出异常： Caused by: org.springframework.beans.factory.BeanInitializationException: Bean state is invalid: age - 最小不能小于10; id - 不能为null AbstractAutoProxyCreator这是一个抽象类，实现了SmartInstantiationAwareBeanPostProcessor接口。主要用于aop在Spring中的应用。 1234567891011121314151617181920212223242526272829303132333435363738394041@Overridepublic Object postProcessBeforeInstantiation(Class&lt;?&gt; beanClass, String beanName) throws BeansException &#123; // 生成缓存key // AbstractAutoProxyCreator内部有个Map用于存储代理类的缓存信息 Object cacheKey = getCacheKey(beanClass, beanName); // targetSourcedBeans是个String集合，如果这个bean被内部的TargetSourceCreator数组属性处理过，那么targetSourcedBeans就会存储这个bean的beanName // 如果targetSourcedBeans内部没有包括当前beanName if (beanName == null || !this.targetSourcedBeans.contains(beanName)) &#123; // advisedBeans属性是个Map&lt;Object, Boolean&gt;类型的map，key为cacheKey，value是个Boolean，如果是true，说明这个bean已经被wrap成代理类，否则还是原先的bean // 这里判断cacheKey是否已经被wrap成代理类，如果没有，返回null，走Spring默认的构造bean流程 if (this.advisedBeans.containsKey(cacheKey)) &#123; return null; &#125; // isInfrastructureClass方法判断该bean是否是aop相关的bean，比如Advice、Advisor、AopInfrastructureBean // shouldSkip方法默认返回false，子类可覆盖。比如AspectJAwareAdvisorAutoProxyCreator子类进行了覆盖，它内部会找出Spring容器中Advisor类型的bean，然后进行遍历判断处理的bean是否是这个Advisor，如果是则过滤 if (isInfrastructureClass(beanClass) || shouldSkip(beanClass, beanName)) &#123; this.advisedBeans.put(cacheKey, Boolean.FALSE); return null; &#125; &#125; if (beanName != null) &#123; // 遍历内部的TargetSourceCreator数组属性，根据bean信息得到TargetSource // 默认情况下TargetSourceCreator数组属性是空的 TargetSource targetSource = getCustomTargetSource(beanClass, beanName); if (targetSource != null) &#123; // 添加beanName到targetSourcedBeans中，证明这个bean被自定义的TargetSourceCreator处理过 this.targetSourcedBeans.add(beanName); // 得到Advice Object[] specificInterceptors = getAdvicesAndAdvisorsForBean(beanClass, beanName, targetSource); // 创建代理 Object proxy = createProxy(beanClass, beanName, specificInterceptors, targetSource); // 添加到proxyTypes属性中 this.proxyTypes.put(cacheKey, proxy.getClass()); // 返回这个代理类，这样后续对该bean便不再处理，除了postProcessAfterInitialization过程 return proxy; &#125; &#125; return null;&#125; 从这个postProcessBeforeInstantiation方法我们得出：如果使用了自定义的TargetSourceCreator，并且这个TargetSourceCreator得到了处理bean的TargetSource结果，那么直接基于这个bean和TargetSource结果构造出代理类。这个过程发生在postProcessBeforeInstantiation方法中，所以这个代理类直接代替了原本该生成的bean。如果没有使用自定义的TargetSourceCreator，那么走默认构造bean的流程。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253@Overridepublic Object getEarlyBeanReference(Object bean, String beanName) throws BeansException &#123; // 生成缓存key Object cacheKey = getCacheKey(bean.getClass(), beanName); // earlyProxyReferences用来存储提前暴露的代理对象的缓存key，这里判断是否已经处理过，没处理过的话放到earlyProxyReferences里 if (!this.earlyProxyReferences.contains(cacheKey)) &#123; this.earlyProxyReferences.add(cacheKey); &#125; return wrapIfNecessary(bean, beanName, cacheKey);&#125;protected Object wrapIfNecessary(Object bean, String beanName, Object cacheKey) &#123; // 如果已经使用了自定义的TargetSourceCreator生成了代理类，直接返回这个代理类 if (beanName != null &amp;&amp; this.targetSourcedBeans.contains(beanName)) &#123; return bean; &#125; // 该bean已经没有被wrap成代理类，直接返回原本生成的实例 if (Boolean.FALSE.equals(this.advisedBeans.get(cacheKey))) &#123; return bean; &#125; // 如果是处理aop自身相关的bean或者这些bean需要被skip，也直接返回这些bean if (isInfrastructureClass(bean.getClass()) || shouldSkip(bean.getClass(), beanName)) &#123; this.advisedBeans.put(cacheKey, Boolean.FALSE); return bean; &#125; // 得到Advice Object[] specificInterceptors = getAdvicesAndAdvisorsForBean(bean.getClass(), beanName, null); if (specificInterceptors != DO_NOT_PROXY) &#123; // 如果被aop处理了 // 添加到advisedBeans属性中，说明该bean已经被wrap成代理类 this.advisedBeans.put(cacheKey, Boolean.TRUE); // 创建代理类 Object proxy = createProxy( bean.getClass(), beanName, specificInterceptors, new SingletonTargetSource(bean)); // 添加到proxyTypes属性中 this.proxyTypes.put(cacheKey, proxy.getClass()); return proxy; &#125; // 如果没有被aop处理，添加到advisedBeans属性中，并说明不是代理类 this.advisedBeans.put(cacheKey, Boolean.FALSE); return bean;&#125;@Overridepublic Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException &#123; if (bean != null) &#123; Object cacheKey = getCacheKey(bean.getClass(), beanName); if (!this.earlyProxyReferences.contains(cacheKey)) &#123; return wrapIfNecessary(bean, beanName, cacheKey); &#125; &#125; return bean;&#125; 从上面这些方法看出，要实例化的bean会通过wrapIfNecessary进行处理，wrapIfNecessary方法会根据情况是否wrap成代理类，最终返回这个结果。getEarlyBeanReference和postProcessAfterInitialization处理流程是一样的，唯一的区别是getEarlyBeanReference是针对单例的，而postProcessAfterInitialization方法是针对prototype的，针对prototype的话，每次实例化都会wrap成代理对象，而单例的话只需要wrap一次即可。 AbstractAutoProxyCreator抽象类有基于注解的子类AnnotationAwareAspectJAutoProxyCreator。这个AnnotationAwareAspectJAutoProxyCreator会扫描出Spring容器中带有@Aspect注解的bean，然后在getAdvicesAndAdvisorsForBean方法中会根据这个aspect查看是否被拦截，如果被拦截那么就wrap成代理类。 默认情况下，AbstractAutoProxyCreator相关的BeanPostProcessor是不会注册到Spring容器中的。比如在SpringBoot中加入aop-starter之后，会触发AopAutoConfiguration自动化配置，然后将AnnotationAwareAspectJAutoProxyCreator注册到Spring容器中。 MethodValidationPostProcessor默认不添加，需要手动添加。支持方法级别的JSR-303规范。需要在类上加上@Validated注解，以及在方法的参数中加上验证注解，比如@Max，@Min，@NotEmpty …。 下面这个BeanForMethodValidation就加上了@Validated注解，并且在方法validate的参数里加上的JSR-303的验证注解。 1234567@Component@Validatedpublic class BeanForMethodValidation &#123; public void validate(@NotEmpty String name, @Min(10) int age) &#123; System.out.println(\"validate, name: \" + name + \", age: \" + age); &#125;&#125; MethodValidationPostProcessor内部使用aop完成对方法的调用。 123456789101112// MethodValidationPostProcessor.class@Overridepublic void afterPropertiesSet() &#123; // 基于validatedAnnotationType属性构造出Pointcut，这个validatedAnnotationType属性默认是@Validated注解类型，可以进行修改 Pointcut pointcut = new AnnotationMatchingPointcut(this.validatedAnnotationType, true); // 基于Pointcut和Advice构造出Advisor this.advisor = new DefaultPointcutAdvisor(pointcut, createMethodValidationAdvice(this.validator));&#125;// MethodValidationInterceptor这个Advice内部使用JSR完成方法参数的验证protected Advice createMethodValidationAdvice(Validator validator) &#123; return (validator != null ? new MethodValidationInterceptor(validator) : new MethodValidationInterceptor());&#125; ScheduledAnnotationBeanPostProcessor默认不添加，使用@EnableScheduling注解后，会被注册到Spring容器中。主要使用Spring Scheduling功能对bean中使用了@Scheduled注解的方法进行调度处理。实现了BeanPostProcessor接口。 1234567891011121314151617181920212223242526272829303132333435363738394041@Overridepublic Object postProcessAfterInitialization(final Object bean, String beanName) &#123; // 判断是否是代理类，如果是代理类，拿到真正的目标类 Class&lt;?&gt; targetClass = AopUtils.getTargetClass(bean); // 判断是否已经处理过。nonAnnotatedClasses属性是个Class集合，用于存储bean对应的class是否有@Scheduled注解的方法，如果没有，则添加到这个集合中 if (!this.nonAnnotatedClasses.contains(targetClass)) &#123; // 找出class中带有@Scheduled注解的方法 Map&lt;Method, Set&lt;Scheduled&gt;&gt; annotatedMethods = MethodIntrospector.selectMethods(targetClass, new MethodIntrospector.MetadataLookup&lt;Set&lt;Scheduled&gt;&gt;() &#123; @Override public Set&lt;Scheduled&gt; inspect(Method method) &#123; Set&lt;Scheduled&gt; scheduledMethods = AnnotationUtils.getRepeatableAnnotations(method, Scheduled.class, Schedules.class); return (!scheduledMethods.isEmpty() ? scheduledMethods : null); &#125; &#125;); // 如果不存在@Scheduled注解的方法 if (annotatedMethods.isEmpty()) &#123; // 添加到nonAnnotatedClasses集合中。下次不用重复处理该类 this.nonAnnotatedClasses.add(targetClass); if (logger.isTraceEnabled()) &#123; logger.trace(\"No @Scheduled annotations found on bean class: \" + bean.getClass()); &#125; &#125; else &#123; // 如果存在@Scheduled注解的方法 // 遍历这些@Scheduled注解的方法 for (Map.Entry&lt;Method, Set&lt;Scheduled&gt;&gt; entry : annotatedMethods.entrySet()) &#123; Method method = entry.getKey(); for (Scheduled scheduled : entry.getValue()) &#123; // 进行调度处理 processScheduled(scheduled, method, bean); &#125; &#125; if (logger.isDebugEnabled()) &#123; logger.debug(annotatedMethods.size() + \" @Scheduled methods processed on bean '\" + beanName + \"': \" + annotatedMethods); &#125; &#125; &#125; return bean;&#125; AsyncAnnotationBeanPostProcessor默认不添加，使用@EnableAsync注解后，会被注册到Spring容器中。AsyncAnnotationBeanPostProcessor内部使用aop处理方法的调用。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950// AsyncAnnotationBeanPostProcessor.class// 实现了BeanFactoryAware接口，这里会得到beanFactory@Overridepublic void setBeanFactory(BeanFactory beanFactory) &#123; super.setBeanFactory(beanFactory); // 构造一个AsyncAnnotationAdvisor // AsyncAnnotationAdvisor内部的Advice是AnnotationAsyncExecutionInterceptor，Pointcut会找出带有@Async的类和@Async的方法 AsyncAnnotationAdvisor advisor = new AsyncAnnotationAdvisor(this.executor, this.exceptionHandler); if (this.asyncAnnotationType != null) &#123; advisor.setAsyncAnnotationType(this.asyncAnnotationType); &#125; advisor.setBeanFactory(beanFactory); this.advisor = advisor;&#125;// AsyncExecutionInterceptor.class。 AnnotationAsyncExecutionInterceptor的父类@Overridepublic Object invoke(final MethodInvocation invocation) throws Throwable &#123; // 得到方法的对应类 Class&lt;?&gt; targetClass = (invocation.getThis() != null ? AopUtils.getTargetClass(invocation.getThis()) : null); // 得到方法 Method specificMethod = ClassUtils.getMostSpecificMethod(invocation.getMethod(), targetClass); final Method userDeclaredMethod = BridgeMethodResolver.findBridgedMethod(specificMethod); // 得到Executor线程池。如果没有在Spring容器中找到TaskExecutor类型的线程池，直接构造一个SimpleAsyncTaskExecutor AsyncTaskExecutor executor = determineAsyncExecutor(userDeclaredMethod); if (executor == null) &#123; throw new IllegalStateException( \"No executor specified and no default executor set on AsyncExecutionInterceptor either\"); &#125; // 把方法在调用封装到Callable中 Callable&lt;Object&gt; task = new Callable&lt;Object&gt;() &#123; @Override public Object call() throws Exception &#123; try &#123; Object result = invocation.proceed(); if (result instanceof Future) &#123; return ((Future&lt;?&gt;) result).get(); &#125; &#125; catch (ExecutionException ex) &#123; handleError(ex.getCause(), userDeclaredMethod, invocation.getArguments()); &#125; catch (Throwable ex) &#123; handleError(ex, userDeclaredMethod, invocation.getArguments()); &#125; return null; &#125; &#125;; // 提交任务 return doSubmit(task, executor, invocation.getMethod().getReturnType());&#125; ServletContextAwareProcessor默认不添加，如果Spring容器是个Web容器，那么会被添加。比如GenericWebApplicationContext容器就在postProcessBeanFactory中添加了ServletContextAwareProcessor。postProcessBeanFactory方法是在Spring容器的refresh过程中被调用的。 ServletContextAwareProcessor实现了BeanPostProcessor接口，如果Spring容器中的bean实现了ServletContextAware或ServletConfigAware接口，那么会进行处理。 12345678910@Overridepublic Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException &#123; if (getServletContext() != null &amp;&amp; bean instanceof ServletContextAware) &#123; ((ServletContextAware) bean).setServletContext(getServletContext()); &#125; if (getServletConfig() != null &amp;&amp; bean instanceof ServletConfigAware) &#123; ((ServletConfigAware) bean).setServletConfig(getServletConfig()); &#125; return bean;&#125; SpringBoot内部特有的BeanPostProcessorEmbeddedServletContainerCustomizerBeanPostProcessor主要处理实现EmbeddedServletContainerCustomizer接口的bean。EmbeddedServletContainerCustomizer接口是SpringBoot提供的用于处理内置的Servlet容器的接口： 123public interface EmbeddedServletContainerCustomizer &#123; void customize(ConfigurableEmbeddedServletContainer container);&#125; 这个EmbeddedServletContainerCustomizerBeanPostProcessor实现了BeanPostProcessor接口，处理过程： 1234567891011121314151617181920212223242526272829@Overridepublic Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException &#123; // 处理ConfigurableEmbeddedServletContainer类型的bean if (bean instanceof ConfigurableEmbeddedServletContainer) &#123; postProcessBeforeInitialization((ConfigurableEmbeddedServletContainer) bean); &#125; return bean;&#125;private void postProcessBeforeInitialization( ConfigurableEmbeddedServletContainer bean) &#123; // 找出Spring容器中EmbeddedServletContainerCustomizer接口的实现类，并遍历依次调用 for (EmbeddedServletContainerCustomizer customizer : getCustomizers()) &#123; customizer.customize(bean); &#125;&#125;private Collection&lt;EmbeddedServletContainerCustomizer&gt; getCustomizers() &#123; if (this.customizers == null) &#123; // Look up does not include the parent context this.customizers = new ArrayList&lt;EmbeddedServletContainerCustomizer&gt;( this.applicationContext .getBeansOfType(EmbeddedServletContainerCustomizer.class, false, false) .values()); Collections.sort(this.customizers, AnnotationAwareOrderComparator.INSTANCE); this.customizers = Collections.unmodifiableList(this.customizers); &#125; return this.customizers;&#125; SpringBoot内部EmbeddedServletContainerCustomizer接口的实现类有ServerProperties、ErrorMvcAutoConfiguration的内部类ErrorPageCustomizer等。 我们也可以实现自定义的EmbeddedServletContainerCustomizer用于修改内置Servlet容器的属性。 SpringBoot内部还有一些比如ConfigurationPropertiesBindingPostProcessor用于处于@ConfigurationProperties注解的processor、DataSourceInitializedPublisher用于发布DataSourceInitializedEvent事件等。读者可查看相关源码。","raw":null,"content":null,"categories":[{"name":"springboot","slug":"springboot","permalink":"http://fangjian0423.github.io/categories/springboot/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://fangjian0423.github.io/tags/spring/"},{"name":"java","slug":"java","permalink":"http://fangjian0423.github.io/tags/java/"},{"name":"springboot","slug":"springboot","permalink":"http://fangjian0423.github.io/tags/springboot/"}]},{"title":"Spring内部的BeanPostProcessor接口总结","slug":"spring-bean-post-processor","date":"2017-06-20T11:30:30.000Z","updated":"2017-06-22T16:54:53.000Z","comments":true,"path":"2017/06/20/spring-bean-post-processor/","link":"","permalink":"http://fangjian0423.github.io/2017/06/20/spring-bean-post-processor/","excerpt":"Spring内部提供了一个BeanPostProcessor接口，这个接口的作用在于对于新构造的实例可以做一些自定义的修改。比如如何构造、属性值的修改、构造器的选择等等。\n只要我们实现了这个接口，便可以对构造的bean进行自定义的修改。","text":"Spring内部提供了一个BeanPostProcessor接口，这个接口的作用在于对于新构造的实例可以做一些自定义的修改。比如如何构造、属性值的修改、构造器的选择等等。 只要我们实现了这个接口，便可以对构造的bean进行自定义的修改。 BeanPostProcessor接口还有一些子接口的定义： InstantiationAwareBeanPostProcessorInstantiationAwareBeanPostProcessor接口继承自BeanPostProcessor接口。多出了3个方法： 12345678910111213141516// postProcessBeforeInstantiation方法的作用在目标对象被实例化之前调用的方法，可以返回目标实例的一个代理用来代替目标实例// beanClass参数表示目标对象的类型，beanName是目标实例在Spring容器中的name// 返回值类型是Object，如果返回的是非null对象，接下来除了postProcessAfterInitialization方法会被执行以外，其它bean构造的那些方法都不再执行。否则那些过程以及postProcessAfterInitialization方法都会执行Object postProcessBeforeInstantiation(Class&lt;?&gt; beanClass, String beanName) throws BeansException;// postProcessAfterInstantiation方法的作用在目标对象被实例化之后并且在属性值被populate之前调用// bean参数是目标实例(这个时候目标对象已经被实例化但是该实例的属性还没有被设置)，beanName是目标实例在Spring容器中的name// 返回值是boolean类型，如果返回true，目标实例内部的返回值会被populate，否则populate这个过程会被忽视boolean postProcessAfterInstantiation(Object bean, String beanName) throws BeansException;// postProcessPropertyValues方法的作用在属性中被设置到目标实例之前调用，可以修改属性的设置// pvs参数表示参数属性值(从BeanDefinition中获取)，pds代表参数的描述信息(比如参数名，类型等描述信息)，bean参数是目标实例，beanName是目标实例在Spring容器中的name// 返回值是PropertyValues，可以使用一个全新的PropertyValues代替原先的PropertyValues用来覆盖属性设置或者直接在参数pvs上修改。如果返回值是null，那么会忽略属性设置这个过程(所有属性不论使用什么注解，最后都是null)PropertyValues postProcessPropertyValues( PropertyValues pvs, PropertyDescriptor[] pds, Object bean, String beanName) throws BeansException; 总结： InstantiationAwareBeanPostProcessor接口继承BeanPostProcessor接口，它内部提供了3个方法，再加上BeanPostProcessor接口内部的2个方法，所以实现这个接口需要实现5个方法。InstantiationAwareBeanPostProcessor接口的主要作用在于目标对象的实例化过程中需要处理的事情，包括实例化对象的前后过程以及实例的属性设置 postProcessBeforeInstantiation方法是最先执行的方法，它在目标对象实例化之前调用，该方法的返回值类型是Object，我们可以返回任何类型的值。由于这个时候目标对象还未实例化，所以这个返回值可以用来代替原本该生成的目标对象的实例(比如代理对象)。如果该方法的返回值代替原本该生成的目标对象，后续只有postProcessAfterInitialization方法会调用，其它方法不再调用；否则按照正常的流程走 postProcessAfterInstantiation方法在目标对象实例化之后调用，这个时候对象已经被实例化，但是该实例的属性还未被设置，都是null。如果该方法返回false，会忽略属性值的设置；如果返回true，会按照正常流程设置属性值 postProcessPropertyValues方法对属性值进行修改(这个时候属性值还未被设置，但是我们可以修改原本该设置进去的属性值)。如果postProcessAfterInstantiation方法返回false，该方法不会被调用。可以在该方法内对属性值进行修改 父接口BeanPostProcessor的2个方法postProcessBeforeInitialization和postProcessAfterInitialization都是在目标对象被实例化之后，并且属性也被设置之后调用的 Instantiation表示实例化，Initialization表示初始化。实例化的意思在对象还未生成，初始化的意思在对象已经生成 SmartInstantiationAwareBeanPostProcessorSmartInstantiationAwareBeanPostProcessor接口继承InstantiationAwareBeanPostProcessor接口。多出了3个方法： 1234567891011// 预测Bean的类型，返回第一个预测成功的Class类型，如果不能预测返回nullClass&lt;?&gt; predictBeanType(Class&lt;?&gt; beanClass, String beanName) throws BeansException;// 选择合适的构造器，比如目标对象有多个构造器，在这里可以进行一些定制化，选择合适的构造器// beanClass参数表示目标实例的类型，beanName是目标实例在Spring容器中的name// 返回值是个构造器数组，如果返回null，会执行下一个PostProcessor的determineCandidateConstructors方法；否则选取该PostProcessor选择的构造器Constructor&lt;?&gt;[] determineCandidateConstructors(Class&lt;?&gt; beanClass, String beanName) throws BeansException;// 获得提前暴露的bean引用。主要用于解决循环引用的问题// 只有单例对象才会调用此方法Object getEarlyBeanReference(Object bean, String beanName) throws BeansException; 总结： SmartInstantiationAwareBeanPostProcessor接口继承InstantiationAwareBeanPostProcessor接口，它内部提供了3个方法，再加上父接口的5个方法，所以实现这个接口需要实现8个方法。SmartInstantiationAwareBeanPostProcessor接口的主要作用也是在于目标对象的实例化过程中需要处理的事情。它是InstantiationAwareBeanPostProcessor接口的一个扩展。主要在Spring框架内部使用 predictBeanType方法用于预测Bean的类型，返回第一个预测成功的Class类型，如果不能预测返回null。主要在于BeanDefinition无法确定Bean类型的时候调用该方法来确定类型 determineCandidateConstructors方法用于选择合适的构造器，比如类有多个构造器，可以实现这个方法选择合适的构造器并用于实例化对象。该方法在postProcessBeforeInstantiation方法和postProcessAfterInstantiation方法之间调用，如果postProcessBeforeInstantiation方法返回了一个新的实例代替了原本该生成的实例，那么该方法会被忽略 getEarlyBeanReference主要用于解决循环引用问题。比如ReferenceA实例内部有ReferenceB的引用，ReferenceB实例内部有ReferenceA的引用。首先先实例化ReferenceA，实例化完成之后提前把这个bean暴露在ObjectFactory中，然后populate属性，这个时候发现需要ReferenceB。然后去实例化ReferenceB，在实例化ReferenceB的时候它需要ReferenceA的实例才能继续，这个时候就会去ObjectFactory中找出了ReferenceA实例，ReferenceB顺利实例化。ReferenceB实例化之后，ReferenceA的populate属性过程也成功完成，注入了ReferenceB实例。提前把这个bean暴露在ObjectFactory中，这个ObjectFactory获取的实例就是通过getEarlyBeanReference方法得到的 BeanPostProcessorBeanPostProcessor接口是最顶层的接口，接口定义： 123456public interface BeanPostProcessor &#123; // 初始化之前的操作 Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException; // 初始化之后的操作 Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException;&#125; 总结： postProcessBeforeInitialization是指bean在初始化之前需要调用的方法 postProcessAfterInitialization是指bean在初始化之后需要调用的方法 postProcessBeforeInitialization和postProcessAfterInitialization方法被调用的时候。这个时候bean已经被实例化，并且所有该注入的属性都已经被注入，是一个完整的bean 这2个方法的返回值可以是原先生成的实例bean，或者使用wrapper包装这个实例 DestructionAwareBeanPostProcessorDestructionAwareBeanPostProcessor接口继承BeanPostProcessor接口。多出了1个方法： 1void postProcessBeforeDestruction(Object bean, String beanName) throws BeansException; 该方法是bean在Spring在容器中被销毁之前调用 MergedBeanDefinitionPostProcessorDestructionAwareBeanPostProcessor接口继承BeanPostProcessor接口。多出了1个方法： 1void postProcessMergedBeanDefinition(RootBeanDefinition beanDefinition, Class&lt;?&gt; beanType, String beanName); 该方法是bean在合并Bean定义之后调用 总结Spring内部对bean的构造已经形成了一套体系。如果我们想修改这套体系，只能使用Spring提供的BeanPostProcessor接口去处理。这样做的好处： 遵循设计模式的开闭原则，对扩展开放，对修改关闭。 我们只需要实现接口进行扩展即可，不需要修改内部的源码 下一篇，将分析Spring内置的一些BeanPostProcessor的功能。","raw":null,"content":null,"categories":[{"name":"springboot","slug":"springboot","permalink":"http://fangjian0423.github.io/categories/springboot/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://fangjian0423.github.io/tags/spring/"},{"name":"java","slug":"java","permalink":"http://fangjian0423.github.io/tags/java/"},{"name":"springboot","slug":"springboot","permalink":"http://fangjian0423.github.io/tags/springboot/"}]},{"title":"Spring类注册笔记","slug":"spring-bean-register-note","date":"2017-06-15T14:14:37.000Z","updated":"2017-06-15T14:49:22.000Z","comments":true,"path":"2017/06/15/spring-bean-register-note/","link":"","permalink":"http://fangjian0423.github.io/2017/06/15/spring-bean-register-note/","excerpt":"对Spring类注册功能做个笔记，包括内置的一些扫描器以及这些扫描器的用途和注意点；还有bean注册相关的接口介绍；最后就是这些扫描器在SpringBoot上的使用。","text":"对Spring类注册功能做个笔记，包括内置的一些扫描器以及这些扫描器的用途和注意点；还有bean注册相关的接口介绍；最后就是这些扫描器在SpringBoot上的使用。 扫描器 BeanDefinitionReader接口。目前有3种实现，分别是GroovyBeanDefinitionReader(groovy文件的读取器)、PropertiesBeanDefinitionReader(Properties文件的读取器)和XmlBeanDefinitionReader(xml文件的读取器)。这3个实现类都继承AbstractBeanDefinitionReader，AbstractBeanDefinitionReader抽象类内部有个BeanDefinitionRegistry接口类型的属性，BeanDefinitionRegistry接口存在的意义在于对bean数据的管理，包括bean的注册、删除、查找等。这3个实现类内部最终对bean的注册都是通过BeanDefinitionRegistry完成的，不同点在于它们处理过程不一样，比如xml文件的解析和properties文件的解析这个过程不一样 AnnotatedBeanDefinitionReader类。独立的一个类，用来注册单独的类，也是使用BeanDefinitionRegistry接口类型的属性完成bean的注册 ClassPathScanningCandidateComponentProvider类。独立的一个类，用来找出具体包下的bean信息，内部有2个TypeFilter集合属性，includeFilters和excludeFilters，分别用于对找出的bean信息做匹配，includeFilters中的TypeFilter只要有一个满足，就不会过滤；excludeFilters中的TypeFilter只要有一个满足，就会被过滤。ClassPathBeanDefinitionScanner是ClassPathScanningCandidateComponentProvider类的子类，提供了scan方法，这个scan方法会找出包下的bean信息并使用BeanDefinitionRegistry接口类型的属性完成bean的注册 ConfigurationClassParser类。独立的一个类，用来解析被@Configuration注解修饰的配置类。在SpringBoot源码分析之Spring容器的refresh过程文章中分析过ConfigurationClassParser的作用。简单点来说就是ConfigurationClassParser会解析被@Configuration注解修饰的类，然后再处理这个类内部被其它注解修饰的情况，比如@Import注解、@ComponentScan注解、@ImportResource注解、@Bean注解等。这里解析过程中也会遇到其它被@Configuration注解修饰的类，这些类会放到ConfigurationClassParser的configurationClasses属性中然后被ConfigurationClassBeanDefinitionReader处理 ConfigurationClassBeanDefinitionReader，独立的一个类，处理ConfigurationClassParser解析出的被@Configuration注解修饰的配置类，会处理配置类内部的被@Bean注解修饰的方法、@ImportResource注解修饰的资源、@Import注解修饰的ImportBeanDefinitionRegistrar接口。最后使用BeanDefinitionRegistry注册 ComponentScanAnnotationParser，独立的一个类，@ComponentScan注解对应的解析器，内部使用ClassPathBeanDefinitionScanner完成 扫描器注意点 第4点和第5点提到的ConfigurationClassParser和ConfigurationClassBeanDefinitionReader在ConfigurationClassPostProcessor这个BeanFactoryPostProcessor中使用；它们都是跟@Configuration注解修饰的类有关系 AnnotatedBeanDefinitionReader构造的时候会使用AnnotationConfigUtils的registerAnnotationConfigProcessors方法预先注册一些processor bean，比如ConfigurationClassPostProcessor、AutowiredAnnotationBeanPostProcessor、RequiredAnnotationBeanPostProcessor、CommonAnnotationBeanPostProcessor ClassPathBeanDefinitionScanner扫描具体包下的类，扫描完之后根据includeAnnotationConfig属性是否使用AnnotationConfigUtils的registerAnnotationConfigProcessors方法预先注册一些processor bean。includeAnnotationConfig属性默认是true，可修改 AnnotatedBeanDefinitionReader和ClassPathBeanDefinitionScanner加入processor的原因在于它们注册或者扫描出来的类在Spring容器的后续处理过程中进行另外的处理。比如扫描出来配置类(被@Configuration注解修饰的类)，会被ConfigurationClassPostProcessor处理(解析内部结构)，比如类中的一些被@Autowired注解修饰的属性会被AutowiredAnnotationBeanPostProcessor处理等等 一些接口 BeanDefinition接口：描述bean实例，包括属性值、构造方法的参数值、是否单例、是否抽象、作用域scope、对应的class名字等1.1 AnnotatedBeanDefinition接口是BeanDefinition接口的子接口，包括了实例的注解信息1.2 ScannedGenericBeanDefinition类是AnnotatedBeanDefinition接口的实现类，ClassPathScanningCandidateComponentProvider扫描出来的类信息就会封装成ScannedGenericBeanDefinition，这是一个被扫描到的类定义1.3 AnnotatedGenericBeanDefinition类也是AnnotatedBeanDefinition接口的实现类，在AnnotatedBeanDefinitionReader读取器读取类后封装成的，这是一个被注解过的类定义 BeanDefinitionRegistry接口：持有BeanDefinition信息的注册中心，可以注册新的BeanDefinition、删除老的BeanDefinition、获取注册中心的BeanDefinition。这个接口是Spring提供的唯一一个可以操作BeanDefinition数据的接口2.1 SimpleBeanDefinitionRegistry是一个简单的BeanDefinitionRegistry接口的实现类，内部使用Map类型的map存储BeanDefinition信息2.2 DefaultListableBeanFactory这个BeanFactory接口的实现类也实现了BeanDefinitionRegistry接口，内部也是使用Map类型的map存储BeanDefinition信息2.3 通用的Spring容器GenericApplicationContext也是BeanDefinitionRegistry接口的实现类，它使用内部属性DefaultListableBeanFactory完成BeanDefinitionRegistry接口的功能(DefaultListableBeanFactory实现了BeanDefinitionRegistry接口) BeanFactory接口：Spring bean容器，用来管理bean的容器 ApplicationContext：应用程序上下文接口，我一般喜欢叫Spring容器。它不仅仅包括bean相关的操作，还包括了很多其它的东西(毕竟是跟应用程序相关的)，比如环境信息的设置、事件触发器、国际化等。 是一个全局的概念 扫描器在SpringBoot中的使用SpringBoot中的SpringApplication类提供了run方法，run方法内部有一个参数叫做source，是Object类型的(有重载的方法，支持多个source，类型是Object数组)。 SpringBoot通过一个叫做BeanDefinitionLoader类的加载器去加载这些source。 12345678910111213141516private int load(Object source) &#123; Assert.notNull(source, \"Source must not be null\"); if (source instanceof Class&lt;?&gt;) &#123; return load((Class&lt;?&gt;) source); &#125; if (source instanceof Resource) &#123; return load((Resource) source); &#125; if (source instanceof Package) &#123; return load((Package) source); &#125; if (source instanceof CharSequence) &#123; return load((CharSequence) source); &#125; throw new IllegalArgumentException(\"Invalid source type \" + source.getClass());&#125; 目前支持4种类型的source，分别是： Class类型：使用AnnotatedBeanDefinitionReader读取器加载 Resource类型：使用XmlBeanDefinitionReader读取器加载 Package类型：使用ClassPathBeanDefinitionScanner扫描器加载 CharSequence：识别这个字符串信息。如果是Class类型，用第1种；如果是Resource类型，用第2种；如果是Package类型，用第3种 Class类型： 12345678@SpringBootApplicationpublic class MyApplication &#123; public static void main(String[] args) &#123; // Class类型，使用AnnotatedBeanDefinitionReader加载 // 加载完毕之后使用ConfigurationClassPostProcessor进行后续的处理 SpringApplication.run(MyApplication.class, args); &#125;&#125; Resource类型： 123456SpringApplication.run(new Object[] &#123; MyApplication.class , new ClassPathResource(\"beans.xml\") // 使用XmlBeanDefinitionReader加载&#125;, args); Package类型： 1234567SpringApplication.run(new Object[] &#123; MyApplication.class , new ClassPathResource(\"beans.xml\") , OtherBean.class.getPackage() // 使用ClassPathBeanDefinitionScanner加载&#125;, args);","raw":null,"content":null,"categories":[{"name":"springboot","slug":"springboot","permalink":"http://fangjian0423.github.io/categories/springboot/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://fangjian0423.github.io/tags/spring/"},{"name":"java","slug":"java","permalink":"http://fangjian0423.github.io/tags/java/"},{"name":"springboot","slug":"springboot","permalink":"http://fangjian0423.github.io/tags/springboot/"}]},{"title":"Spring自定义类扫描器","slug":"spring-custom-component-provider","date":"2017-06-11T10:41:50.000Z","updated":"2017-06-14T07:13:21.000Z","comments":true,"path":"2017/06/11/spring-custom-component-provider/","link":"","permalink":"http://fangjian0423.github.io/2017/06/11/spring-custom-component-provider/","excerpt":"在我们刚开始接触Spring的时候，要定义bean的话需要在xml中编写，比如：\n1&lt;bean id=\"myBean\" class=\"your.pkg.YourClass\"/&gt;\n后来发现如果bean比较多，会需要写很多的bean标签，太麻烦了。于是出现了一个component-scan注解。这个注解直接指定包名就可以，它会去扫描这个包下所有的class，然后判断是否解析：\n1&lt;context:component-scan base-package=\"your.pkg\"/&gt;\n再后来，由于注解Annotation的流行，出现了@ComponentScan注解，作用跟component-scan标签一样，跟@Configuration注解配合使用：\n12@ComponentScan(basePackages = &#123;\"your.pkg\", \"other.pkg\"&#125;)public class Application &#123; ... &#125;\n不论是component-scan标签，还是@ComponentScan注解。它们扫描或解析的bean只能是Spring内部所定义的，比如@Component、@Service、@Controller或@Repository。如果有一些自定义的注解，比如@Consumer、这个注解修饰的类是不会被扫描到的。这个时候我们就得自定义扫描器完成这个操作。","text":"在我们刚开始接触Spring的时候，要定义bean的话需要在xml中编写，比如： 1&lt;bean id=\"myBean\" class=\"your.pkg.YourClass\"/&gt; 后来发现如果bean比较多，会需要写很多的bean标签，太麻烦了。于是出现了一个component-scan注解。这个注解直接指定包名就可以，它会去扫描这个包下所有的class，然后判断是否解析： 1&lt;context:component-scan base-package=\"your.pkg\"/&gt; 再后来，由于注解Annotation的流行，出现了@ComponentScan注解，作用跟component-scan标签一样，跟@Configuration注解配合使用： 12@ComponentScan(basePackages = &#123;\"your.pkg\", \"other.pkg\"&#125;)public class Application &#123; ... &#125; 不论是component-scan标签，还是@ComponentScan注解。它们扫描或解析的bean只能是Spring内部所定义的，比如@Component、@Service、@Controller或@Repository。如果有一些自定义的注解，比如@Consumer、这个注解修饰的类是不会被扫描到的。这个时候我们就得自定义扫描器完成这个操作。 Spring内置的扫描器component-scan标签底层使用ClassPathBeanDefinitionScanner这个类完成扫描工作的。@ComponentScan注解配合@Configuration注解使用，底层使用ComponentScanAnnotationParser解析器完成解析工作。 ComponentScanAnnotationParser解析器内部使用了ClassPathBeanDefinitionScanner扫描器，ClassPathBeanDefinitionScanner扫描器内部的处理过程整理如下： 遍历basePackages，根据每个basePackage找出这个包下的所有的class。比如basePackage为your/pkg，会找出your.pkg包下所有的class。找出之后封装成Resource接口集合，这个Resource接口是Spring对资源的封装，有FileSystemResource、ClassPathResource、UrlResource实现等 遍历找到的Resource集合，通过includeFilters和excludeFilters判断是否解析。这里的includeFilters和excludeFilters是TypeFilter接口类型的集合，是ClassPathBeanDefinitionScanner内部的属性。TypeFilter接口是一个用于判断类型是否满足要求的类型过滤器。excludeFilters中只要有一个TypeFilter满足条件，这个Resource就会被过滤。includeFilters中只要有一个TypeFilter满足条件，这个Resource就不会被过滤 如果没有被过滤。把Resource封装成ScannedGenericBeanDefinition添加到BeanDefinition结果集中 返回最后的BeanDefinition结果集 TypeFilter接口的定义： 1234public interface TypeFilter &#123; boolean match(MetadataReader metadataReader, MetadataReaderFactory metadataReaderFactory) throws IOException;&#125; TypeFilter接口目前有AnnotationTypeFilter实现类(类是否有注解修饰)、RegexPatternTypeFilter(类名是否满足正则表达式)等。 ClassPathBeanDefinitionScanner继承ClassPathScanningCandidateComponentProvider类。 ClassPathScanningCandidateComponentProvider内部的构造函数提供了一个useDefaultFilters参数： 123public ClassPathScanningCandidateComponentProvider(boolean useDefaultFilters) &#123; this(useDefaultFilters, new StandardEnvironment());&#125; useDefaultFilters这个参数表示是否使用默认的TypeFilter，如果设置为true，会添加默认的TypeFilter： 1234567891011121314151617181920protected void registerDefaultFilters() &#123; this.includeFilters.add(new AnnotationTypeFilter(Component.class)); ClassLoader cl = ClassPathScanningCandidateComponentProvider.class.getClassLoader(); try &#123; this.includeFilters.add(new AnnotationTypeFilter( ((Class&lt;? extends Annotation&gt;) ClassUtils.forName(\"javax.annotation.ManagedBean\", cl)), false)); logger.debug(\"JSR-250 'javax.annotation.ManagedBean' found and supported for component scanning\"); &#125; catch (ClassNotFoundException ex) &#123; // JSR-250 1.1 API (as included in Java EE 6) not available - simply skip. &#125; try &#123; this.includeFilters.add(new AnnotationTypeFilter( ((Class&lt;? extends Annotation&gt;) ClassUtils.forName(\"javax.inject.Named\", cl)), false)); logger.debug(\"JSR-330 'javax.inject.Named' annotation found and supported for component scanning\"); &#125; catch (ClassNotFoundException ex) &#123; // JSR-330 API not available - simply skip. &#125;&#125; 我们看到这里includeFilters加上了AnnotationTypeFilter，并且对应的注解是@Component。@Service、@Controller或@Repository注解它们内部都是被@Component注解所修饰的，所以它们也会被识别。 自定义扫描功能一般情况下，我们要自定义扫描功能的话，可以直接使用ClassPathScanningCandidateComponentProvider完成，加上一些自定义的TypeFilter即可。或者写个自定义扫描器继承ClassPathScanningCandidateComponentProvider，并在内部添加自定义的TypeFilter。后者相当于对前者的封装。 我们就以一个简单的例子说明一下自定义扫描的实现，直接使用ClassPathScanningCandidateComponentProvider。 项目结构如下： ./ └── spring └── study └── componentprovider ├── annotation │ └── Consumer.java ├── bean │ ├── ConsumerWithComponentAnnotation.java │ ├── ConsumerWithConsumerAnnotation.java │ ├── ConsumerWithInterface.java │ ├── ConsumerWithNothing.java │ └── ProducerWithInterface.java └── interfaze ├── IConsumer.java └── IProducer.java 我们直接使用ClassPathScanningCandidateComponentProvider扫描spring.study.componentprovider.bean包下的class： 1234ClassPathScanningCandidateComponentProvider provider = new ClassPathScanningCandidateComponentProvider(false); // 不使用默认的TypeFilterprovider.addIncludeFilter(new AnnotationTypeFilter(Consumer.class));provider.addIncludeFilter(new AssignableTypeFilter(IConsumer.class));Set&lt;BeanDefinition&gt; beanDefinitionSet = provider.findCandidateComponents(\"spring.study.componentprovider.bean\"); 这里扫描出来的类只有2个，分别是ConsumerWithConsumerAnnotation(被@Consumer注解修饰)和ConsumerWithInterface(实现了IConsumer接口)。ConsumerWithComponentAnnotation使用@Component注解，ConsumerWithNothing没实现任何借口，没使用任何注解，ProducerWithInterface实现了IProducer接口；所以这3个类不会被识别。 如果我们要自定义ComponentProvider，继承ClassPathScanningCandidateComponentProvider类即可。 RepositoryComponentProvider这个类是SpringData模块提供的，继承自ClassPathScanningCandidateComponentProvider，主要是为了识别SpringData相关的类。 它内部定义了一些自定义TypeFilter，比如InterfaceTypeFilter(识别接口的TypeFilter，目标比较是个接口，而不是实现类)、AllTypeFilter(保存存储TypeList集合，这个集合内部所有的TypeFilter必须全部满足条件才能被识别)等。 有兴趣的读者可以查看源码。","raw":null,"content":null,"categories":[{"name":"springboot","slug":"springboot","permalink":"http://fangjian0423.github.io/categories/springboot/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://fangjian0423.github.io/tags/spring/"},{"name":"java","slug":"java","permalink":"http://fangjian0423.github.io/tags/java/"},{"name":"springboot","slug":"springboot","permalink":"http://fangjian0423.github.io/tags/springboot/"}]},{"title":"SpringBoot源码分析之配置环境的构造过程","slug":"springboot-environment-analysis","date":"2017-06-10T11:30:30.000Z","updated":"2017-06-10T08:05:24.000Z","comments":true,"path":"2017/06/10/springboot-environment-analysis/","link":"","permalink":"http://fangjian0423.github.io/2017/06/10/springboot-environment-analysis/","excerpt":"SpringBoot把配置文件的加载封装成了PropertySourceLoader接口，该接口的定义如下：\n1234567public interface PropertySourceLoader &#123;\t// 支持的文件后缀\tString[] getFileExtensions();\t// 把资源Resource加载成属性源PropertySource\tPropertySource&lt;?&gt; load(String name, Resource resource, String profile)\t\t\tthrows IOException;&#125;\nPropertySource是Spring对name/value键值对的封装接口。该定义了getSource()方法，这个方法会返回得到属性源的源头。比如MapPropertySource的源头就是一个Map，PropertiesPropertySource的源头就是一个Properties。\nPropertySource目前的实现类有不少，比如上面提到的MapPropertySource和PropertiesPropertySource，还有RandomValuePropertySource(source是Random)、SimpleCommandLinePropertySource(source是CommandLineArgs，命令行参数)、ServletConfigPropertySource(source是ServletConfig)等等。\nPropertySourceLoader接口目前有两个实现类：PropertiesPropertySourceLoader和YamlPropertySourceLoader。\nPropertiesPropertySourceLoader支持从xml或properties格式的文件中加载数据。\nYamlPropertySourceLoader支持从yml或者yaml格式的文件中加载数据。","text":"SpringBoot把配置文件的加载封装成了PropertySourceLoader接口，该接口的定义如下： 1234567public interface PropertySourceLoader &#123; // 支持的文件后缀 String[] getFileExtensions(); // 把资源Resource加载成属性源PropertySource PropertySource&lt;?&gt; load(String name, Resource resource, String profile) throws IOException;&#125; PropertySource是Spring对name/value键值对的封装接口。该定义了getSource()方法，这个方法会返回得到属性源的源头。比如MapPropertySource的源头就是一个Map，PropertiesPropertySource的源头就是一个Properties。 PropertySource目前的实现类有不少，比如上面提到的MapPropertySource和PropertiesPropertySource，还有RandomValuePropertySource(source是Random)、SimpleCommandLinePropertySource(source是CommandLineArgs，命令行参数)、ServletConfigPropertySource(source是ServletConfig)等等。 PropertySourceLoader接口目前有两个实现类：PropertiesPropertySourceLoader和YamlPropertySourceLoader。 PropertiesPropertySourceLoader支持从xml或properties格式的文件中加载数据。 YamlPropertySourceLoader支持从yml或者yaml格式的文件中加载数据。 Environment的构造以及PropertySource的生成Environment接口是Spring对当前程序运行期间的环境的封装。主要提供了两大功能：profile和property(父接口PropertyResolver提供)。目前主要有StandardEnvironment、StandardServletEnvironment和MockEnvironment3种实现，分别代表普通程序、Web程序以及测试程序的环境。 下面这段代码就是SpringBoot的run方法内调用的，它会在Spring容器构造之前调用，创建环境信息： 123456789101112131415161718// SpringApplication.classprivate ConfigurableApplicationContext createAndRefreshContext( SpringApplicationRunListeners listeners, ApplicationArguments applicationArguments) &#123; ConfigurableApplicationContext context; // 如果是web环境，创建StandardServletEnvironment // 否则，创建StandardEnvironment // StandardServletEnvironment继承自StandardEnvironment，StandardEnvironment继承AbstractEnvironment // AbstractEnvironment内部有个MutablePropertySources类型的propertySources属性，用于存储多个属性源PropertySource // StandardEnvironment构造的时候会默认加上2个PropertySource。分别是MapPropertySource(调用System.getProperties()配置)和SystemEnvironmentPropertySource(调用System.getenv()配置) ConfigurableEnvironment environment = getOrCreateEnvironment(); // 如果设置了一些启动参数args，添加基于args的SimpleCommandLinePropertySource // 还会配置profile信息，比如设置了spring.profiles.active启动参数，设置到环境信息中 configureEnvironment(environment, applicationArguments.getSourceArgs()); // 触发ApplicationEnvironmentPreparedEvent事件 listeners.environmentPrepared(environment); ...&#125; 在SpringBoot源码分析之SpringBoot的启动过程这篇文章中，我们分析过SpringApplication启动的时候会使用工厂加载机制初始化一些初始化器和监听器。其中org.springframework.boot.context.config.ConfigFileApplicationListener这个监听器会被加载： // spring-boot-version.release/META-INF/spring.factories org.springframework.context.ApplicationListener=\\ ... org.springframework.boot.context.config.ConfigFileApplicationListener,\\ ... ConfigFileApplicationListener会监听SpringApplication启动的时候发生的事件，它的监听代码： 123456789101112131415161718192021222324252627@Overridepublic void onApplicationEvent(ApplicationEvent event) &#123; // 应用环境信息准备好的时候对应的事件。此时Spring容器尚未创建，但是环境已经创建 if (event instanceof ApplicationEnvironmentPreparedEvent) &#123; onApplicationEnvironmentPreparedEvent( (ApplicationEnvironmentPreparedEvent) event); &#125; // Spring容器创建完成并在refresh方法调用之前对应的事件 if (event instanceof ApplicationPreparedEvent) &#123; onApplicationPreparedEvent(event); &#125;&#125;private void onApplicationEnvironmentPreparedEvent( ApplicationEnvironmentPreparedEvent event) &#123; // 使用工厂加载机制读取key为org.springframework.boot.env.EnvironmentPostProcessor的实现类 List&lt;EnvironmentPostProcessor&gt; postProcessors = loadPostProcessors(); // 加上自己。ConfigFileApplicationListener也是一个EnvironmentPostProcessor接口的实现类 postProcessors.add(this); // 排序 AnnotationAwareOrderComparator.sort(postProcessors); // 遍历这些EnvironmentPostProcessor，并调用postProcessEnvironment方法 for (EnvironmentPostProcessor postProcessor : postProcessors) &#123; postProcessor.postProcessEnvironment(event.getEnvironment(), event.getSpringApplication()); &#125;&#125; ConfigFileApplicationListener也是一个EnvironmentPostProcessor接口的实现类，在这里会被调用： 12345678910111213141516171819202122232425// ConfigFileApplicationListener的postProcessEnvironment方法@Overridepublic void postProcessEnvironment(ConfigurableEnvironment environment, SpringApplication application) &#123; // 添加属性源到环境中 addPropertySources(environment, application.getResourceLoader()); // 配置需要ignore的beaninfo configureIgnoreBeanInfo(environment); // 从环境中绑定一些参数到SpringApplication中 bindToSpringApplication(environment, application);&#125;protected void addPropertySources(ConfigurableEnvironment environment, ResourceLoader resourceLoader) &#123; // 添加一个RandomValuePropertySource到环境中 // RandomValuePropertySource是一个用于处理随机数的PropertySource，内部存储一个Random类的实例 RandomValuePropertySource.addToEnvironment(environment); try &#123; // 构造一个内部类Loader，并调用它的load方法 new Loader(environment, resourceLoader).load(); &#125; catch (IOException ex) &#123; throw new IllegalStateException(\"Unable to load configuration files\", ex); &#125;&#125; 内部类Loader的处理过程整理如下： 创建PropertySourcesLoader。PropertySourcesLoader内部有2个属性，分别是PropertySourceLoader集合和MutablePropertySources(内部有PropertySource的集合)。最终加载完毕之后MutablePropertySources属性中的PropertySource会被添加到环境Environment中的属性源列表中。PropertySourcesLoader被构造的时候会使用工厂加载机制获得PropertySourceLoader集合(默认就2个：PropertiesPropertySourceLoader和YamlPropertySourceLoader；可以自己扩展)，然后设置到属性中 获取环境信息中激活的profile(启动项目时设置的spring.profiles.active参数)。如果没设置profile，默认使用default这个profile，并添加到profiles队列中。最后会添加一个null到profiles队列中(为了获取没有指定profile的配置文件。比如环境中有application.yml和appliation-dev.yml，这个null就保证优先加载application.yml文件) profiles队列取出profile数据，使用PropertySourcesLoader内部的各个PropertySourceLoader支持的后缀去目录(默认识别4种目录classpath:/[类加载目录],classpath:/config/[类加载目录下的config目录],file:./[当前目录],file:./config/[当前目录下的config目录])查找application文件名(这4个目录是默认的，可以通过启动参数spring.config.location添加新的目录，文件名可以通过启动参数spring.config.name修改)。比如目录是file:/，文件名是application，后缀为properties，那么就会查找file:/application.properties文件，如果找到，执行第4步 找出的属性源文件被加载，然后添加到PropertySourcesLoader内部的PropertySourceLoader集合中。如果该属性源文件中存在spring.profiles.active配置，识别出来并加入第2步中的profiles队列，然后重复第3步 第4步找到的属性源从PropertySourcesLoader中全部添加到环境信息Environment中。如果这些属性源存在defaultProperties配置，那么会添加到Environment中的属性源集合头部，否则添加到尾部 比如项目中classpath下存在application.yml文件和application-dev.yml，application.yml文件的内容如下： spring.profiles.active: dev 直接启动项目，开始解析，过程如下： 从环境信息中找出是否设置profile，发现没有设置。 添加默认的profile - default，然后添加到队列里，最后添加null的profile。此时profiles队列中有2个元素：default和null profiles队列中先拿出null的profile。然后遍历4个目录和2个PropertySourceLoader中的4个后缀(PropertiesPropertySourceLoader的properties和xml以及YamlPropertySourceLoader的yml和yaml)的application文件名。file:./config/application.properties、file:./application.properties、classpath:/config/application.properties、classpath:/application.properties、file:./config/application.xml; file:./application.xml …. 找到classpath:/application.yml文件，解析成PropertySource并添加到PropertySourcesLoader里的MutablePropertySources中。由于该文件存在spring.profiles.active配置，把dev添加到profiles队列中 profiles队列拿出dev这个profile。由于存在profile，寻找文件的时候会带上profile，重复第3步，比如classpath:/application-dev.yml… 找到classpath:/application-dev.yml文件，解析成PropertySource并添加到PropertySourcesLoader里的MutablePropertySources中 profiles队列拿出default这个profile。寻找文件发现没有找到。结束 这里需要注意一下一些常用的额外参数的问题，整理如下： 如果启动程序的时候设置了系统参数spring.profiles.active，那么这个参数会被设置到环境信息中(由于设置了系统参数，在StandardEnvironment的钩子方法customizePropertySources中被封装成MapPropertySource并添加到Environment中)。这样PropertySourcesLoader加载的时候不会加上default这个默认profile，但是还是会读取profile为null的配置信息。spring.profiles.active支持多个profile，比如java -Dspring.profiles.active=”dev,custom” -jar yourjar.jar 如果设置程序参数spring.config.location，那么查找目录的时候会多出设置的目录，也支持多个目录的设置。这些会在SpringApplication里的configureEnvironment方法中被封装成SimpleCommandLinePropertySource并添加到Environment中。比如java -jar yourjar.jar –spring.config.location=classpath:/custom,file:./custom 1 2 3。有4个参数会被设置到SimpleCommandLinePropertySource中。解析文件的时候会多出2个目录，分别是classpath:/custom和file:./custom 如果设置程序参数spring.config.name，那么查找的文件名就是这个参数值。原理跟spring.config.location一样，都封装到了SimpleCommandLinePropertySource中。比如java -jar yourjar.jar –spring.config.name=myfile。 这样会去查找myfile文件，而不是默认的application文件 如果设置程序参数spring.profiles.active。注意这是程序参数，不是系统参数。比如java -jar yourjar.jar –spring.profiles.active=prod。会去解析prod这个profile(不论是系统参数还是程序参数，都会被封装成多个PropertySource存在于环境信息中。最终获取profile的时候会去环境信息中拿，且都可以拿到) 上面说的每个profile都是在不同文件里的。不同profile也可以存在在一个文件里。因为有profile会去加载带profile的文件的同时也会去加载不带profile的文件，并解析出这个文件中spring.profiles对应的值是profile的数据。比如profile为prod，会去查找application-prod.yml文件，也会去查找application.yml文件，其中application.yml文件只会查找spring.profiles为prod的数据 比如第6点中profile.yml的数据如下： spring: profiles: prod my.name: 1 --- spring: profiles: dev my.name: 2 这里会解析出spring.profiles为prod的数据，也就是my.name为1的数据。 优先级的问题：由于环境信息Environment中保存的PropertySource是MutablePropertySources，那么会去配置值的时候就存在优先级的问题。比如PropertySource1和PropertySource2都存在custom.name配置，那么会从哪个PropertySource中获取这个custom.name配置呢？它会遍历内部的PropertySource列表，越在前面的PropertySource，越先获取；比如PropertySource1在PropertySource2前面，那么会先获取PropertySource1的配置。MutablePropertySources内部添加PropertySource的时候可以选择元素的位置，可以addFirst，也可以addLast，也可以自定义位置。 总结：SpringApplication启动的时候会构造环境信息Environment，如果是web环境，创建StandardServletEnvironment，否则，创建StandardEnvironment。这两种环境创建的时候都会在内部的propertySources属性中加入一些PropertySource。比如属性属性的配置信息封装成MapPropertySource，系统环境配置信息封装成SystemEnvironmentPropertySource等。这些PropertySource集合存在在环境信息中，从环境信息中读取配置的话会遍历这些PropertySource并找到相对应的配置和值。Environment构造完成之后会读取springboot相应的配置文件，从3个角度去查找：目录、文件名和profile。这3个角度有默认值，可以进行覆盖。springboot相关的配置文件读取完成之后会被封装成PropertySource并添加到环境信息中。 @ConfigurationProperties和@EnableConfigurationProperties注解的原理SpringBoot内部规定了一套配置和配置属性类映射规则，可以使用@ConfigurationProperties注解配合前缀属性完成属性类的读取；再通过@EnableConfigurationProperties注解设置配置类就可以把这个配置类注入进来。 比如ES的配置类ElasticsearchProperties和对应的@EnableConfigurationProperties修饰的类ElasticsearchAutoConfiguration： 1234567891011121314151617181920// 使用前缀为spring.data.elasticsearch的配置@ConfigurationProperties(prefix = \"spring.data.elasticsearch\")public class ElasticsearchProperties &#123; private String clusterName = \"elasticsearch\"; private String clusterNodes; private Map&lt;String, String&gt; properties = new HashMap&lt;String, String&gt;(); ...&#125;@Configuration@ConditionalOnClass(&#123; Client.class, TransportClientFactoryBean.class, NodeClientFactoryBean.class &#125;)// 使用@EnableConfigurationProperties注解让ElasticsearchProperties配置生效// 这样ElasticsearchProperties就会自动注入到属性中@EnableConfigurationProperties(ElasticsearchProperties.class)public class ElasticsearchAutoConfiguration implements DisposableBean &#123; ... @Autowired private ElasticsearchProperties properties; ...&#125; 我们分析下这个过程的实现。 @EnableConfigurationProperties注解有个属性value，是个Class数组，它会导入一个selector：EnableConfigurationPropertiesImportSelector。这个selector的selectImport方法： 12345678910111213141516171819@Overridepublic String[] selectImports(AnnotationMetadata metadata) &#123; // 获取@EnableConfigurationProperties注解的属性 MultiValueMap&lt;String, Object&gt; attributes = metadata.getAllAnnotationAttributes( EnableConfigurationProperties.class.getName(), false); // 得到value属性，是个Class数组 Object[] type = attributes == null ? null : (Object[]) attributes.getFirst(\"value\"); if (type == null || type.length == 0) &#123; // 如果value属性不存在 return new String[] &#123; // 返回Registrar，Registrar内部会注册bean ConfigurationPropertiesBindingPostProcessorRegistrar.class .getName() &#125;; &#125; // 如果value属性存在 // 返回Registrar，Registrar内部会注册bean return new String[] &#123; ConfigurationPropertiesBeanRegistrar.class.getName(), ConfigurationPropertiesBindingPostProcessorRegistrar.class.getName() &#125;;&#125; ConfigurationPropertiesBeanRegistrar和ConfigurationPropertiesBindingPostProcessorRegistrar都实现了ImportBeanDefinitionRegistrar接口，会额外注册bean。 12345678910111213141516171819202122232425262728293031323334353637383940414243// ConfigurationPropertiesBeanRegistrar的registerBeanDefinitions方法@Overridepublic void registerBeanDefinitions(AnnotationMetadata metadata, BeanDefinitionRegistry registry) &#123; // 获取@EnableConfigurationProperties注解中的属性值Class数组 MultiValueMap&lt;String, Object&gt; attributes = metadata .getAllAnnotationAttributes( EnableConfigurationProperties.class.getName(), false); List&lt;Class&lt;?&gt;&gt; types = collectClasses(attributes.get(\"value\")); // 遍历这些Class数组 for (Class&lt;?&gt; type : types) &#123; // 如果这个class被@ConfigurationProperties注解修饰 // 获取@ConfigurationProperties注解中的前缀属性 // 否则该前缀为空字符串 String prefix = extractPrefix(type); // 构造bean的名字： 前缀-类全名 // 比如ElasticsearchProperties对应的bean名字就是spring.data.elasticsearch-org.springframework.boot.autoconfigure.data.elasticsearch.ElasticsearchProperties String name = (StringUtils.hasText(prefix) ? prefix + \"-\" + type.getName() : type.getName()); if (!registry.containsBeanDefinition(name)) &#123; // 这个bean没被注册的话进行注册 registerBeanDefinition(registry, type, name); &#125; &#125;&#125;// ConfigurationPropertiesBindingPostProcessorRegistrar的registerBeanDefinitions方法@Overridepublic void registerBeanDefinitions(AnnotationMetadata importingClassMetadata, BeanDefinitionRegistry registry) &#123; // 先判断Spring容器里是否有ConfigurationPropertiesBindingPostProcessor类型的bean // 由于条件里面会判断是否已经存在这个ConfigurationPropertiesBindingPostProcessor类型的bean // 所以实际上条件里的代码只会执行一次 if (!registry.containsBeanDefinition(BINDER_BEAN_NAME)) &#123; BeanDefinitionBuilder meta = BeanDefinitionBuilder .genericBeanDefinition(ConfigurationBeanFactoryMetaData.class); BeanDefinitionBuilder bean = BeanDefinitionBuilder.genericBeanDefinition( ConfigurationPropertiesBindingPostProcessor.class); bean.addPropertyReference(\"beanMetaDataStore\", METADATA_BEAN_NAME); registry.registerBeanDefinition(BINDER_BEAN_NAME, bean.getBeanDefinition()); registry.registerBeanDefinition(METADATA_BEAN_NAME, meta.getBeanDefinition()); &#125;&#125; ConfigurationPropertiesBindingPostProcessor在ConfigurationPropertiesBindingPostProcessorRegistrar中被注册到Spring容器中，它是一个BeanPostProcessor，它的postProcessBeforeInitialization方法如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758// Spring容器中bean被实例化之前要做的事@Overridepublic Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException &#123; // 先获取bean对应的Class中的@ConfigurationProperties注解 ConfigurationProperties annotation = AnnotationUtils .findAnnotation(bean.getClass(), ConfigurationProperties.class); // 如果@ConfigurationProperties注解，说明这是一个配置类。比如ElasticsearchProperties if (annotation != null) &#123; // 调用postProcessBeforeInitialization方法 postProcessBeforeInitialization(bean, beanName, annotation); &#125; // 同样的方法使用beanName去查找 annotation = this.beans.findFactoryAnnotation(beanName, ConfigurationProperties.class); if (annotation != null) &#123; postProcessBeforeInitialization(bean, beanName, annotation); &#125; return bean;&#125;private void postProcessBeforeInitialization(Object bean, String beanName, ConfigurationProperties annotation) &#123; Object target = bean; // 构造一个PropertiesConfigurationFactory PropertiesConfigurationFactory&lt;Object&gt; factory = new PropertiesConfigurationFactory&lt;Object&gt;( target); // 设置属性源，这里的属性源从环境信息Environment中得到 factory.setPropertySources(this.propertySources); // 设置验证器 factory.setValidator(determineValidator(bean)); // 设置ConversionService factory.setConversionService(this.conversionService == null ? getDefaultConversionService() : this.conversionService); if (annotation != null) &#123; // 设置@ConfigurationProperties注解对应的属性到PropertiesConfigurationFactory中 // 比如是否忽略不合法的属性ignoreInvalidFields、忽略未知的字段、忽略嵌套属性、验证器验证不合法后是否抛出异常 factory.setIgnoreInvalidFields(annotation.ignoreInvalidFields()); factory.setIgnoreUnknownFields(annotation.ignoreUnknownFields()); factory.setExceptionIfInvalid(annotation.exceptionIfInvalid()); factory.setIgnoreNestedProperties(annotation.ignoreNestedProperties()); if (StringUtils.hasLength(annotation.prefix())) &#123; // 设置前缀 factory.setTargetName(annotation.prefix()); &#125; &#125; try &#123; // 绑定属性到配置类中，比如ElasticsearchProperties // 会使用环境信息中的属性源进行绑定 // 这样配置类就读取到了配置文件中的配置 factory.bindPropertiesToTarget(); &#125; catch (Exception ex) &#123; String targetClass = ClassUtils.getShortName(target.getClass()); throw new BeanCreationException(beanName, \"Could not bind properties to \" + targetClass + \" (\" + getAnnotationDetails(annotation) + \")\", ex); &#125;&#125; 总结：SpringBoot内部规定了一套配置和配置属性类映射规则，可以使用@ConfigurationProperties注解配合前缀属性完成属性类的读取；再通过@EnableConfigurationProperties注解设置配置类就可以把这个配置类注入进来。由于这个配置类是被注入进来的，所以它肯定在Spring容器中存在；这是因为在ConfigurationPropertiesBeanRegistrar内部会注册配置类到Spring容器中，这个配置类的实例化过程在ConfigurationPropertiesBindingPostProcessor这个BeanPostProcessor完成，它会在实例化bean之前会判断bean是否被@ConfigurationProperties注解修饰，如果有，使用PropertiesConfigurationFactory从环境信息Environment中进行值的绑定。这个ConfigurationPropertiesBeanRegistrar是在使用@EnableConfigurationProperties注解的时候被创建的(通过EnableConfigurationPropertiesImportSelector)。配置类内部属性的绑定成功与否是通过环境信息Environment中的属性源PropertySource决定的。","raw":null,"content":null,"categories":[{"name":"springboot","slug":"springboot","permalink":"http://fangjian0423.github.io/categories/springboot/"}],"tags":[{"name":"java","slug":"java","permalink":"http://fangjian0423.github.io/tags/java/"},{"name":"springboot","slug":"springboot","permalink":"http://fangjian0423.github.io/tags/springboot/"},{"name":"springboot源码分析","slug":"springboot源码分析","permalink":"http://fangjian0423.github.io/tags/springboot源码分析/"}]},{"title":"SpringBoot源码分析系列","slug":"springboot-source-analysis-summary","date":"2017-06-05T11:40:30.000Z","updated":"2017-06-28T01:56:28.000Z","comments":true,"path":"2017/06/05/springboot-source-analysis-summary/","link":"","permalink":"http://fangjian0423.github.io/2017/06/05/springboot-source-analysis-summary/","excerpt":"","text":"自己总结和学习的SpringBoot源码，包括一些例子代码和文章。 本系列不涉及SpringBoot的入门，主要针对想深入学习SpringBoot的读者。 目前已经有的文章： SpringBoot源码分析之SpringBoot的启动过程 SpringBoot源码分析之Spring容器的refresh过程 SpringBoot源码分析之条件注解的底层实现 SpringBoot源码分析之内置Servlet容器 SpringBoot源码分析之SpringBoot可执行文件解析 SpringBoot源码分析之工厂加载机制 SpringBoot源码分析之配置环境的构造过程 另外也会写一些Spring和SpringBoot相关的文章。 Spring： Spring自定义类扫描器 Spring类注册笔记 Spring内部的BeanPostProcessor接口总结 Spring内置的BeanPostProcessor总结 SpringBoot： SpringBoot应用程序的关闭 会写一些例子记录学习的过程，上传到github。 我不能保证写的每个地方都是对的，有问题的地方还请指出来，一起学习!","raw":null,"content":null,"categories":[{"name":"springboot","slug":"springboot","permalink":"http://fangjian0423.github.io/categories/springboot/"}],"tags":[{"name":"java","slug":"java","permalink":"http://fangjian0423.github.io/tags/java/"},{"name":"springboot","slug":"springboot","permalink":"http://fangjian0423.github.io/tags/springboot/"},{"name":"springboot源码分析","slug":"springboot源码分析","permalink":"http://fangjian0423.github.io/tags/springboot源码分析/"}]},{"title":"SpringBoot源码分析之工厂加载机制","slug":"springboot-factory-loading-mechanism","date":"2017-06-05T11:30:30.000Z","updated":"2017-06-05T12:15:43.000Z","comments":true,"path":"2017/06/05/springboot-factory-loading-mechanism/","link":"","permalink":"http://fangjian0423.github.io/2017/06/05/springboot-factory-loading-mechanism/","excerpt":"在之前的一些文章中，我们提到过从spring.factories中找出key为XXX的类。比如@EnableAutoConfiguration注解对应的EnableAutoConfigurationImportSelector中的selectImport方法会在spring.factories文件中找出key为EnableAutoConfiguration对应的值。这些类都是自动化配置类：\n// 这个spring.factories文件在spring-boot-autoconfigure模块的 META-INF/spring.factories中\norg.springframework.boot.autoconfigure.EnableAutoConfiguration=\\\norg.springframework.boot.autoconfigure.admin.SpringApplicationAdminJmxAutoConfiguration,\\\norg.springframework.boot.autoconfigure.aop.AopAutoConfiguration,\\\norg.springframework.boot.autoconfigure.amqp.RabbitAutoConfiguration,\\\norg.springframework.boot.autoconfigure.batch.BatchAutoConfiguration,\\\norg.springframework.boot.autoconfigure.cache.CacheAutoConfiguration,\\\norg.springframework.boot.autoconfigure.cassandra.CassandraAutoConfiguration,\\\norg.springframework.boot.autoconfigure.cloud.CloudAutoConfiguration,\\\norg.springframework.boot.autoconfigure.context.ConfigurationPropertiesAutoConfiguration,\\\norg.springframework.boot.autoconfigure.context.MessageSourceAutoConfiguration,\\\norg.springframework.boot.autoconfigure.context.PropertyPlaceholderAutoConfiguration,\\\norg.springframework.boot.autoconfigure.couchbase.CouchbaseAutoConfiguration,\\\n......\n","text":"在之前的一些文章中，我们提到过从spring.factories中找出key为XXX的类。比如@EnableAutoConfiguration注解对应的EnableAutoConfigurationImportSelector中的selectImport方法会在spring.factories文件中找出key为EnableAutoConfiguration对应的值。这些类都是自动化配置类： // 这个spring.factories文件在spring-boot-autoconfigure模块的 META-INF/spring.factories中 org.springframework.boot.autoconfigure.EnableAutoConfiguration=\\ org.springframework.boot.autoconfigure.admin.SpringApplicationAdminJmxAutoConfiguration,\\ org.springframework.boot.autoconfigure.aop.AopAutoConfiguration,\\ org.springframework.boot.autoconfigure.amqp.RabbitAutoConfiguration,\\ org.springframework.boot.autoconfigure.batch.BatchAutoConfiguration,\\ org.springframework.boot.autoconfigure.cache.CacheAutoConfiguration,\\ org.springframework.boot.autoconfigure.cassandra.CassandraAutoConfiguration,\\ org.springframework.boot.autoconfigure.cloud.CloudAutoConfiguration,\\ org.springframework.boot.autoconfigure.context.ConfigurationPropertiesAutoConfiguration,\\ org.springframework.boot.autoconfigure.context.MessageSourceAutoConfiguration,\\ org.springframework.boot.autoconfigure.context.PropertyPlaceholderAutoConfiguration,\\ org.springframework.boot.autoconfigure.couchbase.CouchbaseAutoConfiguration,\\ ...... 代码的实现，EnableAutoConfigurationImportSelector的selectImport方法： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960@Overridepublic String[] selectImports(AnnotationMetadata metadata) &#123; try &#123; // 获取注解的属性 AnnotationAttributes attributes = getAttributes(metadata); // 读取spring.factories属性文件中的数据 List&lt;String&gt; configurations = getCandidateConfigurations(metadata, attributes); // 删除重复的配置类 configurations = removeDuplicates(configurations); // 找到@EnableAutoConfiguration注解中定义的需要被过滤的配置类 Set&lt;String&gt; exclusions = getExclusions(metadata, attributes); // 删除这些需要被过滤的配置类 configurations.removeAll(exclusions); // 配置类做排序 configurations = sort(configurations); // 记录配置类的处理信息到ConditionEvaluationReport中 recordWithConditionEvaluationReport(configurations, exclusions); // 返回最终得到的自动化配置类 return configurations.toArray(new String[configurations.size()]); &#125; catch (IOException ex) &#123; throw new IllegalStateException(ex); &#125;&#125;protected List&lt;String&gt; getCandidateConfigurations(AnnotationMetadata metadata, AnnotationAttributes attributes) &#123; // 调用SpringFactoriesLoader的loadFactoryNames静态方法 // getSpringFactoriesLoaderFactoryClass方法返回的是EnableAutoConfiguration类对象 return SpringFactoriesLoader.loadFactoryNames( getSpringFactoriesLoaderFactoryClass(), getBeanClassLoader());&#125;public static List&lt;String&gt; loadFactoryNames(Class&lt;?&gt; factoryClass, ClassLoader classLoader) &#123; // 解析出properties文件中需要的key值 String factoryClassName = factoryClass.getName(); try &#123; // 常量FACTORIES_RESOURCE_LOCATION的值为META-INF/spring.factories // 使用类加载器找META-INF/spring.factories资源 Enumeration&lt;URL&gt; urls = (classLoader != null ? classLoader.getResources(FACTORIES_RESOURCE_LOCATION) : ClassLoader.getSystemResources(FACTORIES_RESOURCE_LOCATION)); List&lt;String&gt; result = new ArrayList&lt;String&gt;(); // 遍历找到的资源 while (urls.hasMoreElements()) &#123; URL url = urls.nextElement(); // 使用属性文件加载资源 Properties properties = PropertiesLoaderUtils.loadProperties(new UrlResource(url)); // 找出key为参数factoryClass类对象对应的全名称对应的值 String factoryClassNames = properties.getProperty(factoryClassName); // 以逗号分隔添加到结果集中 result.addAll(Arrays.asList(StringUtils.commaDelimitedListToStringArray(factoryClassNames))); &#125; return result; &#125; catch (IOException ex) &#123; throw new IllegalArgumentException(\"Unable to load [\" + factoryClass.getName() + \"] factories from location [\" + FACTORIES_RESOURCE_LOCATION + \"]\", ex); &#125;&#125; Spring Framework内部使用一种工厂加载机制(Factory Loading Mechanism)。这种机制使用SpringFactoriesLoader完成，SpringFactoriesLoader使用loadFactories方法加载并实例化从META-INF目录里的spring.factories文件出来的工厂，这些spring.factories文件都是从classpath里的jar包里找出来的。 spring.factories文件是以Java的Properties格式存在，key是接口或抽象类的全名、value是以逗号 “ , “ 分隔的实现类，比如： example.MyService=example.MyServiceImpl1,example.MyServiceImpl2 其中example.MyService是接口的全名，example.MyServiceImpl1和example.MyServiceImpl2是这个接口的两种实现。 可通过SpringFactoriesLoader完成： 1234List&lt;String&gt; classes = SpringFactoriesLoader.loadFactoryNames(EnableAutoConfiguration.class, this.getClass().getClassLoader());classes.forEach(clazz -&gt; &#123; System.out.println(\"==== \" + clazz);&#125;); 总结： 工厂加载机制是Spring内部提供的一个约定俗成的加载方式。只需要在模块的META-INF目录下定义Properties格式的spring.factories文件，这个Properties格式的文件中的key是接口或抽象类的全名，value是以逗号 “ , “ 分隔的实现类。 SpringBoot中的autoconfigure模块中的spring.factories就存在于META-INF目录下： ├── META-INF │ ├── MANIFEST.MF │ ├── additional-spring-configuration-metadata.json │ ├── maven │ │ └── org.springframework.boot │ │ └── spring-boot-autoconfigure │ │ ├── pom.properties │ │ └── pom.xml │ ├── spring-configuration-metadata.json │ └── spring.factories ├── org │ └── springframework │ └── boot │ └── autoconfigure │ ├── AbstractDependsOnBeanFactoryPostProcessor.class .... 而且也定义了一些配置，比如自动化配置信息： org.springframework.boot.autoconfigure.EnableAutoConfiguration=... 应用初始化器： org.springframework.context.ApplicationContextInitializer=... 应用监听器： org.springframework.context.ApplicationListener=... 模板可用提供器： org.springframework.boot.autoconfigure.template.TemplateAvailabilityProvider=... 等等。 我们只需要遵守这个机制并在对应的文件中写出需要加载的接口和实例即可，或者自己使用SpringFactoriesLoader实现加载。","raw":null,"content":null,"categories":[{"name":"springboot","slug":"springboot","permalink":"http://fangjian0423.github.io/categories/springboot/"}],"tags":[{"name":"java","slug":"java","permalink":"http://fangjian0423.github.io/tags/java/"},{"name":"springboot","slug":"springboot","permalink":"http://fangjian0423.github.io/tags/springboot/"},{"name":"springboot源码分析","slug":"springboot源码分析","permalink":"http://fangjian0423.github.io/tags/springboot源码分析/"}]},{"title":"SpringData ES中一些底层原理的分析","slug":"spring-data-es-analysis","date":"2017-06-02T12:33:33.000Z","updated":"2017-06-02T15:34:02.000Z","comments":true,"path":"2017/06/02/spring-data-es-analysis/","link":"","permalink":"http://fangjian0423.github.io/2017/06/02/spring-data-es-analysis/","excerpt":"之前写过一篇SpringData ES 关于字段名和索引中的列名字不一致导致的查询问题，顺便深入学习下Spring Data Elasticsearch。\nSpring Data Elasticsearch是Spring Data针对Elasticsearch的实现。\n它跟Spring Data一样，提供了Repository接口，我们只需要定义一个新的接口并继承这个Repository接口，然后就可以注入这个新的接口使用了。\n定义接口：\n12@Repositorypublic interface TaskRepository extends ElasticsearchRepository&lt;Task, String&gt; &#123; &#125;\n注入接口进行使用：\n12345@Autowiredprivate TaskRepository taskRepository;....taskRepository.save(task);","text":"之前写过一篇SpringData ES 关于字段名和索引中的列名字不一致导致的查询问题，顺便深入学习下Spring Data Elasticsearch。 Spring Data Elasticsearch是Spring Data针对Elasticsearch的实现。 它跟Spring Data一样，提供了Repository接口，我们只需要定义一个新的接口并继承这个Repository接口，然后就可以注入这个新的接口使用了。 定义接口： 12@Repositorypublic interface TaskRepository extends ElasticsearchRepository&lt;Task, String&gt; &#123; &#125; 注入接口进行使用： 12345@Autowiredprivate TaskRepository taskRepository;....taskRepository.save(task); Repository接口的代理生成上面的例子中TaskRepository是个接口，而我们却直接注入了这个接口并调用方法；很明显，这是错误的。 其实SpringData ES内部基于这个TaskRepository接口构造一个SimpleElasticsearchRepository，真正被注入的是这个SimpleElasticsearchRepository。 这个过程是如何实现的呢？ 来分析一下。 ElasticsearchRepositoriesAutoConfiguration自动化配置类会导入ElasticsearchRepositoriesRegistrar这个ImportBeanDefinitionRegistrar。 ElasticsearchRepositoriesRegistrar继承自AbstractRepositoryConfigurationSourceSupport，是个ImportBeanDefinitionRegistrar接口的实现类，会被Spring容器调用registerBeanDefinitions进行自定义bean的注册。 ElasticsearchRepositoriesRegistrar委托给RepositoryConfigurationDelegate完成bean的解析。 整个解析过程可以分3个步骤： 找出模块中的org.springframework.data.repository.Repository接口的实现类或者org.springframework.data.repository.RepositoryDefinition注解的修饰类，并会过滤掉org.springframework.data.repository.NoRepositoryBean注解的修饰类。找出后封装到RepositoryConfiguration中 遍历这些RepositoryConfiguration，然后构造成BeanDefinition并注册到Spring容器中。需要注意的是这些RepositoryConfiguration会以beanClass为ElasticsearchRepositoryFactoryBean这个类的方式被注册，并把对应的Repository接口当做构造参数传递给ElasticsearchRepositoryFactoryBean，还会设置相应的属性比如elasticsearchOperations、evaluationContextProvider、namedQueries、repositoryBaseClass、lazyInit、queryLookupStrategyKey ElasticsearchRepositoryFactoryBean被实例化的时候设置对应的构造参数和属性。设置完毕以后调用afterPropertiesSet方法(实现了InitializingBean接口)。在afterPropertiesSet方法内部会去创建RepositoryFactorySupport类，并进行一些初始化，比如namedQueries、repositoryBaseClass等。然后通过这个RepositoryFactorySupport的getRepository方法基于Repository接口创建出代理类，并使用AOP添加了几个MethodInterceptor 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142// 遍历基于第1步条件得到的RepositoryConfiguration集合for (RepositoryConfiguration&lt;? extends RepositoryConfigurationSource&gt; configuration : extension .getRepositoryConfigurations(configurationSource, resourceLoader, inMultiStoreMode)) &#123; // 构造出BeanDefinitionBuilder BeanDefinitionBuilder definitionBuilder = builder.build(configuration); extension.postProcess(definitionBuilder, configurationSource); if (isXml) &#123; // 设置elasticsearchOperations属性 extension.postProcess(definitionBuilder, (XmlRepositoryConfigurationSource) configurationSource); &#125; else &#123; // 设置elasticsearchOperations属性 extension.postProcess(definitionBuilder, (AnnotationRepositoryConfigurationSource) configurationSource); &#125; // 使用命名策略生成bean的名字 AbstractBeanDefinition beanDefinition = definitionBuilder.getBeanDefinition(); String beanName = beanNameGenerator.generateBeanName(beanDefinition, registry); if (LOGGER.isDebugEnabled()) &#123; LOGGER.debug(REPOSITORY_REGISTRATION, extension.getModuleName(), beanName, configuration.getRepositoryInterface(), extension.getRepositoryFactoryClassName()); &#125; beanDefinition.setAttribute(FACTORY_BEAN_OBJECT_TYPE, configuration.getRepositoryInterface()); // 注册到Spring容器中 registry.registerBeanDefinition(beanName, beanDefinition); definitions.add(new BeanComponentDefinition(beanDefinition, beanName));&#125;// build方法public BeanDefinitionBuilder build(RepositoryConfiguration&lt;?&gt; configuration) &#123; Assert.notNull(registry, \"BeanDefinitionRegistry must not be null!\"); Assert.notNull(resourceLoader, \"ResourceLoader must not be null!\"); // 得到factoryBeanName，这里会使用extension.getRepositoryFactoryClassName()去获得 // extension.getRepositoryFactoryClassName()返回的正是ElasticsearchRepositoryFactoryBean String factoryBeanName = configuration.getRepositoryFactoryBeanName(); factoryBeanName = StringUtils.hasText(factoryBeanName) ? factoryBeanName : extension.getRepositoryFactoryClassName(); // 基于factoryBeanName构造BeanDefinitionBuilder BeanDefinitionBuilder builder = BeanDefinitionBuilder.rootBeanDefinition(factoryBeanName); builder.getRawBeanDefinition().setSource(configuration.getSource()); // 设置ElasticsearchRepositoryFactoryBean的构造参数，这里是对应的Repository接口 // 设置一些的属性值 builder.addConstructorArgValue(configuration.getRepositoryInterface()); builder.addPropertyValue(\"queryLookupStrategyKey\", configuration.getQueryLookupStrategyKey()); builder.addPropertyValue(\"lazyInit\", configuration.isLazyInit()); builder.addPropertyValue(\"repositoryBaseClass\", configuration.getRepositoryBaseClassName()); NamedQueriesBeanDefinitionBuilder definitionBuilder = new NamedQueriesBeanDefinitionBuilder( extension.getDefaultNamedQueryLocation()); if (StringUtils.hasText(configuration.getNamedQueriesLocation())) &#123; definitionBuilder.setLocations(configuration.getNamedQueriesLocation()); &#125; builder.addPropertyValue(\"namedQueries\", definitionBuilder.build(configuration.getSource())); // 查找是否有对应Repository接口的自定义实现类 String customImplementationBeanName = registerCustomImplementation(configuration); // 存在自定义实现类的话，设置到属性中 if (customImplementationBeanName != null) &#123; builder.addPropertyReference(\"customImplementation\", customImplementationBeanName); builder.addDependsOn(customImplementationBeanName); &#125; RootBeanDefinition evaluationContextProviderDefinition = new RootBeanDefinition( ExtensionAwareEvaluationContextProvider.class); evaluationContextProviderDefinition.setSource(configuration.getSource()); // 设置一些的属性值 builder.addPropertyValue(\"evaluationContextProvider\", evaluationContextProviderDefinition); return builder;&#125;// RepositoryFactorySupport的getRepository方法，获得Repository接口的代理类public &lt;T&gt; T getRepository(Class&lt;T&gt; repositoryInterface, Object customImplementation) &#123; // 获取Repository的元数据 RepositoryMetadata metadata = getRepositoryMetadata(repositoryInterface); // 获取Repository的自定义实现类 Class&lt;?&gt; customImplementationClass = null == customImplementation ? null : customImplementation.getClass(); // 根据元数据和自定义实现类得到Repository的RepositoryInformation信息类 // 获取信息类的时候如果发现repositoryBaseClass是空的话会根据meta中的信息去自动匹配 // 具体匹配过程在下面的getRepositoryBaseClass方法中说明 RepositoryInformation information = getRepositoryInformation(metadata, customImplementationClass); // 验证 validate(information, customImplementation); // 得到最终的目标类实例，会通过repositoryBaseClass去查找 Object target = getTargetRepository(information); // 创建代理工厂 ProxyFactory result = new ProxyFactory(); result.setTarget(target); result.setInterfaces(new Class[] &#123; repositoryInterface, Repository.class &#125;); // 进行aop相关的设置 result.addAdvice(SurroundingTransactionDetectorMethodInterceptor.INSTANCE); result.addAdvisor(ExposeInvocationInterceptor.ADVISOR); if (TRANSACTION_PROXY_TYPE != null) &#123; result.addInterface(TRANSACTION_PROXY_TYPE); &#125; // 使用RepositoryProxyPostProcessor处理 for (RepositoryProxyPostProcessor processor : postProcessors) &#123; processor.postProcess(result, information); &#125; if (IS_JAVA_8) &#123; // 如果是JDK8的话，添加DefaultMethodInvokingMethodInterceptor result.addAdvice(new DefaultMethodInvokingMethodInterceptor()); &#125; // 添加QueryExecutorMethodInterceptor result.addAdvice(new QueryExecutorMethodInterceptor(information, customImplementation, target)); // 使用代理工厂创建出代理类，这里是使用jdk内置的代理模式 return (T) result.getProxy(classLoader);&#125;// 目标类的获取protected Class&lt;?&gt; getRepositoryBaseClass(RepositoryMetadata metadata) &#123; // 如果Repository接口属于QueryDsl，抛出异常。目前还不支持 if (isQueryDslRepository(metadata.getRepositoryInterface())) &#123; throw new IllegalArgumentException(\"QueryDsl Support has not been implemented yet.\"); &#125; // 如果主键是数值类型的话，repositoryBaseClass为NumberKeyedRepository if (Integer.class.isAssignableFrom(metadata.getIdType()) || Long.class.isAssignableFrom(metadata.getIdType()) || Double.class.isAssignableFrom(metadata.getIdType())) &#123; return NumberKeyedRepository.class; &#125; else if (metadata.getIdType() == String.class) &#123; // 如果主键是String类型的话，repositoryBaseClass为SimpleElasticsearchRepository return SimpleElasticsearchRepository.class; &#125; else if (metadata.getIdType() == UUID.class) &#123; // 如果主键是UUID类型的话，repositoryBaseClass为UUIDElasticsearchRepository return UUIDElasticsearchRepository.class; &#125; else &#123; // 否则报错 throw new IllegalArgumentException(\"Unsupported ID type \" + metadata.getIdType()); &#125;&#125; ElasticsearchRepositoryFactoryBean是一个FactoryBean接口的实现类，getObject方法返回的上面提到的getRepository方法返回的代理对象；getObjectType方法返回的是对应Repository接口类型。 我们文章一开始提到的注入TaskRepository的时候，实际上这个对象是ElasticsearchRepositoryFactoryBean类型的实例，只不过ElasticsearchRepositoryFactoryBean实现了FactoryBean接口，所以注入的时候会得到一个代理对象，这个代理对象是由jdk内置的代理生成的，并且它的target对象是SimpleElasticsearchRepository(主键是String类型)。 SpringData ES中ElasticsearchOperations的介绍ElasticsearchTemplate实现了ElasticsearchOperations接口。 ElasticsearchOperations接口是SpringData对Elasticsearch操作的一层封装，比如有创建索引createIndex方法、获取索引的设置信息getSetting方法、查询对象queryForObject方法、分页查询方法queryForPage、删除文档delete方法、更新文档update方法等等。 ElasticsearchTemplate是具体的实现类，它有这些属性： 12345678910// elasticsearch提供的基于java的客户端连接接口。java对es集群的操作使用这个接口完成private Client client;// 一个转换器接口，定义了2个方法，分别可以获得MappingContext和ConversionService// MappingContext接口用于获取所有的持久化实体和这些实体的属性// ConversionService目前在SpringData ES中没有被使用private ElasticsearchConverter elasticsearchConverter;// 内部使用EntityMapper完成对象到json字符串和json字符串到对象的映射。默认使用jackson完成映射，可自定义private ResultsMapper resultsMapper;// 查询超时时间private String searchTimeout; Client接口在ElasticsearchAutoConfiguration自动化配置类里被构造： 12345678910@Bean@ConditionalOnMissingBeanpublic Client elasticsearchClient() &#123; try &#123; return createClient(); &#125; catch (Exception ex) &#123; throw new IllegalStateException(ex); &#125;&#125; ElasticsearchTemplate、ElasticsearchConverter以及SimpleElasticsearchMappingContext在ElasticsearchDataAutoConfiguration自动化配置类里被构造： 123456789101112131415161718192021222324@Bean@ConditionalOnMissingBeanpublic ElasticsearchTemplate elasticsearchTemplate(Client client, ElasticsearchConverter converter) &#123; try &#123; return new ElasticsearchTemplate(client, converter); &#125; catch (Exception ex) &#123; throw new IllegalStateException(ex); &#125;&#125;@Bean@ConditionalOnMissingBeanpublic ElasticsearchConverter elasticsearchConverter( SimpleElasticsearchMappingContext mappingContext) &#123; return new MappingElasticsearchConverter(mappingContext);&#125;@Bean@ConditionalOnMissingBeanpublic SimpleElasticsearchMappingContext mappingContext() &#123; return new SimpleElasticsearchMappingContext();&#125; 需要注意的是这个bean被自动化配置类构造的前提是它们在Spring容器中并不存在。 Repository的调用过程以自定义的TaskRepository的save方法为例，大致的执行流程如下所示： SimpleElasticsearchRepository的save方法具体的分析在SpringData ES 关于字段名和索引中的列名字不一致导致的查询问题中分析过。 像自定义的Repository查询方法，或者Repository接口的自定义实现类的操作这些底层，可以去QueryExecutorMethodInterceptor中查看，本文就不做具体分析了。","raw":null,"content":null,"categories":[{"name":"java","slug":"java","permalink":"http://fangjian0423.github.io/categories/java/"}],"tags":[{"name":"java","slug":"java","permalink":"http://fangjian0423.github.io/tags/java/"},{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://fangjian0423.github.io/tags/elasticsearch/"}]},{"title":"SpringBoot源码分析之SpringBoot可执行文件解析","slug":"springboot-executable-jar","date":"2017-05-31T12:33:33.000Z","updated":"2017-05-31T13:53:46.000Z","comments":true,"path":"2017/05/31/springboot-executable-jar/","link":"","permalink":"http://fangjian0423.github.io/2017/05/31/springboot-executable-jar/","excerpt":"SpringBoot提供了一个插件spring-boot-maven-plugin用于把程序打包成一个可执行的jar包。在pom文件里加入这个插件即可：\n12345678&lt;build&gt;    &lt;plugins&gt;        &lt;plugin&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;        &lt;/plugin&gt;    &lt;/plugins&gt;&lt;/build&gt;\n打包完生成的executable-jar-1.0-SNAPSHOT.jar内部的结构如下：\n├── META-INF\n│   ├── MANIFEST.MF\n│   └── maven\n│       └── spring.study\n│           └── executable-jar\n│               ├── pom.properties\n│               └── pom.xml\n├── lib\n│   ├── aopalliance-1.0.jar\n│   ├── classmate-1.1.0.jar\n│   ├── spring-boot-1.3.5.RELEASE.jar\n│   ├── spring-boot-autoconfigure-1.3.5.RELEASE.jar\n│   ├── ...\n├── org\n│   └── springframework\n│       └── boot\n│           └── loader\n│               ├── ExecutableArchiveLauncher$1.class\n│               ├── ...\n└── spring\n    └── study\n        └── executablejar\n            └── ExecutableJarApplication.class\n然后可以直接执行jar包就能启动程序了：\n1java -jar executable-jar-1.0-SNAPSHOT.jar","text":"SpringBoot提供了一个插件spring-boot-maven-plugin用于把程序打包成一个可执行的jar包。在pom文件里加入这个插件即可： 12345678&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 打包完生成的executable-jar-1.0-SNAPSHOT.jar内部的结构如下： ├── META-INF │ ├── MANIFEST.MF │ └── maven │ └── spring.study │ └── executable-jar │ ├── pom.properties │ └── pom.xml ├── lib │ ├── aopalliance-1.0.jar │ ├── classmate-1.1.0.jar │ ├── spring-boot-1.3.5.RELEASE.jar │ ├── spring-boot-autoconfigure-1.3.5.RELEASE.jar │ ├── ... ├── org │ └── springframework │ └── boot │ └── loader │ ├── ExecutableArchiveLauncher$1.class │ ├── ... └── spring └── study └── executablejar └── ExecutableJarApplication.class 然后可以直接执行jar包就能启动程序了： 1java -jar executable-jar-1.0-SNAPSHOT.jar 打包出来fat jar内部有4种文件类型： META-INF文件夹：程序入口，其中MANIFEST.MF用于描述jar包的信息 lib目录：放置第三方依赖的jar包，比如springboot的一些jar包 spring boot loader相关的代码 模块自身的代码 MANIFEST.MF文件的内容： Manifest-Version: 1.0 Implementation-Title: executable-jar Implementation-Version: 1.0-SNAPSHOT Archiver-Version: Plexus Archiver Built-By: Format Start-Class: spring.study.executablejar.ExecutableJarApplication Implementation-Vendor-Id: spring.study Spring-Boot-Version: 1.3.5.RELEASE Created-By: Apache Maven 3.2.3 Build-Jdk: 1.8.0_20 Implementation-Vendor: Pivotal Software, Inc. Main-Class: org.springframework.boot.loader.JarLauncher 我们看到，它的Main-Class是org.springframework.boot.loader.JarLauncher，当我们使用java -jar执行jar包的时候会调用JarLauncher的main方法，而不是我们编写的SpringApplication。 那么JarLauncher这个类是的作用是什么的？ 它是SpringBoot内部提供的工具Spring Boot Loader提供的一个用于执行Application类的工具类(fat jar内部有spring loader相关的代码就是因为这里用到了)。相当于Spring Boot Loader提供了一套标准用于执行SpringBoot打包出来的jar Spring Boot Loader抽象的一些类抽象类Launcher：各种Launcher的基础抽象类，用于启动应用程序；跟Archive配合使用；目前有3种实现，分别是JarLauncher、WarLauncher以及PropertiesLauncher Archive：归档文件的基础抽象类。JarFileArchive就是jar包文件的抽象。它提供了一些方法比如getUrl会返回这个Archive对应的URL；getManifest方法会获得Manifest数据等。ExplodedArchive是文件目录的抽象 JarFile：对jar包的封装，每个JarFileArchive都会对应一个JarFile。JarFile被构造的时候会解析内部结构，去获取jar包里的各个文件或文件夹，这些文件或文件夹会被封装到Entry中，也存储在JarFileArchive中。如果Entry是个jar，会解析成JarFileArchive。 比如一个JarFileArchive对应的URL为： jar:file:/Users/format/Develop/gitrepository/springboot-analysis/springboot-executable-jar/target/executable-jar-1.0-SNAPSHOT.jar!/ 它对应的JarFile为： /Users/format/Develop/gitrepository/springboot-analysis/springboot-executable-jar/target/executable-jar-1.0-SNAPSHOT.jar 这个JarFile有很多Entry，比如： META-INF/ META-INF/MANIFEST.MF spring/ spring/study/ .... spring/study/executablejar/ExecutableJarApplication.class lib/spring-boot-starter-1.3.5.RELEASE.jar lib/spring-boot-1.3.5.RELEASE.jar ... JarFileArchive内部的一些依赖jar对应的URL(SpringBoot使用org.springframework.boot.loader.jar.Handler处理器来处理这些URL)： jar:file:/Users/Format/Develop/gitrepository/springboot-analysis/springboot-executable-jar/target/executable-jar-1.0-SNAPSHOT.jar!/lib/spring-boot-starter-web-1.3.5.RELEASE.jar!/ jar:file:/Users/Format/Develop/gitrepository/springboot-analysis/springboot-executable-jar/target/executable-jar-1.0-SNAPSHOT.jar!/lib/spring-boot-loader-1.3.5.RELEASE.jar!/org/springframework/boot/loader/JarLauncher.class 我们看到如果有jar包中包含jar，或者jar包中包含jar包里面的class文件，那么会使用 !/ 分隔开，这种方式只有org.springframework.boot.loader.jar.Handler能处理，它是SpringBoot内部扩展出来的一种URL协议。 JarLauncher的执行过程JarLauncher的main方法： 1234public static void main(String[] args) &#123; // 构造JarLauncher，然后调用它的launch方法。参数是控制台传递的 new JarLauncher().launch(args);&#125; JarLauncher被构造的时候会调用父类ExecutableArchiveLauncher的构造方法。 ExecutableArchiveLauncher的构造方法内部会去构造Archive，这里构造了JarFileArchive。构造JarFileArchive的过程中还会构造很多东西，比如JarFile，Entry … JarLauncher的launch方法：123456789101112131415161718192021222324252627282930313233343536373839404142434445protected void launch(String[] args) &#123; try &#123; // 在系统属性中设置注册了自定义的URL处理器：org.springframework.boot.loader.jar.Handler。如果URL中没有指定处理器，会去系统属性中查询 JarFile.registerUrlProtocolHandler(); // getClassPathArchives方法在会去找lib目录下对应的第三方依赖JarFileArchive，同时也会项目自身的JarFileArchive // 根据getClassPathArchives得到的JarFileArchive集合去创建类加载器ClassLoader。这里会构造一个LaunchedURLClassLoader类加载器，这个类加载器继承URLClassLoader，并使用这些JarFileArchive集合的URL构造成URLClassPath // LaunchedURLClassLoader类加载器的父类加载器是当前执行类JarLauncher的类加载器 ClassLoader classLoader = createClassLoader(getClassPathArchives()); // getMainClass方法会去项目自身的Archive中的Manifest中找出key为Start-Class的类 // 调用重载方法launch launch(args, getMainClass(), classLoader); &#125; catch (Exception ex) &#123; ex.printStackTrace(); System.exit(1); &#125;&#125;// Archive的getMainClass方法// 这里会找出spring.study.executablejar.ExecutableJarApplication这个类public String getMainClass() throws Exception &#123; Manifest manifest = getManifest(); String mainClass = null; if (manifest != null) &#123; mainClass = manifest.getMainAttributes().getValue(\"Start-Class\"); &#125; if (mainClass == null) &#123; throw new IllegalStateException( \"No 'Start-Class' manifest entry specified in \" + this); &#125; return mainClass;&#125;// launch重载方法protected void launch(String[] args, String mainClass, ClassLoader classLoader) throws Exception &#123; // 创建一个MainMethodRunner，并把args和Start-Class传递给它 Runnable runner = createMainMethodRunner(mainClass, args, classLoader); // 构造新线程 Thread runnerThread = new Thread(runner); // 线程设置类加载器以及名字，然后启动 runnerThread.setContextClassLoader(classLoader); runnerThread.setName(Thread.currentThread().getName()); runnerThread.start();&#125; MainMethodRunner的run方法： 12345678910111213141516171819202122232425@Overridepublic void run() &#123; try &#123; // 根据Start-Class进行实例化 Class&lt;?&gt; mainClass = Thread.currentThread().getContextClassLoader() .loadClass(this.mainClassName); // 找出main方法 Method mainMethod = mainClass.getDeclaredMethod(\"main\", String[].class); // 如果main方法不存在，抛出异常 if (mainMethod == null) &#123; throw new IllegalStateException( this.mainClassName + \" does not have a main method\"); &#125; // 调用 mainMethod.invoke(null, new Object[] &#123; this.args &#125;); &#125; catch (Exception ex) &#123; UncaughtExceptionHandler handler = Thread.currentThread() .getUncaughtExceptionHandler(); if (handler != null) &#123; handler.uncaughtException(Thread.currentThread(), ex); &#125; throw new RuntimeException(ex); &#125;&#125; Start-Class的main方法调用之后，内部会构造Spring容器，启动内置Servlet容器等过程。 这些过程我们都已经分析过了。 关于自定义的类加载器LaunchedURLClassLoaderLaunchedURLClassLoader重写了loadClass方法，也就是说它修改了默认的类加载方式(先看该类是否已加载这部分不变，后面真正去加载类的规则改变了，不再是直接从父类加载器中去加载)。LaunchedURLClassLoader定义了自己的类加载规则： 12345678910111213141516171819202122232425private Class&lt;?&gt; doLoadClass(String name) throws ClassNotFoundException &#123; // 1) Try the root class loader try &#123; if (this.rootClassLoader != null) &#123; return this.rootClassLoader.loadClass(name); &#125; &#125; catch (Exception ex) &#123; // Ignore and continue &#125; // 2) Try to find locally try &#123; findPackage(name); Class&lt;?&gt; cls = findClass(name); return cls; &#125; catch (Exception ex) &#123; // Ignore and continue &#125; // 3) Use standard loading return super.loadClass(name, false);&#125; 加载规则： 如果根类加载器存在，调用它的加载方法。这里是根类加载是ExtClassLoader 调用LaunchedURLClassLoader自身的findClass方法，也就是URLClassLoader的findClass方法 调用父类的loadClass方法，也就是执行默认的类加载顺序(从BootstrapClassLoader开始从下往下寻找) LaunchedURLClassLoader自身的findClass方法： 12345678910111213141516171819202122232425262728protected Class&lt;?&gt; findClass(final String name) throws ClassNotFoundException&#123; try &#123; return AccessController.doPrivileged( new PrivilegedExceptionAction&lt;Class&lt;?&gt;&gt;() &#123; public Class&lt;?&gt; run() throws ClassNotFoundException &#123; // 把类名解析成路径并加上.class后缀 String path = name.replace('.', '/').concat(\".class\"); // 基于之前得到的第三方jar包依赖以及自己的jar包得到URL数组，进行遍历找出对应类名的资源 // 比如path是org/springframework/boot/loader/JarLauncher.class，它在jar:file:/Users/Format/Develop/gitrepository/springboot-analysis/springboot-executable-jar/target/executable-jar-1.0-SNAPSHOT.jar!/lib/spring-boot-loader-1.3.5.RELEASE.jar!/中被找出 // 那么找出的资源对应的URL为jar:file:/Users/Format/Develop/gitrepository/springboot-analysis/springboot-executable-jar/target/executable-jar-1.0-SNAPSHOT.jar!/lib/spring-boot-loader-1.3.5.RELEASE.jar!/org/springframework/boot/loader/JarLauncher.class Resource res = ucp.getResource(path, false); if (res != null) &#123; // 找到了资源 try &#123; return defineClass(name, res); &#125; catch (IOException e) &#123; throw new ClassNotFoundException(name, e); &#125; &#125; else &#123; // 找不到资源的话直接抛出ClassNotFoundException异常 throw new ClassNotFoundException(name); &#125; &#125; &#125;, acc); &#125; catch (java.security.PrivilegedActionException pae) &#123; throw (ClassNotFoundException) pae.getException(); &#125;&#125; 下面是LaunchedURLClassLoader的一个测试： 12345678910111213141516// 注册org.springframework.boot.loader.jar.Handler URL协议处理器JarFile.registerUrlProtocolHandler();// 构造LaunchedURLClassLoader类加载器，这里使用了2个URL，分别对应jar包中依赖包spring-boot-loader和spring-boot，使用 \"!/\" 分开，需要org.springframework.boot.loader.jar.Handler处理器处理LaunchedURLClassLoader classLoader = new LaunchedURLClassLoader( new URL[] &#123; new URL(\"jar:file:/Users/Format/Develop/gitrepository/springboot-analysis/springboot-executable-jar/target/executable-jar-1.0-SNAPSHOT.jar!/lib/spring-boot-loader-1.3.5.RELEASE.jar!/\") , new URL(\"jar:file:/Users/Format/Develop/gitrepository/springboot-analysis/springboot-executable-jar/target/executable-jar-1.0-SNAPSHOT.jar!/lib/spring-boot-1.3.5.RELEASE.jar!/\") &#125;, LaunchedURLClassLoaderTest.class.getClassLoader());// 加载类// 这2个类都会在第二步本地查找中被找出(URLClassLoader的findClass方法)classLoader.loadClass(\"org.springframework.boot.loader.JarLauncher\");classLoader.loadClass(\"org.springframework.boot.SpringApplication\");// 在第三步使用默认的加载顺序在ApplicationClassLoader中被找出classLoader.loadClass(\"org.springframework.boot.autoconfigure.web.DispatcherServletAutoConfiguration\"); Spring Boot Loader的作用SpringBoot在可执行jar包中定义了自己的一套规则，比如第三方依赖jar包在/lib目录下，jar包的URL路径使用自定义的规则并且这个规则需要使用org.springframework.boot.loader.jar.Handler处理器处理。它的Main-Class使用JarLauncher，如果是war包，使用WarLauncher执行。这些Launcher内部都会另起一个线程启动自定义的SpringApplication类。 这些特性通过spring-boot-maven-plugin插件打包完成。","raw":null,"content":null,"categories":[{"name":"springboot","slug":"springboot","permalink":"http://fangjian0423.github.io/categories/springboot/"}],"tags":[{"name":"java","slug":"java","permalink":"http://fangjian0423.github.io/tags/java/"},{"name":"springboot","slug":"springboot","permalink":"http://fangjian0423.github.io/tags/springboot/"},{"name":"springboot源码分析","slug":"springboot源码分析","permalink":"http://fangjian0423.github.io/tags/springboot源码分析/"}]},{"title":"SpringData ES 关于字段名和索引中的列名字不一致导致的查询问题","slug":"spring-data-es-query-problem","date":"2017-05-23T16:33:33.000Z","updated":"2017-05-31T13:09:22.000Z","comments":true,"path":"2017/05/24/spring-data-es-query-problem/","link":"","permalink":"http://fangjian0423.github.io/2017/05/24/spring-data-es-query-problem/","excerpt":"最近工作中使用了Spring Data Elasticsearch。发生它存在一个问题：\nDocument对应的POJO的属性跟es里面文档的字段名字不一样，这样Repository里面编写自定义的查询方法就会查询不出结果。\n比如有个Person类，它有2个属性goodFace和goodAt。这2个属性在es的索引里对应的字段表为good_face和good_at：\n1234567891011@Document(replicas = 1, shards = 1, type = \"person\", indexName = \"person\")@Getter@Setter@JsonNaming(PropertyNamingStrategy.SnakeCaseStrategy.class)public class Person &#123;    @Id    private String id;    private String name;    private boolean goodFace;    private String goodAt;&#125;\nRepository中的自定义查询：\n12345@Repositorypublic interface PersonRepository extends ElasticsearchRepository&lt;Person, String&gt; &#123;    List&lt;Person&gt; findByGoodFace(boolean isGoodFace);    List&lt;Person&gt; findByName(String name);&#125;\n方法findByGoodFace是查询不出结果的，而findByName是ok的。\n为什么findByGoodFace不行而findByName可以呢，来探究一下。","text":"最近工作中使用了Spring Data Elasticsearch。发生它存在一个问题： Document对应的POJO的属性跟es里面文档的字段名字不一样，这样Repository里面编写自定义的查询方法就会查询不出结果。 比如有个Person类，它有2个属性goodFace和goodAt。这2个属性在es的索引里对应的字段表为good_face和good_at： 1234567891011@Document(replicas = 1, shards = 1, type = \"person\", indexName = \"person\")@Getter@Setter@JsonNaming(PropertyNamingStrategy.SnakeCaseStrategy.class)public class Person &#123; @Id private String id; private String name; private boolean goodFace; private String goodAt;&#125; Repository中的自定义查询： 12345@Repositorypublic interface PersonRepository extends ElasticsearchRepository&lt;Person, String&gt; &#123; List&lt;Person&gt; findByGoodFace(boolean isGoodFace); List&lt;Person&gt; findByName(String name);&#125; 方法findByGoodFace是查询不出结果的，而findByName是ok的。 为什么findByGoodFace不行而findByName可以呢，来探究一下。 Person类的name属性跟ES中的字段名是一模一样的，而goodFace字段在ES中的字段是good_face(因为我们使用了SnakeCaseStrategy策略)。 所以产生这个问题的原因在于ES中文档的字段名跟POJO中的字段名不统一造成的。 但是我们使用PersonRepository的save方法保存文档的时候属性和字段是可以对上的。 那为什么使用repository的save方法能对应上文档和字段，而自定义的find方法却不行呢？ ES是使用jackson来完成POJO到json的映射关系的。 在Person类上使用@JsonNaming注解完成POJO和json的映射，我们使用了SnakeCaseStrategy策略，这个策略会把属性从驼峰方式改成小写带下划线的方式。 比如goodAt属性映射的时候就会变成good_at，good_face变成good_face，name变成name。 Spring Data Elasticsearch把对ES的操作封装成了一个ElasticsearchOperations接口。比如queryForObject、queryForPage、count、queryForList方法。 ElasticsearchOperations接口目前有一个实现类ElasticsearchTemplate。 ElasticsearchTemplate内部有个ResultsMapper属性，这个ResultsMapper目前只有一个实现类DefaultResultMapper，DefaultResultMapper内部使用DefaultEntityMapper完成映射。DefaultEntityMapper是个EntityMapper接口的实现类，它的定义如下： 1234public interface EntityMapper &#123; public String mapToString(Object object) throws IOException; public &lt;T&gt; T mapToObject(String source, Class&lt;T&gt; clazz) throws IOException;&#125; 方法很明白：对象到json字符串的转换和json字符串倒对象的转换。 DefaultEntityMapper内部使用jackson的ObjectMapper完成。 自定义的Repository继承自ElasticsearchRepository，最后会使用代理映射成SimpleElasticsearchRepository。 SimpleElasticsearchRepository内部有个属性ElasticsearchOperations用于完成与ES的交互。 我们看下SimpleElasticsearchRepository的save方法： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061@Overridepublic &lt;S extends T&gt; S save(S entity) &#123; Assert.notNull(entity, \"Cannot save 'null' entity.\"); // createIndexQuery方法会构造一个IndexQuery，然后调用ElasticsearchOperations的index方法 elasticsearchOperations.index(createIndexQuery(entity)); elasticsearchOperations.refresh(entityInformation.getIndexName()); return entity;&#125;// ElasticsearchTemplate的index方法@Overridepublic String index(IndexQuery query) &#123; // 调用prepareIndex方法构造一个IndexRequestBuilder String documentId = prepareIndex(query).execute().actionGet().getId(); // 设置保存文档的id if (query.getObject() != null) &#123; setPersistentEntityId(query.getObject(), documentId); &#125; return documentId;&#125;private IndexRequestBuilder prepareIndex(IndexQuery query) &#123; try &#123; // 从@Document注解中得到索引的名字 String indexName = isBlank(query.getIndexName()) ? retrieveIndexNameFromPersistentEntity(query.getObject() .getClass())[0] : query.getIndexName(); // 从@Document注解中得到索引的类型 String type = isBlank(query.getType()) ? retrieveTypeFromPersistentEntity(query.getObject().getClass())[0] : query.getType(); IndexRequestBuilder indexRequestBuilder = null; if (query.getObject() != null) &#123; // save方法这里保存的object就是POJO // 得到id字段 String id = isBlank(query.getId()) ? getPersistentEntityId(query.getObject()) : query.getId(); if (id != null) &#123; // 如果设置了id字段 indexRequestBuilder = client.prepareIndex(indexName, type, id); &#125; else &#123; // 如果没有设置id字段 indexRequestBuilder = client.prepareIndex(indexName, type); &#125; // 使用ResultsMapper映射POJO到json字符串 indexRequestBuilder.setSource(resultsMapper.getEntityMapper().mapToString(query.getObject())); &#125; else if (query.getSource() != null) &#123; // 如果自定义了source属性，直接赋值 indexRequestBuilder = client.prepareIndex(indexName, type, query.getId()).setSource(query.getSource()); &#125; else &#123; // 没有设置object属性或者source属性，抛出ElasticsearchException异常 throw new ElasticsearchException(\"object or source is null, failed to index the document [id: \" + query.getId() + \"]\"); &#125; if (query.getVersion() != null) &#123; // 设置版本 indexRequestBuilder.setVersion(query.getVersion()); indexRequestBuilder.setVersionType(EXTERNAL); &#125; if (query.getParentId() != null) &#123; // 设置parentId indexRequestBuilder.setParent(query.getParentId()); &#125; return indexRequestBuilder; &#125; catch (IOException e) &#123; throw new ElasticsearchException(\"failed to index the document [id: \" + query.getId() + \"]\", e); &#125;&#125; save方法使用ResultsMapper完成了POJO到json的转换，所以save方法保存成功对应的文档数据： 1indexRequestBuilder.setSource(resultsMapper.getEntityMapper().mapToString(query.getObject())); 自定义的findByGoodFace方法： 由于是Repository中的自定义方法，会被Spring Data通过代理进行构造，内部还是用了AOP，最终在QueryExecutorMethodInterceptor中并解析成ElasticsearchPartQuery这个RepositoryQuery接口的实现类，然后调用execute方法： 123456789101112131415161718192021222324252627282930313233@Overridepublic Object execute(Object[] parameters) &#123; ParametersParameterAccessor accessor = new ParametersParameterAccessor(queryMethod.getParameters(), parameters); CriteriaQuery query = createQuery(accessor); if(tree.isDelete()) &#123; // 如果是删除方法 Object result = countOrGetDocumentsForDelete(query, accessor); elasticsearchOperations.delete(query, queryMethod.getEntityInformation().getJavaType()); return result; &#125; else if (queryMethod.isPageQuery()) &#123; // 如果是分页查询 query.setPageable(accessor.getPageable()); return elasticsearchOperations.queryForPage(query, queryMethod.getEntityInformation().getJavaType()); &#125; else if (queryMethod.isStreamQuery()) &#123; // 如果是流式查询 Class&lt;?&gt; entityType = queryMethod.getEntityInformation().getJavaType(); if (query.getPageable() == null) &#123; query.setPageable(new PageRequest(0, 20)); &#125; return StreamUtils.createStreamFromIterator((CloseableIterator&lt;Object&gt;) elasticsearchOperations.stream(query, entityType)); &#125; else if (queryMethod.isCollectionQuery()) &#123; // 如果是集合查询 if (accessor.getPageable() == null) &#123; int itemCount = (int) elasticsearchOperations.count(query, queryMethod.getEntityInformation().getJavaType()); query.setPageable(new PageRequest(0, Math.max(1, itemCount))); &#125; else &#123; query.setPageable(accessor.getPageable()); &#125; return elasticsearchOperations.queryForList(query, queryMethod.getEntityInformation().getJavaType()); &#125; else if (tree.isCountProjection()) &#123; // 如果是count查询 return elasticsearchOperations.count(query, queryMethod.getEntityInformation().getJavaType()); &#125; // 单个查询 return elasticsearchOperations.queryForObject(query, queryMethod.getEntityInformation().getJavaType());&#125; findByGoodFace方法是个集合查询，最终会调用ElasticsearchOperations的queryForList方法： 12345678910111213141516171819202122232425262728293031323334@Overridepublic &lt;T&gt; List&lt;T&gt; queryForList(CriteriaQuery query, Class&lt;T&gt; clazz) &#123; // 调用queryForPage方法 return queryForPage(query, clazz).getContent();&#125;@Overridepublic &lt;T&gt; Page&lt;T&gt; queryForPage(CriteriaQuery criteriaQuery, Class&lt;T&gt; clazz) &#123; // 查询解析器进行语法的解析 QueryBuilder elasticsearchQuery = new CriteriaQueryProcessor().createQueryFromCriteria(criteriaQuery.getCriteria()); QueryBuilder elasticsearchFilter = new CriteriaFilterProcessor().createFilterFromCriteria(criteriaQuery.getCriteria()); SearchRequestBuilder searchRequestBuilder = prepareSearch(criteriaQuery, clazz); if (elasticsearchQuery != null) &#123; searchRequestBuilder.setQuery(elasticsearchQuery); &#125; else &#123; searchRequestBuilder.setQuery(QueryBuilders.matchAllQuery()); &#125; if (criteriaQuery.getMinScore() &gt; 0) &#123; searchRequestBuilder.setMinScore(criteriaQuery.getMinScore()); &#125; if (elasticsearchFilter != null) searchRequestBuilder.setPostFilter(elasticsearchFilter); if (logger.isDebugEnabled()) &#123; logger.debug(\"doSearch query:\\n\" + searchRequestBuilder.toString()); &#125; SearchResponse response = getSearchResponse(searchRequestBuilder .execute()); // 最终的结果是用ResultsMapper进行映射 return resultsMapper.mapResults(response, clazz, criteriaQuery.getPageable());&#125; 自定义的方法使用ElasticsearchQueryCreator去创建CriteriaQuery，内部做一些词法的分析，有了CriteriaQuery之后，使用CriteriaQueryProcessor基于Criteria构造了QueryBuilder，最后使用QueryBuilder去做rest请求得到es的查询结果。这些过程中是没有用到ResultsMapper，而只是用反射得到POJO的属性，只有在得到查询结果后才会用ResultsMapper去做映射。 如果出现了这种情况，解决方案目前有两种： 1.使用repository的search方法，参数可以是QueryBuilder或者SearchQuery 1234personRepository.search( QueryBuilders.boolQuery() .must(QueryBuilders.termQuery(\"good_face\", true))) 2.使用@Query注解 12@Query(\"&#123;\\\"bool\\\" : &#123;\\\"must\\\" : &#123;\\\"term\\\" : &#123;\\\"good_face\\\" : \\\"?0\\\"&#125;&#125;&#125;&#125;\")List&lt;Person&gt; findByGoodFace(boolean isGoodFace); 暂时发现这两种解决方法，不知还有否更好的解决方案。","raw":null,"content":null,"categories":[{"name":"java","slug":"java","permalink":"http://fangjian0423.github.io/categories/java/"}],"tags":[{"name":"java","slug":"java","permalink":"http://fangjian0423.github.io/tags/java/"},{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://fangjian0423.github.io/tags/elasticsearch/"}]},{"title":"SpringBoot源码分析之内置Servlet容器","slug":"springboot-embedded-servlet-container","date":"2017-05-22T10:35:36.000Z","updated":"2017-05-22T12:51:52.000Z","comments":true,"path":"2017/05/22/springboot-embedded-servlet-container/","link":"","permalink":"http://fangjian0423.github.io/2017/05/22/springboot-embedded-servlet-container/","excerpt":"SpringBoot内置了Servlet容器，这样项目的发布、部署就不需要额外的Servlet容器，直接启动jar包即可。SpringBoot官方文档上有一个小章节内置servlet容器支持用于说明内置Servlet的相关问题。\n在SpringBoot源码分析之SpringBoot的启动过程文章中我们了解到如果是Web程序，那么会构造AnnotationConfigEmbeddedWebApplicationContext类型的Spring容器，在SpringBoot源码分析之Spring容器的refresh过程文章中我们知道AnnotationConfigEmbeddedWebApplicationContext类型的Spring容器在refresh的过程中会在onRefresh方法中创建内置的Servlet容器。\n接下来，我们分析一下内置的Servlet容器相关的知识点。","text":"SpringBoot内置了Servlet容器，这样项目的发布、部署就不需要额外的Servlet容器，直接启动jar包即可。SpringBoot官方文档上有一个小章节内置servlet容器支持用于说明内置Servlet的相关问题。 在SpringBoot源码分析之SpringBoot的启动过程文章中我们了解到如果是Web程序，那么会构造AnnotationConfigEmbeddedWebApplicationContext类型的Spring容器，在SpringBoot源码分析之Spring容器的refresh过程文章中我们知道AnnotationConfigEmbeddedWebApplicationContext类型的Spring容器在refresh的过程中会在onRefresh方法中创建内置的Servlet容器。 接下来，我们分析一下内置的Servlet容器相关的知识点。 内置Servlet容器相关的接口和类SpringBoot对内置的Servlet容器做了一层封装： public interface EmbeddedServletContainer { // 启动内置的Servlet容器，如果容器已经启动，则不影响 void start() throws EmbeddedServletContainerException; // 关闭内置的Servlet容器，如果容器已经关系，则不影响 void stop() throws EmbeddedServletContainerException; // 内置的Servlet容器监听的端口 int getPort(); } 它目前有3个实现类，分别是JettyEmbeddedServletContainer、TomcatEmbeddedServletContainer和UndertowEmbeddedServletContainer，分别对应Jetty、Tomcat和Undertow这3个Servlet容器。 EmbeddedServletContainerFactory接口是一个工厂接口，用于生产EmbeddedServletContainer： public interface EmbeddedServletContainerFactory { // 获得一个已经配置好的内置Servlet容器，但是这个容器还没有监听端口。需要手动调用内置Servlet容器的start方法监听端口 // 参数是一群ServletContextInitializer，Servlet容器启动的时候会遍历这些ServletContextInitializer，并调用onStartup方法 EmbeddedServletContainer getEmbeddedServletContainer( ServletContextInitializer... initializers); } ServletContextInitializer表示Servlet初始化器，用于设置ServletContext中的一些配置，在使用EmbeddedServletContainerFactory接口的getEmbeddedServletContainer方法获取Servlet内置容器并且容器启动的时候调用onStartup方法： public interface ServletContextInitializer { void onStartup(ServletContext servletContext) throws ServletException; } EmbeddedServletContainerFactory是在EmbeddedServletContainerAutoConfiguration这个自动化配置类中被注册到Spring容器中的(前期是Spring容器中不存在EmbeddedServletContainerFactory类型的bean，可以自己定义EmbeddedServletContainerFactory类型的bean)： @AutoConfigureOrder(Ordered.HIGHEST_PRECEDENCE) @Configuration @ConditionalOnWebApplication // 在Web环境下才会起作用 @Import(BeanPostProcessorsRegistrar.class) // 会Import一个内部类BeanPostProcessorsRegistrar public class EmbeddedServletContainerAutoConfiguration { @Configuration // Tomcat类和Servlet类必须在classloader中存在 @ConditionalOnClass({ Servlet.class, Tomcat.class }) // 当前Spring容器中不存在EmbeddedServletContainerFactory类型的实例 @ConditionalOnMissingBean(value = EmbeddedServletContainerFactory.class, search = SearchStrategy.CURRENT) public static class EmbeddedTomcat { @Bean public TomcatEmbeddedServletContainerFactory tomcatEmbeddedServletContainerFactory() { // 上述条件注解成立的话就会构造TomcatEmbeddedServletContainerFactory这个EmbeddedServletContainerFactory return new TomcatEmbeddedServletContainerFactory(); } } @Configuration // Server类、Servlet类、Loader类以及WebAppContext类必须在classloader中存在 @ConditionalOnClass({ Servlet.class, Server.class, Loader.class, WebAppContext.class }) // 当前Spring容器中不存在EmbeddedServletContainerFactory类型的实例 @ConditionalOnMissingBean(value = EmbeddedServletContainerFactory.class, search = SearchStrategy.CURRENT) public static class EmbeddedJetty { @Bean public JettyEmbeddedServletContainerFactory jettyEmbeddedServletContainerFactory() { // 上述条件注解成立的话就会构造JettyEmbeddedServletContainerFactory这个EmbeddedServletContainerFactory return new JettyEmbeddedServletContainerFactory(); } } @Configuration // Undertow类、Servlet类、以及SslClientAuthMode类必须在classloader中存在 @ConditionalOnClass({ Servlet.class, Undertow.class, SslClientAuthMode.class }) // 当前Spring容器中不存在EmbeddedServletContainerFactory类型的实例 @ConditionalOnMissingBean(value = EmbeddedServletContainerFactory.class, search = SearchStrategy.CURRENT) public static class EmbeddedUndertow { @Bean public UndertowEmbeddedServletContainerFactory undertowEmbeddedServletContainerFactory() { // 上述条件注解成立的话就会构造JettyEmbeddedServletContainerFactory这个EmbeddedServletContainerFactory return new UndertowEmbeddedServletContainerFactory(); } } // 在EmbeddedServletContainerAutoConfiguration自动化配置类中被导入，实现了BeanFactoryAware接口(BeanFactory会被自动注入进来)和ImportBeanDefinitionRegistrar接口(会被ConfigurationClassBeanDefinitionReader解析并注册到Spring容器中) public static class EmbeddedServletContainerCustomizerBeanPostProcessorRegistrar implements ImportBeanDefinitionRegistrar, BeanFactoryAware { private ConfigurableListableBeanFactory beanFactory; @Override public void setBeanFactory(BeanFactory beanFactory) throws BeansException { if (beanFactory instanceof ConfigurableListableBeanFactory) { this.beanFactory = (ConfigurableListableBeanFactory) beanFactory; } } @Override public void registerBeanDefinitions(AnnotationMetadata importingClassMetadata, BeanDefinitionRegistry registry) { if (this.beanFactory == null) { return; } // 如果Spring容器中不存在EmbeddedServletContainerCustomizerBeanPostProcessor类型的bean if (ObjectUtils.isEmpty(this.beanFactory.getBeanNamesForType( EmbeddedServletContainerCustomizerBeanPostProcessor.class, true, false))) { // 注册一个EmbeddedServletContainerCustomizerBeanPostProcessor registry.registerBeanDefinition( &quot;embeddedServletContainerCustomizerBeanPostProcessor&quot;, new RootBeanDefinition( EmbeddedServletContainerCustomizerBeanPostProcessor.class)); } } } } EmbeddedServletContainerCustomizerBeanPostProcessor是一个BeanPostProcessor，它在postProcessBeforeInitialization过程中去寻找Spring容器中EmbeddedServletContainerCustomizer类型的bean，并依次调用EmbeddedServletContainerCustomizer接口的customize方法做一些定制化： @Override public Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException { // 在Spring容器中寻找ConfigurableEmbeddedServletContainer类型的bean，SpringBoot内部的3种内置Servlet容器工厂都实现了这个接口，该接口的作用就是进行Servlet容器的配置 // 比如添加Servlet初始化器addInitializers、添加错误页addErrorPages、设置session超时时间setSessionTimeout、设置端口setPort等等 if (bean instanceof ConfigurableEmbeddedServletContainer) { postProcessBeforeInitialization((ConfigurableEmbeddedServletContainer) bean); } return bean; } private void postProcessBeforeInitialization( ConfigurableEmbeddedServletContainer bean) { for (EmbeddedServletContainerCustomizer customizer : getCustomizers()) { // 遍历获取的每个定制化器，并调用customize方法进行一些定制 customizer.customize(bean); } } private Collection&lt;EmbeddedServletContainerCustomizer&gt; getCustomizers() { if (this.customizers == null) { this.customizers = new ArrayList&lt;EmbeddedServletContainerCustomizer&gt;( // 找出Spring容器中EmbeddedServletContainerCustomizer类型的bean this.applicationContext .getBeansOfType(EmbeddedServletContainerCustomizer.class, false, false) .values()); // 定制化器做排序 Collections.sort(this.customizers, AnnotationAwareOrderComparator.INSTANCE); // 设置定制化器到属性中 this.customizers = Collections.unmodifiableList(this.customizers); } return this.customizers; } SpringBoot内置了一些EmbeddedServletContainerCustomizer，比如ErrorPageCustomizer、ServerProperties、TomcatWebSocketContainerCustomizer等。 定制器比如ServerProperties表示服务端的一些配置，以server为前缀，比如有server.port、server.contextPath、server.displayName等，它同时也实现了EmbeddedServletContainerCustomizer接口，其中customize方法的一部分代码如下： @Override public void customize(ConfigurableEmbeddedServletContainer container) { // 3种ServletContainerFactory都实现了ConfigurableEmbeddedServletContainer接口，所以下面的这些设置相当于对ServletContainerFactory进行设置 // 如果配置了端口信息 if (getPort() != null) { container.setPort(getPort()); } ... // 如果配置了displayName if (getDisplayName() != null) { container.setDisplayName(getDisplayName()); } // 如果配置了server.session.timeout，session超时时间。注意：这里的Session指的是ServerProperties的内部静态类Session if (getSession().getTimeout() != null) { container.setSessionTimeout(getSession().getTimeout()); } ... // 如果使用的是Tomcat内置Servlet容器，设置对应的Tomcat配置 if (container instanceof TomcatEmbeddedServletContainerFactory) { getTomcat().customizeTomcat(this, (TomcatEmbeddedServletContainerFactory) container); } // 如果使用的是Jetty内置Servlet容器，设置对应的Tomcat配置 if (container instanceof JettyEmbeddedServletContainerFactory) { getJetty().customizeJetty(this, (JettyEmbeddedServletContainerFactory) container); } // 如果使用的是Undertow内置Servlet容器，设置对应的Tomcat配置 if (container instanceof UndertowEmbeddedServletContainerFactory) { getUndertow().customizeUndertow(this, (UndertowEmbeddedServletContainerFactory) container); } // 添加SessionConfiguringInitializer这个Servlet初始化器 // SessionConfiguringInitializer初始化器的作用是基于ServerProperties的内部静态类Session设置Servlet中session和cookie的配置 container.addInitializers(new SessionConfiguringInitializer(this.session)); // 添加InitParameterConfiguringServletContextInitializer初始化器 // InitParameterConfiguringServletContextInitializer初始化器的作用是基于ServerProperties的contextParameters配置设置到ServletContext的init param中 container.addInitializers(new InitParameterConfiguringServletContextInitializer( getContextParameters())); } ErrorPageCustomizer在ErrorMvcAutoConfiguration自动化配置里定义，是个内部静态类： @Bean public ErrorPageCustomizer errorPageCustomizer() { return new ErrorPageCustomizer(this.properties); } private static class ErrorPageCustomizer implements EmbeddedServletContainerCustomizer, Ordered { private final ServerProperties properties; protected ErrorPageCustomizer(ServerProperties properties) { this.properties = properties; } @Override public void customize(ConfigurableEmbeddedServletContainer container) { // 添加错误页ErrorPage，这个ErrorPage对应的路径是 /error // 可以通过配置修改 ${servletPath} + ${error.path} container.addErrorPages(new ErrorPage(this.properties.getServletPrefix() + this.properties.getError().getPath())); } @Override public int getOrder() { return 0; } } DispatcherServlet的构造DispatcherServlet是SpringMVC中的核心分发器。它是在DispatcherServletAutoConfiguration这个自动化配置类里构造的(如果Spring容器内没有自定义的DispatcherServlet)，并且还会被加到Servlet容器中(通过ServletRegistrationBean完成)。 DispatcherServletAutoConfiguration这个自动化配置类存在2个条件注解@ConditionalOnWebApplication和@ConditionalOnClass(DispatcherServlet.class)都满足条件，所以会被构造(存在@AutoConfigureAfter(EmbeddedServletContainerAutoConfiguration.class)注解，会在EmbeddedServletContainerAutoConfiguration自动化配置类构造后构造)： @AutoConfigureOrder(Ordered.HIGHEST_PRECEDENCE) @Configuration @ConditionalOnWebApplication @ConditionalOnClass(DispatcherServlet.class) @AutoConfigureAfter(EmbeddedServletContainerAutoConfiguration.class) public class DispatcherServletAutoConfiguration ... DispatcherServletAutoConfiguration有个内部类DispatcherServletConfiguration，它会构造DispatcherServlet(使用了条件类DefaultDispatcherServletCondition，如果Spring容器已经存在自定义的DispatcherServlet类型的bean，该类就不会被构造，会直接使用自定义的DispatcherServlet)： @Configuration // 条件类DefaultDispatcherServletCondition，是EmbeddedServletContainerAutoConfiguration的内部类 // DefaultDispatcherServletCondition条件类会去Spring容器中找DispatcherServlet类型的实例，如果找到了不会构造DispatcherServletConfiguration，否则就是构造DispatcherServletConfiguration，该类内部会构造DispatcherServlet // 所以如果我们要自定义DispatcherServlet的话只需要自定义DispatcherServlet即可，这样DispatcherServletConfiguration内部就不会构造DispatcherServlet @Conditional(DefaultDispatcherServletCondition.class) // Servlet3.0开始才有的类，支持以编码的形式注册Servlet @ConditionalOnClass(ServletRegistration.class) // spring.mvc 为前缀的配置 @EnableConfigurationProperties(WebMvcProperties.class) protected static class DispatcherServletConfiguration { @Autowired private ServerProperties server; @Autowired private WebMvcProperties webMvcProperties; @Autowired(required = false) private MultipartConfigElement multipartConfig; // Spring容器注册DispatcherServlet @Bean(name = DEFAULT_DISPATCHER_SERVLET_BEAN_NAME) public DispatcherServlet dispatcherServlet() { // 直接构造DispatcherServlet，并设置WebMvcProperties中的一些配置 DispatcherServlet dispatcherServlet = new DispatcherServlet(); dispatcherServlet.setDispatchOptionsRequest( this.webMvcProperties.isDispatchOptionsRequest()); dispatcherServlet.setDispatchTraceRequest( this.webMvcProperties.isDispatchTraceRequest()); dispatcherServlet.setThrowExceptionIfNoHandlerFound( this.webMvcProperties.isThrowExceptionIfNoHandlerFound()); return dispatcherServlet; } @Bean(name = DEFAULT_DISPATCHER_SERVLET_REGISTRATION_BEAN_NAME) public ServletRegistrationBean dispatcherServletRegistration() { // 直接使用DispatcherServlet和server配置中的servletPath路径构造ServletRegistrationBean // ServletRegistrationBean实现了ServletContextInitializer接口，在onStartup方法中对应的Servlet注册到Servlet容器中 // 所以这里DispatcherServlet会被注册到Servlet容器中，对应的urlMapping为server.servletPath配置 ServletRegistrationBean registration = new ServletRegistrationBean( dispatcherServlet(), this.server.getServletMapping()); registration.setName(DEFAULT_DISPATCHER_SERVLET_BEAN_NAME); if (this.multipartConfig != null) { registration.setMultipartConfig(this.multipartConfig); } return registration; } @Bean // 构造文件上传相关的bean @ConditionalOnBean(MultipartResolver.class) @ConditionalOnMissingBean(name = DispatcherServlet.MULTIPART_RESOLVER_BEAN_NAME) public MultipartResolver multipartResolver(MultipartResolver resolver) { return resolver; } } ServletRegistrationBean实现了ServletContextInitializer接口，是个Servlet初始化器，onStartup方法代码： @Override public void onStartup(ServletContext servletContext) throws ServletException { Assert.notNull(this.servlet, &quot;Servlet must not be null&quot;); String name = getServletName(); if (!isEnabled()) { logger.info(&quot;Servlet &quot; + name + &quot; was not registered (disabled)&quot;); return; } logger.info(&quot;Mapping servlet: &apos;&quot; + name + &quot;&apos; to &quot; + this.urlMappings); // 把servlet添加到Servlet容器中，Servlet容器启动的时候会加载这个Servlet Dynamic added = servletContext.addServlet(name, this.servlet); if (added == null) { logger.info(&quot;Servlet &quot; + name + &quot; was not registered &quot; + &quot;(possibly already registered?)&quot;); return; } // 进行Servlet的一些配置，比如urlMapping，loadOnStartup等 configure(added); } 类似ServletRegistrationBean的还有ServletListenerRegistrationBean和FilterRegistrationBean，它们都是Servlet初始化器，分别都是在Servlet容器中添加Listener和Filter。 1个小漏洞：如果定义了一个名字为dispatcherServlet的bean，但是它不是DispatcherServlet类型，那么DispatcherServlet就不会被构造，@RestController和@Controller注解的控制器就没办法生效： @Bean(name = &quot;dispatcherServlet&quot;) public Object test() { return new Object(); } 内置Servlet容器的创建和启动web程序对应的Spring容器是AnnotationConfigEmbeddedWebApplicationContext，继承自EmbeddedWebApplicationContext。在onRefresh方法中会去创建内置Servlet容器： @Override protected void onRefresh() { super.onRefresh(); try { // 创建内置Servlet容器 createEmbeddedServletContainer(); } catch (Throwable ex) { throw new ApplicationContextException(&quot;Unable to start embedded container&quot;, ex); } } private void createEmbeddedServletContainer() { EmbeddedServletContainer localContainer = this.embeddedServletContainer; ServletContext localServletContext = getServletContext(); // 内置Servlet容器和ServletContext都还没初始化的时候执行 if (localContainer == null &amp;&amp; localServletContext == null) { // 从Spring容器中获取EmbeddedServletContainerFactory，如果EmbeddedServletContainerFactory不存在或者有多个的话会抛出异常中止程序 EmbeddedServletContainerFactory containerFactory = getEmbeddedServletContainerFactory(); // 获取Servlet初始化器并创建Servlet容器，依次调用Servlet初始化器中的onStartup方法 this.embeddedServletContainer = containerFactory .getEmbeddedServletContainer(getSelfInitializer()); } // 内置Servlet容器已经初始化但是ServletContext还没初始化的时候执行 else if (localServletContext != null) { try { // 对已经存在的Servlet 容器依次调用Servlet初始化器中的onStartup方法 getSelfInitializer().onStartup(localServletContext); } catch (ServletException ex) { throw new ApplicationContextException(&quot;Cannot initialize servlet context&quot;, ex); } } initPropertySources(); } getSelfInitializer方法获得的Servlet初始化器内部会去构造一个ServletContextInitializerBeans(Servlet初始化器的集合)，ServletContextInitializerBeans构造的时候会去Spring容器中查找ServletContextInitializer类型的bean，其中ServletRegistrationBean、FilterRegistrationBean、ServletListenerRegistrationBean会被找出(如果有定义)，这3种ServletContextInitializer会在onStartup方法中将Servlet、Filter、Listener添加到Servlet容器中(如果我们只定义了Servlet、Filter或者Listener，ServletContextInitializerBeans内部会调用addAdaptableBeans方法把它们包装成RegistrationBean)： // selfInitialize方法内部调用的getServletContextInitializerBeans方法获得ServletContextInitializerBeans protected Collection&lt;ServletContextInitializer&gt; getServletContextInitializerBeans() { return new ServletContextInitializerBeans(getBeanFactory()); } private void addServletContextInitializerBean(String beanName, ServletContextInitializer initializer, ListableBeanFactory beanFactory) { if (initializer instanceof ServletRegistrationBean) { Servlet source = ((ServletRegistrationBean) initializer).getServlet(); addServletContextInitializerBean(Servlet.class, beanName, initializer, beanFactory, source); } else if (initializer instanceof FilterRegistrationBean) { Filter source = ((FilterRegistrationBean) initializer).getFilter(); addServletContextInitializerBean(Filter.class, beanName, initializer, beanFactory, source); } else if (initializer instanceof DelegatingFilterProxyRegistrationBean) { String source = ((DelegatingFilterProxyRegistrationBean) initializer) .getTargetBeanName(); addServletContextInitializerBean(Filter.class, beanName, initializer, beanFactory, source); } else if (initializer instanceof ServletListenerRegistrationBean) { EventListener source = ((ServletListenerRegistrationBean&lt;?&gt;) initializer) .getListener(); addServletContextInitializerBean(EventListener.class, beanName, initializer, beanFactory, source); } else { addServletContextInitializerBean(ServletContextInitializer.class, beanName, initializer, beanFactory, null); } } Servlet容器创建完毕之后在finishRefresh方法中会去启动： @Override protected void finishRefresh() { super.finishRefresh(); // 调用startEmbeddedServletContainer方法 EmbeddedServletContainer localContainer = startEmbeddedServletContainer(); if (localContainer != null) { // 发布EmbeddedServletContainerInitializedEvent事件 publishEvent( new EmbeddedServletContainerInitializedEvent(this, localContainer)); } } private EmbeddedServletContainer startEmbeddedServletContainer() { // 先得到在onRefresh方法中构造的Servlet容器embeddedServletContainer EmbeddedServletContainer localContainer = this.embeddedServletContainer; if (localContainer != null) { // 启动 localContainer.start(); } return localContainer; } 自定义Servlet、Filter、ListenerSpringBoot默认只会添加一个Servlet，也就是DispatcherServlet，如果我们想添加自定义的Servlet或者是Filter还是Listener，有以下几种方法。 1.在Spring容器中声明ServletRegistrationBean、FilterRegistrationBean或者ServletListenerRegistrationBean。原理在DispatcherServlet的构造章节中已经说明 @Bean public ServletRegistrationBean customServlet() { return new ServletRegistrationBean(new CustomServlet(), &quot;/custom&quot;); } private static class CustomServlet extends HttpServlet { @Override protected void service(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException { resp.getWriter().write(&quot;receive by custom servlet&quot;); } } 2.@ServletComponentScan注解和@WebServlet、@WebFilter以及@WebListener注解配合使用。@ServletComponentScan注解启用ImportServletComponentScanRegistrar类，是个ImportBeanDefinitionRegistrar接口的实现类，会被Spring容器所解析。ServletComponentScanRegistrar内部会解析@ServletComponentScan注解，然后会在Spring容器中注册ServletComponentRegisteringPostProcessor，是个BeanFactoryPostProcessor，会去解析扫描出来的类是不是有@WebServlet、@WebListener、@WebFilter这3种注解，有的话把这3种类型的类转换成ServletRegistrationBean、FilterRegistrationBean或者ServletListenerRegistrationBean，然后让Spring容器去解析： @SpringBootApplication @ServletComponentScan public class EmbeddedServletApplication { ... } @WebServlet(urlPatterns = &quot;/simple&quot;) public class SimpleServlet extends HttpServlet { @Override protected void service(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException { resp.getWriter().write(&quot;receive by SimpleServlet&quot;); } } 3.在Spring容器中声明Servlet、Filter或者Listener。因为在ServletContextInitializerBeans内部会去调用addAdaptableBeans方法把它们包装成ServletRegistrationBean： @Bean(name = &quot;dispatcherServlet&quot;) public DispatcherServlet myDispatcherServlet() { return new DispatcherServlet(); } Whitelabel Error Page原理为什么SpringBoot的程序里Controller发生了错误，我们没有进行异常的捕捉，会跳转到Whitelabel Error Page页面，这是如何实现的？ SpringBoot內部提供了一个ErrorController叫做BasicErrorController，对应的@RequestMapping地址为 “server.error.path” 配置 或者 “error.path” 配置，这2个配置没配的话默认是/error，之前分析过ErrorPageCustomizer这个定制化器会把ErrorPage添加到Servlet容器中(这个ErrorPage的path就是上面说的那2个配置)，这样Servlet容器发生错误的时候就会访问ErrorPage配置的path，所以程序发生异常且没有被catch的话，就会走Servlet容器配置的ErrorPage。下面这段代码是BasicErrorController对应的处理请求方法： @RequestMapping(produces = &quot;text/html&quot;) public ModelAndView errorHtml(HttpServletRequest request, HttpServletResponse response) { // 设置响应码 response.setStatus(getStatus(request).value()); // 设置一些信息，比如timestamp、statusCode、错误message等 Map&lt;String, Object&gt; model = getErrorAttributes(request, isIncludeStackTrace(request, MediaType.TEXT_HTML)); // 返回error视图 return new ModelAndView(&quot;error&quot;, model); } 这里名字为error视图会被BeanNameViewResolver这个视图解析器解析，它会去Spring容器中找出name为error的View，error这个bean在ErrorMvcAutoConfiguration自动化配置类里定义，它返回了一个SpelView视图，也就是刚才见到的Whitelabel Error Page(error.whitelabel.enabled配置需要是true，否则WhitelabelErrorViewConfiguration自动化配置类不会被注册)： @Configuration @ConditionalOnProperty(prefix = &quot;server.error.whitelabel&quot;, name = &quot;enabled&quot;, matchIfMissing = true) @Conditional(ErrorTemplateMissingCondition.class) protected static class WhitelabelErrorViewConfiguration { // Whitelabel Error Page private final SpelView defaultErrorView = new SpelView( &quot;&lt;html&gt;&lt;body&gt;&lt;h1&gt;Whitelabel Error Page&lt;/h1&gt;&quot; + &quot;&lt;p&gt;This application has no explicit mapping for /error, so you are seeing this as a fallback.&lt;/p&gt;&quot; + &quot;&lt;div id=&apos;created&apos;&gt;${timestamp}&lt;/div&gt;&quot; + &quot;&lt;div&gt;There was an unexpected error (type=${error}, status=${status}).&lt;/div&gt;&quot; + &quot;&lt;div&gt;${message}&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;&quot;); @Bean(name = &quot;error&quot;) // bean的名字是error @ConditionalOnMissingBean(name = &quot;error&quot;) // 名字为error的bean不存在才会构造 public View defaultErrorView() { return this.defaultErrorView; } @Bean @ConditionalOnMissingBean(BeanNameViewResolver.class) public BeanNameViewResolver beanNameViewResolver() { // BeanNameViewResolver会去Spring容器找对应bean的视图 BeanNameViewResolver resolver = new BeanNameViewResolver(); resolver.setOrder(Ordered.LOWEST_PRECEDENCE - 10); return resolver; } } 如果自定义了error页面，比如使用freemarker模板的话存在/templates/error.ftl页面，使用thymeleaf模板的话存在/templates/error.html页面。那么Whitelabel Error Page就不会生效了，而是会跳到这些error页面。这又是如何实现的呢? 这是因为ErrorMvcAutoConfiguration自动化配置类里的内部类 WhitelabelErrorViewConfiguration自动化配置类里有个条件类ErrorTemplateMissingCondition，它的getMatchOutcome方法： @Override public ConditionOutcome getMatchOutcome(ConditionContext context, AnnotatedTypeMetadata metadata) { // 从spring.factories文件中找出key为TemplateAvailabilityProvider为类，TemplateAvailabilityProvider用来查询视图是否可用 List&lt;TemplateAvailabilityProvider&gt; availabilityProviders = SpringFactoriesLoader .loadFactories(TemplateAvailabilityProvider.class, context.getClassLoader()); // 遍历各个TemplateAvailabilityProvider for (TemplateAvailabilityProvider availabilityProvider : availabilityProviders) // 如果error视图可用 if (availabilityProvider.isTemplateAvailable(&quot;error&quot;, context.getEnvironment(), context.getClassLoader(), context.getResourceLoader())) { // 条件不生效。WhitelabelErrorViewConfiguration不会被构造 return ConditionOutcome.noMatch(&quot;Template from &quot; + availabilityProvider + &quot; found for error view&quot;); } } // 条件生效。WhitelabelErrorViewConfiguration被构造 return ConditionOutcome.match(&quot;No error template view detected&quot;); } 比如FreeMarkerTemplateAvailabilityProvider这个TemplateAvailabilityProvider的逻辑如下： public class FreeMarkerTemplateAvailabilityProvider implements TemplateAvailabilityProvider { @Override public boolean isTemplateAvailable(String view, Environment environment, ClassLoader classLoader, ResourceLoader resourceLoader) { // 判断是否存在freemarker包中的Configuration类，存在的话才会继续 if (ClassUtils.isPresent(&quot;freemarker.template.Configuration&quot;, classLoader)) { // 构造属性解析器 RelaxedPropertyResolver resolver = new RelaxedPropertyResolver(environment, &quot;spring.freemarker.&quot;); // 设置一些配置 String loaderPath = resolver.getProperty(&quot;template-loader-path&quot;, FreeMarkerProperties.DEFAULT_TEMPLATE_LOADER_PATH); String prefix = resolver.getProperty(&quot;prefix&quot;, FreeMarkerProperties.DEFAULT_PREFIX); String suffix = resolver.getProperty(&quot;suffix&quot;, FreeMarkerProperties.DEFAULT_SUFFIX); // 查找对应的资源文件是否存在 return resourceLoader.getResource(loaderPath + prefix + view + suffix) .exists(); } return false; } } 所以BeanNameViewResolver不会被构造，Whitelabel Error Page也不会构造，而是直接去找自定义的error视图。 一些测试代码： https://github.com/fangjian0423/springboot-analysis/tree/master/springboot-embedded-servlet-conatiner","raw":null,"content":null,"categories":[{"name":"springboot","slug":"springboot","permalink":"http://fangjian0423.github.io/categories/springboot/"}],"tags":[{"name":"java","slug":"java","permalink":"http://fangjian0423.github.io/tags/java/"},{"name":"springboot","slug":"springboot","permalink":"http://fangjian0423.github.io/tags/springboot/"},{"name":"springboot源码分析","slug":"springboot源码分析","permalink":"http://fangjian0423.github.io/tags/springboot源码分析/"}]},{"title":"SpringBoot源码分析之条件注解的底层实现","slug":"springboot-condition-annotation","date":"2017-05-16T11:40:13.000Z","updated":"2017-05-31T13:16:37.000Z","comments":true,"path":"2017/05/16/springboot-condition-annotation/","link":"","permalink":"http://fangjian0423.github.io/2017/05/16/springboot-condition-annotation/","excerpt":"SpringBoot内部提供了特有的注解：条件注解(Conditional Annotation)。比如@ConditionalOnBean、@ConditionalOnClass、@ConditionalOnExpression、@ConditionalOnMissingBean等。\n条件注解存在的意义在于动态识别(也可以说是代码自动化执行)。比如@ConditionalOnClass会检查类加载器中是否存在对应的类，如果有的话被注解修饰的类就有资格被Spring容器所注册，否则会被skip。\n比如FreemarkerAutoConfiguration这个自动化配置类的定义如下：\n123456@Configuration@ConditionalOnClass(&#123; freemarker.template.Configuration.class,\t\tFreeMarkerConfigurationFactory.class &#125;)@AutoConfigureAfter(WebMvcAutoConfiguration.class)@EnableConfigurationProperties(FreeMarkerProperties.class)public class FreeMarkerAutoConfiguration\n这个自动化配置类被@ConditionalOnClass条件注解修饰，这个条件注解存在的意义在于判断类加载器中是否存在freemarker.template.Configuration和FreeMarkerConfigurationFactory这两个类，如果都存在的话会在Spring容器中加载这个FreeMarkerAutoConfiguration配置类；否则不会加载。","text":"SpringBoot内部提供了特有的注解：条件注解(Conditional Annotation)。比如@ConditionalOnBean、@ConditionalOnClass、@ConditionalOnExpression、@ConditionalOnMissingBean等。 条件注解存在的意义在于动态识别(也可以说是代码自动化执行)。比如@ConditionalOnClass会检查类加载器中是否存在对应的类，如果有的话被注解修饰的类就有资格被Spring容器所注册，否则会被skip。 比如FreemarkerAutoConfiguration这个自动化配置类的定义如下： 123456@Configuration@ConditionalOnClass(&#123; freemarker.template.Configuration.class, FreeMarkerConfigurationFactory.class &#125;)@AutoConfigureAfter(WebMvcAutoConfiguration.class)@EnableConfigurationProperties(FreeMarkerProperties.class)public class FreeMarkerAutoConfiguration 这个自动化配置类被@ConditionalOnClass条件注解修饰，这个条件注解存在的意义在于判断类加载器中是否存在freemarker.template.Configuration和FreeMarkerConfigurationFactory这两个类，如果都存在的话会在Spring容器中加载这个FreeMarkerAutoConfiguration配置类；否则不会加载。 条件注解内部的一些基础在分析条件注解的底层实现之前，我们先来看一下这些条件注解的定义。以@ConditionalOnClass注解为例，它的定义如下： 12345678@Target(&#123; ElementType.TYPE, ElementType.METHOD &#125;)@Retention(RetentionPolicy.RUNTIME)@Documented@Conditional(OnClassCondition.class)public @interface ConditionalOnClass &#123; Class&lt;?&gt;[] value() default &#123;&#125;; // 需要匹配的类 String[] name() default &#123;&#125;; // 需要匹配的类名&#125; 它有2个属性，分别是类数组和字符串数组(作用一样，类型不一样)，而且被@Conditional注解所修饰，这个@Conditional注解有个名为values的Class&lt;? extends Condition&gt;[]类型的属性。 这个Condition是个接口，用于匹配组件是否有资格被容器注册，定义如下： 1234public interface Condition &#123; // ConditionContext内部会存储Spring容器、应用程序环境信息、资源加载器、类加载器 boolean matches(ConditionContext context, AnnotatedTypeMetadata metadata);&#125; 也就是说@Conditional注解属性中可以持有多个Condition接口的实现类，所有的Condition接口需要全部匹配成功后这个@Conditional修饰的组件才有资格被注册。 Condition接口有个子接口ConfigurationCondition： 123456789101112public interface ConfigurationCondition extends Condition &#123; ConfigurationPhase getConfigurationPhase(); public static enum ConfigurationPhase &#123; PARSE_CONFIGURATION, REGISTER_BEAN &#125;&#125; 这个子接口是一种特殊的条件接口，多了一个getConfigurationPhase方法，也就是条件注解的生效阶段。只有在ConfigurationPhase中定义的两种阶段下才会生效。 Condition接口有个实现抽象类SpringBootCondition，SpringBoot中所有条件注解对应的条件类都继承这个抽象类。它实现了matches方法： 12345678910111213141516171819202122232425@Overridepublic final boolean matches(ConditionContext context, AnnotatedTypeMetadata metadata) &#123; String classOrMethodName = getClassOrMethodName(metadata); // 得到类名或者方法名(条件注解可以作用的类或者方法上) try &#123; ConditionOutcome outcome = getMatchOutcome(context, metadata); // 抽象方法，具体子类实现。ConditionOutcome记录了匹配结果boolean和log信息 logOutcome(classOrMethodName, outcome); // log记录一下匹配信息 recordEvaluation(context, classOrMethodName, outcome); // 报告记录一下匹配信息 return outcome.isMatch(); // 返回是否匹配 &#125; catch (NoClassDefFoundError ex) &#123; throw new IllegalStateException( \"Could not evaluate condition on \" + classOrMethodName + \" due to \" + ex.getMessage() + \" not \" + \"found. Make sure your own configuration does not rely on \" + \"that class. This can also happen if you are \" + \"@ComponentScanning a springframework package (e.g. if you \" + \"put a @ComponentScan in the default package by mistake)\", ex); &#125; catch (RuntimeException ex) &#123; throw new IllegalStateException( \"Error processing condition on \" + getName(metadata), ex); &#125;&#125; 基于Class的条件注解SpringBoot提供了两个基于Class的条件注解：@ConditionalOnClass(类加载器中存在指明的类)或者@ConditionalOnMissingClass(类加载器中不存在指明的类)。 @ConditionalOnClass或者@ConditionalOnMissingClass注解对应的条件类是OnClassCondition，定义如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465@Order(Ordered.HIGHEST_PRECEDENCE) // 优先级、最高级别class OnClassCondition extends SpringBootCondition &#123; @Override public ConditionOutcome getMatchOutcome(ConditionContext context, AnnotatedTypeMetadata metadata) &#123; StringBuffer matchMessage = new StringBuffer(); // 记录匹配信息 MultiValueMap&lt;String, Object&gt; onClasses = getAttributes(metadata, ConditionalOnClass.class); // 得到@ConditionalOnClass注解的属性 if (onClasses != null) &#123; // 如果属性存在 List&lt;String&gt; missing = getMatchingClasses(onClasses, MatchType.MISSING, context); // 得到在类加载器中不存在的类 if (!missing.isEmpty()) &#123; // 如果存在类加载器中不存在对应的类，返回一个匹配失败的ConditionalOutcome return ConditionOutcome .noMatch(\"required @ConditionalOnClass classes not found: \" + StringUtils.collectionToCommaDelimitedString(missing)); &#125; // 如果类加载器中存在对应的类的话，匹配信息进行记录 matchMessage.append(\"@ConditionalOnClass classes found: \" + StringUtils.collectionToCommaDelimitedString( getMatchingClasses(onClasses, MatchType.PRESENT, context))); &#125; // 对@ConditionalOnMissingClass注解做相同的逻辑处理(说明@ConditionalOnClass和@ConditionalOnMissingClass可以一起使用) MultiValueMap&lt;String, Object&gt; onMissingClasses = getAttributes(metadata, ConditionalOnMissingClass.class); if (onMissingClasses != null) &#123; List&lt;String&gt; present = getMatchingClasses(onMissingClasses, MatchType.PRESENT, context); if (!present.isEmpty()) &#123; return ConditionOutcome .noMatch(\"required @ConditionalOnMissing classes found: \" + StringUtils.collectionToCommaDelimitedString(present)); &#125; matchMessage.append(matchMessage.length() == 0 ? \"\" : \" \"); matchMessage.append(\"@ConditionalOnMissing classes not found: \" + StringUtils.collectionToCommaDelimitedString(getMatchingClasses( onMissingClasses, MatchType.MISSING, context))); &#125; // 返回全部匹配成功的ConditionalOutcome return ConditionOutcome.match(matchMessage.toString()); &#125; private enum MatchType &#123; // 枚举：匹配类型。用于查询类名在对应的类加载器中是否存在。 PRESENT &#123; // 匹配成功 @Override public boolean matches(String className, ConditionContext context) &#123; return ClassUtils.isPresent(className, context.getClassLoader()); &#125; &#125;, MISSING &#123; // 匹配不成功 @Override public boolean matches(String className, ConditionContext context) &#123; return !ClassUtils.isPresent(className, context.getClassLoader()); &#125; &#125;; public abstract boolean matches(String className, ConditionContext context); &#125;&#125; 比如FreemarkerAutoConfiguration中的@ConditionalOnClass注解中有value属性是freemarker.template.Configuration.class和FreeMarkerConfigurationFactory.class。在OnClassCondition执行过程中得到的最终ConditionalOutcome中的log message如下： 1@ConditionalOnClass classes found: freemarker.template.Configuration,org.springframework.ui.freemarker.FreeMarkerConfigurationFactory 基于Bean的条件注解@ConditionalOnBean(Spring容器中存在指明的bean)、@ConditionalOnMissingBean(Spring容器中不存在指明的bean)以及ConditionalOnSingleCandidate(Spring容器中存在且只存在一个指明的bean)都是基于Bean的条件注解，它们对应的条件类是ConditionOnBean。 @ConditionOnBean注解定义如下： 1234567891011@Target(&#123; ElementType.TYPE, ElementType.METHOD &#125;)@Retention(RetentionPolicy.RUNTIME)@Documented@Conditional(OnBeanCondition.class)public @interface ConditionalOnBean &#123; Class&lt;?&gt;[] value() default &#123;&#125;; // 匹配的bean类型 String[] type() default &#123;&#125;; // 匹配的bean类型的类名 Class&lt;? extends Annotation&gt;[] annotation() default &#123;&#125;; // 匹配的bean注解 String[] name() default &#123;&#125;; // 匹配的bean的名字 SearchStrategy search() default SearchStrategy.ALL; // 搜索策略。提供CURRENT(只在当前容器中找)、PARENTS(只在所有的父容器中找；但是不包括当前容器)和ALL(CURRENT和PARENTS的组合)&#125; OnBeanCondition条件类的匹配代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445@Overridepublic ConditionOutcome getMatchOutcome(ConditionContext context, AnnotatedTypeMetadata metadata) &#123; StringBuffer matchMessage = new StringBuffer(); // 记录匹配信息 if (metadata.isAnnotated(ConditionalOnBean.class.getName())) &#123; BeanSearchSpec spec = new BeanSearchSpec(context, metadata, ConditionalOnBean.class); // 构造一个BeanSearchSpec，会从@ConditionalOnBean注解中获取属性，然后设置到BeanSearchSpec中 List&lt;String&gt; matching = getMatchingBeans(context, spec); // 从BeanFactory中根据策略找出所有匹配的bean if (matching.isEmpty()) &#123; // 如果没有匹配的bean，返回一个没有匹配成功的ConditionalOutcome return ConditionOutcome .noMatch(\"@ConditionalOnBean \" + spec + \" found no beans\"); &#125; // 如果找到匹配的bean，匹配信息进行记录 matchMessage.append( \"@ConditionalOnBean \" + spec + \" found the following \" + matching); &#125; if (metadata.isAnnotated(ConditionalOnSingleCandidate.class.getName())) &#123; // 相同的逻辑，针对@ConditionalOnSingleCandidate注解 BeanSearchSpec spec = new SingleCandidateBeanSearchSpec(context, metadata, ConditionalOnSingleCandidate.class); List&lt;String&gt; matching = getMatchingBeans(context, spec); if (matching.isEmpty()) &#123; return ConditionOutcome.noMatch( \"@ConditionalOnSingleCandidate \" + spec + \" found no beans\"); &#125; else if (!hasSingleAutowireCandidate(context.getBeanFactory(), matching)) &#123; // 多了一层判断，判断是否只有一个bean return ConditionOutcome.noMatch(\"@ConditionalOnSingleCandidate \" + spec + \" found no primary candidate amongst the\" + \" following \" + matching); &#125; matchMessage.append(\"@ConditionalOnSingleCandidate \" + spec + \" found \" + \"a primary candidate amongst the following \" + matching); &#125; if (metadata.isAnnotated(ConditionalOnMissingBean.class.getName())) &#123; // 相同的逻辑，针对@ConditionalOnMissingBean注解 BeanSearchSpec spec = new BeanSearchSpec(context, metadata, ConditionalOnMissingBean.class); List&lt;String&gt; matching = getMatchingBeans(context, spec); if (!matching.isEmpty()) &#123; return ConditionOutcome.noMatch(\"@ConditionalOnMissingBean \" + spec + \" found the following \" + matching); &#125; matchMessage.append(matchMessage.length() == 0 ? \"\" : \" \"); matchMessage.append(\"@ConditionalOnMissingBean \" + spec + \" found no beans\"); &#125; return ConditionOutcome.match(matchMessage.toString()); //返回匹配成功的ConditonalOutcome&#125; SpringBoot还提供了其他比如ConditionalOnJava、ConditionalOnNotWebApplication、ConditionalOnWebApplication、ConditionalOnResource、ConditionalOnProperty、ConditionalOnExpression等条件注解，有兴趣的读者可以自行查看它们的底层处理逻辑。 各种条件注解的总结 条件注解 对应的Condition处理类 处理逻辑 @ConditionalOnBean OnBeanCondition Spring容器中是否存在对应的实例。可以通过实例的类型、类名、注解、昵称去容器中查找(可以配置从当前容器中查找或者父容器中查找或者两者一起查找)这些属性都是数组，通过”与”的关系进行查找 @ConditionalOnClass OnClassCondition 类加载器中是否存在对应的类。可以通过Class指定(value属性)或者Class的全名指定(name属性)。如果是多个类或者多个类名的话，关系是”与”关系，也就是说这些类或者类名都必须同时在类加载器中存在 @ConditionalOnExpression OnExpressionCondition 判断SpEL 表达式是否成立 @ConditionalOnJava OnJavaCondition 指定Java版本是否符合要求。内部有2个属性value和range。value表示一个枚举的Java版本，range表示比这个老或者新于等于指定的Java版本(默认是新于等于)。内部会基于某些jdk版本特有的类去类加载器中查询，比如如果是jdk9，类加载器中需要存在java.security.cert.URICertStoreParameters；如果是jdk8，类加载器中需要存在java.util.function.Function；如果是jdk7，类加载器中需要存在java.nio.file.Files；如果是jdk6，类加载器中需要存在java.util.ServiceLoader @ConditionalOnMissingBean OnBeanCondition Spring容器中是否缺少对应的实例。可以通过实例的类型、类名、注解、昵称去容器中查找(可以配置从当前容器中查找或者父容器中查找或者两者一起查找)这些属性都是数组，通过”与”的关系进行查找。还多了2个属性ignored(类名)和ignoredType(类名)，匹配的过程中会忽略这些bean @ConditionalOnMissingClass OnClassCondition 跟ConditionalOnClass的处理逻辑一样，只是条件相反，在类加载器中不存在对应的类 @ConditionalOnNotWebApplication OnWebApplicationCondition 应用程序是否是非Web程序，没有提供属性，只是一个标识。会从判断Web程序特有的类是否存在，环境是否是Servlet环境，容器是否是Web容器等 @ConditionalOnProperty OnPropertyCondition 应用环境中的屬性是否存在。提供prefix、name、havingValue以及matchIfMissing属性。prefix表示属性名的前缀，name是属性名，havingValue是具体的属性值，matchIfMissing是个boolean值，如果属性不存在，这个matchIfMissing为true的话，会继续验证下去，否则属性不存在的话直接就相当于匹配不成功 @ConditionalOnResource OnResourceCondition 是否存在指定的资源文件。只有一个属性resources，是个String数组。会从类加载器中去查询对应的资源文件是否存在 @ConditionalOnSingleCandidate OnBeanCondition Spring容器中是否存在且只存在一个对应的实例。只有3个属性value、type、search。跟ConditionalOnBean中的这3种属性值意义一样 @ConditionalOnWebApplication OnWebApplicationCondition 应用程序是否是Web程序，没有提供属性，只是一个标识。会从判断Web程序特有的类是否存在，环境是否是Servlet环境，容器是否是Web容器等 例子 例子意义 @ConditionalOnBean(javax.sql.DataSource.class) Spring容器或者所有父容器中需要存在至少一个javax.sql.DataSource类的实例 @ConditionalOnClass({ Configuration.class,FreeMarkerConfigurationFactory.class }) 类加载器中必须存在Configuration和FreeMarkerConfigurationFactory这两个类 @ConditionalOnExpression(“‘${server.host}’==’localhost’”) server.host配置项的值需要是localhost ConditionalOnJava(JavaVersion.EIGHT) Java版本至少是8 @ConditionalOnMissingBean(value = ErrorController.class, search = SearchStrategy.CURRENT) Spring当前容器中不存在ErrorController类型的bean @ConditionalOnMissingClass(“GenericObjectPool”) 类加载器中不能存在GenericObjectPool这个类 @ConditionalOnNotWebApplication 必须在非Web应用下才会生效 @ConditionalOnProperty(prefix = “spring.aop”, name = “auto”, havingValue = “true”, matchIfMissing = true) 应用程序的环境中必须有spring.aop.auto这项配置，且它的值是true或者环境中不存在spring.aop.auto配置(matchIfMissing为true) @ConditionalOnResource(resources=”mybatis.xml”) 类加载路径中必须存在mybatis.xml文件 @ConditionalOnSingleCandidate(PlatformTransactionManager.class) Spring当前或父容器中必须存在PlatformTransactionManager这个类型的实例，且只有一个实例 @ConditionalOnWebApplication 必须在Web应用下才会生效 SpringBoot条件注解的激活机制分析完了条件注解的执行逻辑之后，接下来的问题就是SpringBoot是如何让这些条件注解生效的？ SpringBoot使用ConditionEvaluator这个内部类完成条件注解的解析和判断。 在Spring容器的refresh过程中，只有跟解析或者注册bean有关系的类都会使用ConditionEvaluator完成条件注解的判断，这个过程中一些类不满足条件的话就会被skip。这些类比如有AnnotatedBeanDefinitionReader、ConfigurationClassBeanDefinitionReader、ConfigurationClassParse、ClassPathScanningCandidateComponentProvider等。 比如ConfigurationClassParser的构造函数会初始化内部属性conditionEvaluator： 1234567891011121314public ConfigurationClassParser(MetadataReaderFactory metadataReaderFactory, ProblemReporter problemReporter, Environment environment, ResourceLoader resourceLoader, BeanNameGenerator componentScanBeanNameGenerator, BeanDefinitionRegistry registry) &#123; this.metadataReaderFactory = metadataReaderFactory; this.problemReporter = problemReporter; this.environment = environment; this.resourceLoader = resourceLoader; this.registry = registry; this.componentScanParser = new ComponentScanAnnotationParser( resourceLoader, environment, componentScanBeanNameGenerator, registry); // 构造ConditionEvaluator用于处理条件注解 this.conditionEvaluator = new ConditionEvaluator(registry, environment, resourceLoader);&#125; ConfigurationClassParser对每个配置类进行解析的时候都会使用ConditionEvaluator： 123if (this.conditionEvaluator.shouldSkip(configClass.getMetadata(), ConfigurationPhase.PARSE_CONFIGURATION)) &#123; return;&#125; ConditionEvaluator的skip方法： 1234567891011121314151617181920212223242526272829303132333435363738394041424344public boolean shouldSkip(AnnotatedTypeMetadata metadata, ConfigurationPhase phase) &#123; // 如果这个类没有被@Conditional注解所修饰，不会skip if (metadata == null || !metadata.isAnnotated(Conditional.class.getName())) &#123; return false; &#125; // 如果参数中沒有设置条件注解的生效阶段 if (phase == null) &#123; // 是配置类的话直接使用PARSE_CONFIGURATION阶段 if (metadata instanceof AnnotationMetadata &amp;&amp; ConfigurationClassUtils.isConfigurationCandidate((AnnotationMetadata) metadata)) &#123; return shouldSkip(metadata, ConfigurationPhase.PARSE_CONFIGURATION); &#125; // 否则使用REGISTER_BEAN阶段 return shouldSkip(metadata, ConfigurationPhase.REGISTER_BEAN); &#125; // 要解析的配置类的条件集合 List&lt;Condition&gt; conditions = new ArrayList&lt;Condition&gt;(); // 获取配置类的条件注解得到条件数据，并添加到集合中 for (String[] conditionClasses : getConditionClasses(metadata)) &#123; for (String conditionClass : conditionClasses) &#123; Condition condition = getCondition(conditionClass, this.context.getClassLoader()); conditions.add(condition); &#125; &#125; // 对条件集合做个排序 AnnotationAwareOrderComparator.sort(conditions); // 遍历条件集合 for (Condition condition : conditions) &#123; ConfigurationPhase requiredPhase = null; if (condition instanceof ConfigurationCondition) &#123; requiredPhase = ((ConfigurationCondition) condition).getConfigurationPhase(); &#125; // 没有这个解析类不需要阶段的判断或者解析类和参数中的阶段一致才会继续进行 if (requiredPhase == null || requiredPhase == phase) &#123; // 阶段一致切不满足条件的话，返回true并跳过这个bean的解析 if (!condition.matches(this.context, metadata)) &#123; return true; &#125; &#125; &#125; return false;&#125; SpringBoot在条件注解的解析log记录在了ConditionEvaluationReport类中，可以通过BeanFactory获取(BeanFactory是有父子关系的；每个BeanFactory都存有一份ConditionEvaluationReport，互不相干)： 12345678910ConditionEvaluationReport conditionEvaluationReport = beanFactory.getBean(\"autoConfigurationReport\", ConditionEvaluationReport.class);Map&lt;String, ConditionEvaluationReport.ConditionAndOutcomes&gt; result = conditionEvaluationReport.getConditionAndOutcomesBySource();for(String key : result.keySet()) &#123; ConditionEvaluationReport.ConditionAndOutcomes conditionAndOutcomes = result.get(key); Iterator&lt;ConditionEvaluationReport.ConditionAndOutcome&gt; iterator = conditionAndOutcomes.iterator(); while(iterator.hasNext()) &#123; ConditionEvaluationReport.ConditionAndOutcome conditionAndOutcome = iterator.next(); System.out.println(key + \" -- \" + conditionAndOutcome.getCondition().getClass().getSimpleName() + \" -- \" + conditionAndOutcome.getOutcome()); &#125;&#125; 打印出条件注解下的类加载信息： ....... org.springframework.boot.autoconfigure.freemarker.FreeMarkerAutoConfiguration -- OnClassCondition -- required @ConditionalOnClass classes not found: freemarker.template.Configuration,org.springframework.ui.freemarker.FreeMarkerConfigurationFactory org.springframework.boot.autoconfigure.groovy.template.GroovyTemplateAutoConfiguration -- OnClassCondition -- required @ConditionalOnClass classes not found: groovy.text.markup.MarkupTemplateEngine org.springframework.boot.autoconfigure.gson.GsonAutoConfiguration -- OnClassCondition -- required @ConditionalOnClass classes not found: com.google.gson.Gson org.springframework.boot.autoconfigure.h2.H2ConsoleAutoConfiguration -- OnClassCondition -- required @ConditionalOnClass classes not found: org.h2.server.web.WebServlet org.springframework.boot.autoconfigure.hateoas.HypermediaAutoConfiguration -- OnClassCondition -- required @ConditionalOnClass classes not found: org.springframework.hateoas.Resource,org.springframework.plugin.core.Plugin org.springframework.boot.autoconfigure.hazelcast.HazelcastAutoConfiguration -- OnClassCondition -- required @ConditionalOnClass classes not found: com.hazelcast.core.HazelcastInstance ....... 一些测试的例子代码在 https://github.com/fangjian0423/springboot-analysis/tree/master/springboot-conditional 上","raw":null,"content":null,"categories":[{"name":"springboot","slug":"springboot","permalink":"http://fangjian0423.github.io/categories/springboot/"}],"tags":[{"name":"java","slug":"java","permalink":"http://fangjian0423.github.io/tags/java/"},{"name":"springboot","slug":"springboot","permalink":"http://fangjian0423.github.io/tags/springboot/"},{"name":"springboot源码分析","slug":"springboot源码分析","permalink":"http://fangjian0423.github.io/tags/springboot源码分析/"}]},{"title":"SpringBoot源码分析之Spring容器的refresh过程","slug":"springboot-context-refresh","date":"2017-05-10T11:55:21.000Z","updated":"2017-05-31T13:18:20.000Z","comments":true,"path":"2017/05/10/springboot-context-refresh/","link":"","permalink":"http://fangjian0423.github.io/2017/05/10/springboot-context-refresh/","excerpt":"上一篇文章中，我们分析了SpringBoot的启动过程：构造SpringApplication并调用它的run方法。其中构造SpringApplication的时候会初始化一些监听器和初始化器；run方法调用的过程中会有对应的监听器监听，并且会创建Spring容器。\nSpring容器创建之后，会调用它的refresh方法，refresh的时候会做很多事情：比如完成配置类的解析、各种BeanFactoryPostProcessor和BeanPostProcessor的注册、国际化配置的初始化、web内置容器的构造等等。\n我们来分析一下这个refresh过程。","text":"上一篇文章中，我们分析了SpringBoot的启动过程：构造SpringApplication并调用它的run方法。其中构造SpringApplication的时候会初始化一些监听器和初始化器；run方法调用的过程中会有对应的监听器监听，并且会创建Spring容器。 Spring容器创建之后，会调用它的refresh方法，refresh的时候会做很多事情：比如完成配置类的解析、各种BeanFactoryPostProcessor和BeanPostProcessor的注册、国际化配置的初始化、web内置容器的构造等等。 我们来分析一下这个refresh过程。 还是以web程序为例，那么对应的Spring容器为AnnotationConfigEmbeddedWebApplicationContext。它的refresh方法调用了父类AbstractApplicationContext的refresh方法： 12345678910111213141516171819202122232425262728293031public void refresh() throws BeansException, IllegalStateException &#123; // refresh过程只能一个线程处理，不允许并发执行 synchronized (this.startupShutdownMonitor) &#123; prepareRefresh(); ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory(); prepareBeanFactory(beanFactory); try &#123; postProcessBeanFactory(beanFactory); invokeBeanFactoryPostProcessors(beanFactory); registerBeanPostProcessors(beanFactory); initMessageSource(); initApplicationEventMulticaster(); onRefresh(); registerListeners(); finishBeanFactoryInitialization(beanFactory); finishRefresh(); &#125; catch (BeansException ex) &#123; if (logger.isWarnEnabled()) &#123; logger.warn(\"Exception encountered during context initialization - \" + \"cancelling refresh attempt: \" + ex); &#125; destroyBeans(); cancelRefresh(ex); throw ex; &#125; finally &#123; resetCommonCaches(); &#125; &#125;&#125; prepareRefresh方法表示在真正做refresh操作之前需要准备做的事情： 设置Spring容器的启动时间，撤销关闭状态，开启活跃状态。 初始化属性源信息(Property) 验证环境信息里一些必须存在的属性 prepareBeanFactory方法从Spring容器获取BeanFactory(Spring Bean容器)并进行相关的设置为后续的使用做准备： 设置classloader(用于加载bean)，设置表达式解析器(解析bean定义中的一些表达式)，添加属性编辑注册器(注册属性编辑器) 添加ApplicationContextAwareProcessor这个BeanPostProcessor。取消ResourceLoaderAware、ApplicationEventPublisherAware、MessageSourceAware、ApplicationContextAware、EnvironmentAware这5个接口的自动注入。因为ApplicationContextAwareProcessor把这5个接口的实现工作做了 设置特殊的类型对应的bean。BeanFactory对应刚刚获取的BeanFactory；ResourceLoader、ApplicationEventPublisher、ApplicationContext这3个接口对应的bean都设置为当前的Spring容器 注入一些其它信息的bean，比如environment、systemProperties等 postProcessBeanFactory方法BeanFactory设置之后再进行后续的一些BeanFactory操作。 不同的Spring容器做不同的操作。比如GenericWebApplicationContext容器会在BeanFactory中添加ServletContextAwareProcessor用于处理ServletContextAware类型的bean初始化的时候调用setServletContext或者setServletConfig方法(跟ApplicationContextAwareProcessor原理一样)。 AnnotationConfigEmbeddedWebApplicationContext对应的postProcessBeanFactory方法： 12345678910111213@Overrideprotected void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory) &#123; // 调用父类EmbeddedWebApplicationContext的实现 super.postProcessBeanFactory(beanFactory); // 查看basePackages属性，如果设置了会使用ClassPathBeanDefinitionScanner去扫描basePackages包下的bean并注册 if (this.basePackages != null &amp;&amp; this.basePackages.length &gt; 0) &#123; this.scanner.scan(this.basePackages); &#125; // 查看annotatedClasses属性，如果设置了会使用AnnotatedBeanDefinitionReader去注册这些bean if (this.annotatedClasses != null &amp;&amp; this.annotatedClasses.length &gt; 0) &#123; this.reader.register(this.annotatedClasses); &#125;&#125; 父类EmbeddedWebApplicationContext的实现： 123456@Overrideprotected void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory) &#123; beanFactory.addBeanPostProcessor( new WebApplicationContextServletContextAwareProcessor(this)); beanFactory.ignoreDependencyInterface(ServletContextAware.class);&#125; invokeBeanFactoryPostProcessors方法在Spring容器中找出实现了BeanFactoryPostProcessor接口的processor并执行。Spring容器会委托给PostProcessorRegistrationDelegate的invokeBeanFactoryPostProcessors方法执行。 介绍两个接口： BeanFactoryPostProcessor：用来修改Spring容器中已经存在的bean的定义，使用ConfigurableListableBeanFactory对bean进行处理 BeanDefinitionRegistryPostProcessor：继承BeanFactoryPostProcessor，作用跟BeanFactoryPostProcessor一样，只不过是使用BeanDefinitionRegistry对bean进行处理 基于web程序的Spring容器AnnotationConfigEmbeddedWebApplicationContext构造的时候，会初始化内部属性AnnotatedBeanDefinitionReader reader，这个reader构造的时候会在BeanFactory中注册一些post processor，包括BeanPostProcessor和BeanFactoryPostProcessor(比如ConfigurationClassPostProcessor、AutowiredAnnotationBeanPostProcessor)： AnnotationConfigUtils.registerAnnotationConfigProcessors(this.registry); invokeBeanFactoryPostProcessors方法处理BeanFactoryPostProcessor的逻辑如下： 从Spring容器中找出BeanDefinitionRegistryPostProcessor类型的bean(这些processor是在容器刚创建的时候通过构造AnnotatedBeanDefinitionReader的时候注册到容器中的)，然后按照优先级分别执行，优先级的逻辑如下： 实现PriorityOrdered接口的BeanDefinitionRegistryPostProcessor先全部找出来，然后排序后依次执行 实现Ordered接口的BeanDefinitionRegistryPostProcessor找出来，然后排序后依次执行 没有实现PriorityOrdered和Ordered接口的BeanDefinitionRegistryPostProcessor找出来执行并依次执行 接下来从Spring容器内查找BeanFactoryPostProcessor接口的实现类，然后执行(如果processor已经执行过，则忽略)，这里的查找规则跟上面查找BeanDefinitionRegistryPostProcessor一样，先找PriorityOrdered，然后是Ordered，最后是两者都没。 这里需要说明的是ConfigurationClassPostProcessor这个processor是优先级最高的被执行的processor(实现了PriorityOrdered接口)。这个ConfigurationClassPostProcessor会去BeanFactory中找出所有有@Configuration注解的bean，然后使用ConfigurationClassParser去解析这个类。ConfigurationClassParser内部有个Map类型的configurationClasses属性用于保存解析的类，ConfigurationClass是一个对要解析的配置类的封装，内部存储了配置类的注解信息、被@Bean注解修饰的方法、@ImportResource注解修饰的信息、ImportBeanDefinitionRegistrar等都存储在这个封装类中。 这里ConfigurationClassPostProcessor最先被处理还有另外一个原因是如果程序中有自定义的BeanFactoryPostProcessor，那么这个PostProcessor首先得通过ConfigurationClassPostProcessor被解析出来，然后才能被Spring容器找到并执行。(ConfigurationClassPostProcessor不先执行的话，这个Processor是不会被解析的，不会被解析的话也就不会执行了)。 在我们的程序中，只有主类RefreshContextApplication有@Configuration注解(@SpringBootApplication注解带有@Configuration注解)，所以这个配置类会被ConfigurationClassParser解析。解析过程如下： 处理@PropertySources注解：进行一些配置信息的解析 处理@ComponentScan注解：使用ComponentScanAnnotationParser扫描basePackage下的需要解析的类(@SpringBootApplication注解也包括了@ComponentScan注解，只不过basePackages是空的，空的话会去获取当前@Configuration修饰的类所在的包)，并注册到BeanFactory中(这个时候bean并没有进行实例化，而是进行了注册。具体的实例化在finishBeanFactoryInitialization方法中执行)。对于扫描出来的类，递归解析 处理@Import注解：先递归找出所有的注解，然后再过滤出只有@Import注解的类，得到@Import注解的值。比如查找@SpringBootApplication注解的@Import注解数据的话，首先发现@SpringBootApplication不是一个@Import注解，然后递归调用修饰了@SpringBootApplication的注解，发现有个@EnableAutoConfiguration注解，再次递归发现被@Import(EnableAutoConfigurationImportSelector.class)修饰，还有@AutoConfigurationPackage注解修饰，再次递归@AutoConfigurationPackage注解，发现被@Import(AutoConfigurationPackages.Registrar.class)注解修饰，所以@SpringBootApplication注解对应的@Import注解有2个，分别是@Import(AutoConfigurationPackages.Registrar.class)和@Import(EnableAutoConfigurationImportSelector.class)。找出所有的@Import注解之后，开始处理逻辑： 遍历这些@Import注解内部的属性类集合 如果这个类是个ImportSelector接口的实现类，实例化这个ImportSelector，如果这个类也是DeferredImportSelector接口的实现类，那么加入ConfigurationClassParser的deferredImportSelectors属性中让第6步处理。否则调用ImportSelector的selectImports方法得到需要Import的类，然后对这些类递归做@Import注解的处理 如果这个类是ImportBeanDefinitionRegistrar接口的实现类，设置到配置类的importBeanDefinitionRegistrars属性中 其它情况下把这个类入队到ConfigurationClassParser的importStack(队列)属性中，然后把这个类当成是@Configuration注解修饰的类递归重头开始解析这个类 处理@ImportResource注解：获取@ImportResource注解的locations属性，得到资源文件的地址信息。然后遍历这些资源文件并把它们添加到配置类的importedResources属性中 处理@Bean注解：获取被@Bean注解修饰的方法，然后添加到配置类的beanMethods属性中 处理DeferredImportSelector：处理第3步@Import注解产生的DeferredImportSelector，进行selectImports方法的调用找出需要import的类，然后再调用第3步相同的处理逻辑处理 这里@SpringBootApplication注解被@EnableAutoConfiguration修饰，@EnableAutoConfiguration注解被@Import(EnableAutoConfigurationImportSelector.class)修饰，所以在第3步会找出这个@Import修饰的类EnableAutoConfigurationImportSelector，这个类刚好实现了DeferredImportSelector接口，接着就会在第6步被执行。第6步selectImport得到的类就是自动化配置类。 EnableAutoConfigurationImportSelector的selectImport方法会在spring.factories文件中找出key为EnableAutoConfiguration对应的值，有81个，这81个就是所谓的自动化配置类(XXXAutoConfiguration)。 ConfigurationClassParser解析完成之后，被解析出来的类会放到configurationClasses属性中。然后使用ConfigurationClassBeanDefinitionReader去解析这些类。 这个时候这些bean只是被加载到了Spring容器中。下面这段代码是ConfigurationClassBeanDefinitionReader的解析bean过程： 12345678910111213141516171819202122232425262728293031323334public void loadBeanDefinitions(Set&lt;ConfigurationClass&gt; configurationModel) &#123; TrackedConditionEvaluator trackedConditionEvaluator = new TrackedConditionEvaluator(); for (ConfigurationClass configClass : configurationModel) &#123; // 对每一个配置类，调用loadBeanDefinitionsForConfigurationClass方法 loadBeanDefinitionsForConfigurationClass(configClass, trackedConditionEvaluator); &#125;&#125;private void loadBeanDefinitionsForConfigurationClass(ConfigurationClass configClass, TrackedConditionEvaluator trackedConditionEvaluator) &#123; // 使用条件注解判断是否需要跳过这个配置类 if (trackedConditionEvaluator.shouldSkip(configClass)) &#123; // 跳过配置类的话在Spring容器中移除bean的注册 String beanName = configClass.getBeanName(); if (StringUtils.hasLength(beanName) &amp;&amp; this.registry.containsBeanDefinition(beanName)) &#123; this.registry.removeBeanDefinition(beanName); &#125; this.importRegistry.removeImportingClassFor(configClass.getMetadata().getClassName()); return; &#125; if (configClass.isImported()) &#123; // 如果自身是被@Import注释所import的，注册自己 registerBeanDefinitionForImportedConfigurationClass(configClass); &#125; // 注册方法中被@Bean注解修饰的bean for (BeanMethod beanMethod : configClass.getBeanMethods()) &#123; loadBeanDefinitionsForBeanMethod(beanMethod); &#125; // 注册@ImportResource注解注释的资源文件中的bean loadBeanDefinitionsFromImportedResources(configClass.getImportedResources()); // 注册@Import注解中的ImportBeanDefinitionRegistrar接口的registerBeanDefinitions loadBeanDefinitionsFromRegistrars(configClass.getImportBeanDefinitionRegistrars());&#125; invokeBeanFactoryPostProcessors方法总结来说就是从Spring容器中找出BeanDefinitionRegistryPostProcessor和BeanFactoryPostProcessor接口的实现类并按照一定的规则顺序进行执行。 其中ConfigurationClassPostProcessor这个BeanDefinitionRegistryPostProcessor优先级最高，它会对项目中的@Configuration注解修饰的类(@Component、@ComponentScan、@Import、@ImportResource修饰的类也会被处理)进行解析，解析完成之后把这些bean注册到BeanFactory中。需要注意的是这个时候注册进来的bean还没有实例化。 registerBeanPostProcessors方法从Spring容器中找出的BeanPostProcessor接口的bean，并设置到BeanFactory的属性中。之后bean被实例化的时候会调用这个BeanPostProcessor。 该方法委托给了PostProcessorRegistrationDelegate类的registerBeanPostProcessors方法执行。这里的过程跟invokeBeanFactoryPostProcessors类似： 先找出实现了PriorityOrdered接口的BeanPostProcessor并排序后加到BeanFactory的BeanPostProcessor集合中 找出实现了Ordered接口的BeanPostProcessor并排序后加到BeanFactory的BeanPostProcessor集合中 没有实现PriorityOrdered和Ordered接口的BeanPostProcessor加到BeanFactory的BeanPostProcessor集合中 这些已经存在的BeanPostProcessor在postProcessBeanFactory方法中已经说明，都是由AnnotationConfigUtils的registerAnnotationConfigProcessors方法注册的。这些BeanPostProcessor包括有AutowiredAnnotationBeanPostProcessor(处理被@Autowired注解修饰的bean并注入)、RequiredAnnotationBeanPostProcessor(处理被@Required注解修饰的方法)、CommonAnnotationBeanPostProcessor(处理@PreDestroy、@PostConstruct、@Resource等多个注解的作用)等。 如果是自定义的BeanPostProcessor，已经被ConfigurationClassPostProcessor注册到容器内。 这些BeanPostProcessor会在这个方法内被实例化(通过调用BeanFactory的getBean方法，如果没有找到实例化的类，就会去实例化)。 initMessageSource方法在Spring容器中初始化一些国际化相关的属性。 initApplicationEventMulticaster方法在Spring容器中初始化事件广播器，事件广播器用于事件的发布。 在SpringBoot源码分析之SpringBoot的启动过程中分析过，EventPublishingRunListener这个SpringApplicationRunListener会监听事件，其中发生contextPrepared事件的时候EventPublishingRunListener会把事件广播器注入到BeanFactory中。 所以initApplicationEventMulticaster不再需要再次注册，只需要拿出BeanFactory中的事件广播器然后设置到Spring容器的属性中即可。如果没有使用SpringBoot的话，Spring容器得需要自己初始化事件广播器。 onRefresh方法一个模板方法，不同的Spring容器做不同的事情。 比如web程序的容器AnnotationConfigEmbeddedWebApplicationContext中会调用createEmbeddedServletContainer方法去创建内置的Servlet容器。 目前SpringBoot只支持3种内置的Servlet容器： Tomcat Jetty Undertow registerListeners方法把Spring容器内的时间监听器和BeanFactory中的时间监听器都添加的事件广播器中。 然后如果存在early event的话，广播出去。 finishBeanFactoryInitialization方法实例化BeanFactory中已经被注册但是未实例化的所有实例(懒加载的不需要实例化)。 比如invokeBeanFactoryPostProcessors方法中根据各种注解解析出来的类，在这个时候都会被初始化。 实例化的过程各种BeanPostProcessor开始起作用。 finishRefresh方法refresh做完之后需要做的其他事情。 初始化生命周期处理器，并设置到Spring容器中(LifecycleProcessor) 调用生命周期处理器的onRefresh方法，这个方法会找出Spring容器中实现了SmartLifecycle接口的类并进行start方法的调用 发布ContextRefreshedEvent事件告知对应的ApplicationListener进行响应的操作 调用LiveBeansView的registerApplicationContext方法：如果设置了JMX相关的属性，则就调用该方法 发布EmbeddedServletContainerInitializedEvent事件告知对应的ApplicationListener进行响应的操作 总结Spring容器的refresh过程就是上述11个方法的介绍。内容还是非常多的，本文也只是说了个大概，像bean的实例化过程没有具体去分析，这方面的内容以后会看情况去做分析。 这篇文章也是为之后的文章比如内置Servlet容器的创建启动、条件注解的使用等打下基础。 例子写了一个例子用来验证容器的refresh过程，包括bean解析，processor的使用、Lifecycle的使用等。 可以启动项目debug去看看对应的过程，这样对Spring容器会有一个更好的理解。 地址在：https://github.com/fangjian0423/springboot-analysis/tree/master/springboot-refresh-context","raw":null,"content":null,"categories":[{"name":"springboot","slug":"springboot","permalink":"http://fangjian0423.github.io/categories/springboot/"}],"tags":[{"name":"java","slug":"java","permalink":"http://fangjian0423.github.io/tags/java/"},{"name":"springboot","slug":"springboot","permalink":"http://fangjian0423.github.io/tags/springboot/"},{"name":"springboot源码分析","slug":"springboot源码分析","permalink":"http://fangjian0423.github.io/tags/springboot源码分析/"}]},{"title":"SpringBoot源码分析之SpringBoot的启动过程","slug":"springboot-startup-analysis","date":"2017-04-30T12:33:33.000Z","updated":"2017-05-31T13:20:59.000Z","comments":true,"path":"2017/04/30/springboot-startup-analysis/","link":"","permalink":"http://fangjian0423.github.io/2017/04/30/springboot-startup-analysis/","excerpt":"SpringBoot的启动很简单，代码如下：\n123456@SpringBootApplicationpublic class MyApplication &#123;    public static void main(String[] args) &#123;        SpringApplication.run(MyApplication.class, args);    &#125;&#125;\n从代码上可以看出，调用了SpringApplication的静态方法run。这个run方法会构造一个SpringApplication的实例，然后再调用这里实例的run方法就表示启动SpringBoot。\n因此，想要分析SpringBoot的启动过程，我们需要熟悉SpringApplication的构造过程以及SpringApplication的run方法执行过程即可。\n我们以上述这段代码为例，分析SpringBoot的启动过程。","text":"SpringBoot的启动很简单，代码如下： 123456@SpringBootApplicationpublic class MyApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(MyApplication.class, args); &#125;&#125; 从代码上可以看出，调用了SpringApplication的静态方法run。这个run方法会构造一个SpringApplication的实例，然后再调用这里实例的run方法就表示启动SpringBoot。 因此，想要分析SpringBoot的启动过程，我们需要熟悉SpringApplication的构造过程以及SpringApplication的run方法执行过程即可。 我们以上述这段代码为例，分析SpringBoot的启动过程。 SpringApplication的构造过程SpringApplication构造的时候内部会调用一个private方法initialize： 1234567891011121314151617public SpringApplication(Object... sources) &#123; initialize(sources); // sources目前是一个MyApplication的class对象&#125;private void initialize(Object[] sources) &#123; if (sources != null &amp;&amp; sources.length &gt; 0) &#123; this.sources.addAll(Arrays.asList(sources)); // 把sources设置到SpringApplication的sources属性中，目前只是一个MyApplication类对象 &#125; this.webEnvironment = deduceWebEnvironment(); // 判断是否是web程序(javax.servlet.Servlet和org.springframework.web.context.ConfigurableWebApplicationContext都必须在类加载器中存在)，并设置到webEnvironment属性中 // 从spring.factories文件中找出key为ApplicationContextInitializer的类并实例化后设置到SpringApplication的initializers属性中。这个过程也就是找出所有的应用程序初始化器 setInitializers((Collection) getSpringFactoriesInstances( ApplicationContextInitializer.class)); // 从spring.factories文件中找出key为ApplicationListener的类并实例化后设置到SpringApplication的listeners属性中。这个过程就是找出所有的应用程序事件监听器 setListeners((Collection) getSpringFactoriesInstances(ApplicationListener.class)); // 找出main类，这里是MyApplication类 this.mainApplicationClass = deduceMainApplicationClass();&#125; ApplicationContextInitializer，应用程序初始化器，做一些初始化的工作： 123public interface ApplicationContextInitializer&lt;C extends ConfigurableApplicationContext&gt; &#123; void initialize(C applicationContext);&#125; ApplicationListener，应用程序事件(ApplicationEvent)监听器： 123public interface ApplicationListener&lt;E extends ApplicationEvent&gt; extends EventListener &#123; void onApplicationEvent(E event);&#125; 这里的应用程序事件(ApplicationEvent)有应用程序启动事件(ApplicationStartedEvent)，失败事件(ApplicationFailedEvent)，准备事件(ApplicationPreparedEvent)等。 应用程序事件监听器跟监听事件是绑定的。比如ConfigServerBootstrapApplicationListener只跟ApplicationEnvironmentPreparedEvent事件绑定，LiquibaseServiceLocatorApplicationListener只跟ApplicationStartedEvent事件绑定，LoggingApplicationListener跟所有事件绑定等。 默认情况下，initialize方法从spring.factories文件中找出的key为ApplicationContextInitializer的类有： org.springframework.boot.context.config.DelegatingApplicationContextInitializer org.springframework.boot.context.ContextIdApplicationContextInitializer org.springframework.boot.context.ConfigurationWarningsApplicationContextInitializer org.springframework.boot.context.web.ServerPortInfoApplicationContextInitializer org.springframework.boot.autoconfigure.logging.AutoConfigurationReportLoggingInitializer key为ApplicationListener的有： org.springframework.boot.context.config.ConfigFileApplicationListener org.springframework.boot.context.config.AnsiOutputApplicationListener org.springframework.boot.logging.LoggingApplicationListener org.springframework.boot.logging.ClasspathLoggingApplicationListener org.springframework.boot.autoconfigure.BackgroundPreinitializer org.springframework.boot.context.config.DelegatingApplicationListener org.springframework.boot.builder.ParentContextCloserApplicationListener org.springframework.boot.context.FileEncodingApplicationListener org.springframework.boot.liquibase.LiquibaseServiceLocatorApplicationListener SpringApplication的执行分析run方法之前，先看一下SpringApplication中的一些事件和监听器概念。 首先是SpringApplicationRunListeners类和SpringApplicationRunListener类的介绍。 SpringApplicationRunListeners内部持有SpringApplicationRunListener集合和1个Log日志类。用于SpringApplicationRunListener监听器的批量执行。 SpringApplicationRunListener看名字也知道用于监听SpringApplication的run方法的执行。 它定义了5个步骤： started(run方法执行的时候立马执行；对应事件的类型是ApplicationStartedEvent) environmentPrepared(ApplicationContext创建之前并且环境信息准备好的时候调用；对应事件的类型是ApplicationEnvironmentPreparedEvent) contextPrepared(ApplicationContext创建好并且在source加载之前调用一次；没有具体的对应事件) contextLoaded(ApplicationContext创建并加载之后并在refresh之前调用；对应事件的类型是ApplicationPreparedEvent) finished(run方法结束之前调用；对应事件的类型是ApplicationReadyEvent或ApplicationFailedEvent) SpringApplicationRunListener目前只有一个实现类EventPublishingRunListener，它把监听的过程封装成了SpringApplicationEvent事件并让内部属性(属性名为multicaster)ApplicationEventMulticaster接口的实现类SimpleApplicationEventMulticaster广播出去，广播出去的事件对象会被SpringApplication中的listeners属性进行处理。 所以说SpringApplicationRunListener和ApplicationListener之间的关系是通过ApplicationEventMulticaster广播出去的SpringApplicationEvent所联系起来的。 SpringApplication的run方法代码如下： 1234567891011121314151617181920212223242526272829303132public ConfigurableApplicationContext run(String... args) &#123; StopWatch stopWatch = new StopWatch(); // 构造一个任务执行观察器 stopWatch.start(); // 开始执行，记录开始时间 ConfigurableApplicationContext context = null; configureHeadlessProperty(); // 获取SpringApplicationRunListeners，内部只有一个EventPublishingRunListener SpringApplicationRunListeners listeners = getRunListeners(args); // 上面分析过，会封装成SpringApplicationEvent事件然后广播出去给SpringApplication中的listeners所监听 // 这里接受ApplicationStartedEvent事件的listener会执行相应的操作 listeners.started(); try &#123; // 构造一个应用程序参数持有类 ApplicationArguments applicationArguments = new DefaultApplicationArguments( args); // 创建Spring容器 context = createAndRefreshContext(listeners, applicationArguments); // 容器创建完成之后执行额外一些操作 afterRefresh(context, applicationArguments); // 广播出ApplicationReadyEvent事件给相应的监听器执行 listeners.finished(context, null); stopWatch.stop(); // 执行结束，记录执行时间 if (this.logStartupInfo) &#123; new StartupInfoLogger(this.mainApplicationClass) .logStarted(getApplicationLog(), stopWatch); &#125; return context; // 返回Spring容器 &#125; catch (Throwable ex) &#123; handleRunFailure(context, listeners, ex); // 这个过程报错的话会执行一些异常操作、然后广播出ApplicationFailedEvent事件给相应的监听器执行 throw new IllegalStateException(ex); &#125;&#125; 创建容器的方法createAndRefreshContext如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253private ConfigurableApplicationContext createAndRefreshContext( SpringApplicationRunListeners listeners, ApplicationArguments applicationArguments) &#123; ConfigurableApplicationContext context; // 定义Spring容器 // 创建应用程序的环境信息。如果是web程序，创建StandardServletEnvironment；否则，创建StandardEnvironment ConfigurableEnvironment environment = getOrCreateEnvironment(); // 配置一些环境信息。比如profile，命令行参数 configureEnvironment(environment, applicationArguments.getSourceArgs()); // 广播出ApplicationEnvironmentPreparedEvent事件给相应的监听器执行 listeners.environmentPrepared(environment); // 环境信息的校对 if (isWebEnvironment(environment) &amp;&amp; !this.webEnvironment) &#123; environment = convertToStandardEnvironment(environment); &#125; if (this.bannerMode != Banner.Mode.OFF) &#123; // 是否在控制台上打印自定义的banner printBanner(environment); &#125; // Create, load, refresh and run the ApplicationContext context = createApplicationContext(); // 创建Spring容器 context.setEnvironment(environment); // 设置Spring容器的环境信息 postProcessApplicationContext(context); // 回调方法，Spring容器创建之后做一些额外的事 applyInitializers(context); // SpringApplication的的初始化器开始工作 // 遍历调用SpringApplicationRunListener的contextPrepared方法。目前只是将这个事件广播器注册到Spring容器中 listeners.contextPrepared(context); if (this.logStartupInfo) &#123; logStartupInfo(context.getParent() == null); logStartupProfileInfo(context); &#125; // 把应用程序参数持有类注册到Spring容器中，并且是一个单例 context.getBeanFactory().registerSingleton(\"springApplicationArguments\", applicationArguments); Set&lt;Object&gt; sources = getSources(); Assert.notEmpty(sources, \"Sources must not be empty\"); load(context, sources.toArray(new Object[sources.size()])); // 广播出ApplicationPreparedEvent事件给相应的监听器执行 listeners.contextLoaded(context); // Spring容器的刷新 refresh(context); if (this.registerShutdownHook) &#123; try &#123; context.registerShutdownHook(); &#125; catch (AccessControlException ex) &#123; // Not allowed in some environments. &#125; &#125; return context;&#125; Spring容器的创建createApplicationContext方法如下： 123456789101112131415161718protected ConfigurableApplicationContext createApplicationContext() &#123; Class&lt;?&gt; contextClass = this.applicationContextClass; if (contextClass == null) &#123; try &#123; // 如果是web程序，那么构造org.springframework.boot.context.embedded.AnnotationConfigEmbeddedWebApplicationContext容器 // 否则构造org.springframework.context.annotation.AnnotationConfigApplicationContext容器 contextClass = Class.forName(this.webEnvironment ? DEFAULT_WEB_CONTEXT_CLASS : DEFAULT_CONTEXT_CLASS); &#125; catch (ClassNotFoundException ex) &#123; throw new IllegalStateException( \"Unable create a default ApplicationContext, \" + \"please specify an ApplicationContextClass\", ex); &#125; &#125; return (ConfigurableApplicationContext) BeanUtils.instantiate(contextClass);&#125; Spring容器创建之后有个回调方法postProcessApplicationContext： 12345678910111213141516171819202122 protected void postProcessApplicationContext(ConfigurableApplicationContext context) &#123; if (this.webEnvironment) &#123; // 如果是web程序 if (context instanceof ConfigurableWebApplicationContext) &#123; // 并且也是Spring Web容器 ConfigurableWebApplicationContext configurableContext = (ConfigurableWebApplicationContext) context; if (this.beanNameGenerator != null) &#123; // 如果SpringApplication设置了是实例命名生成器，注册到Spring容器中 configurableContext.getBeanFactory().registerSingleton( AnnotationConfigUtils.CONFIGURATION_BEAN_NAME_GENERATOR, this.beanNameGenerator); &#125; &#125; &#125; if (this.resourceLoader != null) &#123; // 如果SpringApplication设置了资源加载器，设置到Spring容器中 if (context instanceof GenericApplicationContext) &#123; ((GenericApplicationContext) context) .setResourceLoader(this.resourceLoader); &#125; if (context instanceof DefaultResourceLoader) &#123; ((DefaultResourceLoader) context) .setClassLoader(this.resourceLoader.getClassLoader()); &#125; &#125;&#125; 初始化器做的工作，比如ContextIdApplicationContextInitializer会设置应用程序的id；AutoConfigurationReportLoggingInitializer会给应用程序添加一个条件注解解析器报告等： 123456789protected void applyInitializers(ConfigurableApplicationContext context) &#123; // 遍历每个初始化器，对调用对应的initialize方法 for (ApplicationContextInitializer initializer : getInitializers()) &#123; Class&lt;?&gt; requiredType = GenericTypeResolver.resolveTypeArgument( initializer.getClass(), ApplicationContextInitializer.class); Assert.isInstanceOf(requiredType, context, \"Unable to call initializer.\"); initializer.initialize(context); &#125;&#125; Spring容器的刷新refresh方法内部会做很多很多的事情：比如BeanFactory的设置，BeanFactoryPostProcessor接口的执行、BeanPostProcessor接口的执行、自动化配置类的解析、条件注解的解析、国际化的初始化等等。这部分内容会在之后的文章中进行讲解。 run方法中的Spring容器创建完成之后会调用afterRefresh方法，代码如下： 123456789101112131415161718192021222324 protected void afterRefresh(ConfigurableApplicationContext context, ApplicationArguments args) &#123; afterRefresh(context, args.getSourceArgs()); // 目前是个空实现 callRunners(context, args); // 调用Spring容器中的ApplicationRunner和CommandLineRunner接口的实现类 &#125; private void callRunners(ApplicationContext context, ApplicationArguments args) &#123; List&lt;Object&gt; runners = new ArrayList&lt;Object&gt;(); // 找出Spring容器中ApplicationRunner接口的实现类 runners.addAll(context.getBeansOfType(ApplicationRunner.class).values()); // 找出Spring容器中CommandLineRunner接口的实现类 runners.addAll(context.getBeansOfType(CommandLineRunner.class).values()); // 对runners进行排序 AnnotationAwareOrderComparator.sort(runners); // 遍历runners依次执行 for (Object runner : new LinkedHashSet&lt;Object&gt;(runners)) &#123; if (runner instanceof ApplicationRunner) &#123; // 如果是ApplicationRunner，进行ApplicationRunner的run方法调用 callRunner((ApplicationRunner) runner, args); &#125; if (runner instanceof CommandLineRunner) &#123; // 如果是CommandLineRunner，进行CommandLineRunner的run方法调用 callRunner((CommandLineRunner) runner, args); &#125; &#125;&#125; 这样run方法执行完成之后。Spring容器也已经初始化完成，各种监听器和初始化器也做了相应的工作。 总结SpringBoot启动的时候，不论调用什么方法，都会构造一个SpringApplication的实例，然后调用这个实例的run方法，这样就表示启动SpringBoot。 在run方法调用之前，也就是构造SpringApplication的时候会进行初始化的工作，初始化的时候会做以下几件事： 把参数sources设置到SpringApplication属性中，这个sources可以是任何类型的参数。本文的例子中这个sources就是MyApplication的class对象 判断是否是web程序，并设置到webEnvironment这个boolean属性中 找出所有的初始化器，默认有5个，设置到initializers属性中 找出所有的应用程序监听器，默认有9个，设置到listeners属性中 找出运行的主类(main class) SpringApplication构造完成之后调用run方法，启动SpringApplication，run方法执行的时候会做以下几件事： 构造一个StopWatch，观察SpringApplication的执行 找出所有的SpringApplicationRunListener并封装到SpringApplicationRunListeners中，用于监听run方法的执行。监听的过程中会封装成事件并广播出去让初始化过程中产生的应用程序监听器进行监听 构造Spring容器(ApplicationContext)，并返回3.1 创建Spring容器的判断是否是web环境，是的话构造AnnotationConfigEmbeddedWebApplicationContext，否则构造AnnotationConfigApplicationContext3.2 初始化过程中产生的初始化器在这个时候开始工作3.3 Spring容器的刷新(完成bean的解析、各种processor接口的执行、条件注解的解析等等) 从Spring容器中找出ApplicationRunner和CommandLineRunner接口的实现类并排序后依次执行 例子写了一个例子用来验证分析的启动逻辑，包括自定义的初始化器、监听器、ApplicationRunner和CommandLineRunner。 地址在：https://github.com/fangjian0423/springboot-analysis/tree/master/springboot-startup","raw":null,"content":null,"categories":[{"name":"springboot","slug":"springboot","permalink":"http://fangjian0423.github.io/categories/springboot/"}],"tags":[{"name":"java","slug":"java","permalink":"http://fangjian0423.github.io/tags/java/"},{"name":"springboot","slug":"springboot","permalink":"http://fangjian0423.github.io/tags/springboot/"},{"name":"springboot源码分析","slug":"springboot源码分析","permalink":"http://fangjian0423.github.io/tags/springboot源码分析/"}]},{"title":"记录自己理解的一些设计模式","slug":"design-pattern","date":"2017-03-26T07:33:29.000Z","updated":"2017-03-26T07:51:53.000Z","comments":true,"path":"2017/03/26/design-pattern/","link":"","permalink":"http://fangjian0423.github.io/2017/03/26/design-pattern/","excerpt":"记录一下自己理解的一些设计模式，并尽量使用表达清楚的例子进行讲解。","text":"记录一下自己理解的一些设计模式，并尽量使用表达清楚的例子进行讲解。 策略模式策略模式应该是最基础的一个设计模式，它是对行为的一个抽象。jdk中的Comparator比较器就是一个使用策略设计模式的策略。 比如有一个Student学生类，有name和age两个属性。如果有个需求需要打印学生名单，并按照字母顺序排序，可以使用Comparator接口并在内部使用name进行比较即可。 如果哪一天需要按照年龄进行排序，那么只需要修改Comparator即可，也就是使用一个新的策略，其它完全不变。 工厂模式工厂模式的意义在于对象的创建、管理可以使用工厂去管理，而不是创建者自身。最典型的工厂模式使用者就是Spring，Spring内部的容器就是一个工厂，所有的bean都由这个容器管理，包括它们的创建、销毁、注入都被这个容器管理。 工厂模式分简单工厂和抽象工厂。它们的区别在于抽象工厂抽象程度更高，把工厂也抽象成了一个接口，这样可以再每添加一个新的对象的时候而不需要修改工厂的代码。 比如有个Repository接口，用于存储数据，有DatabaseRepository，CacheRepository，FileRepository分别在数据库，缓存，文件中存储数据，定义如下： public interface Repository { void save(Object obj); } class DatabaseRepository implements Repository { @Override public void save(Object obj) { System.out.println(&quot;save in database&quot;); } } class CacheRepository implements Repository { @Override public void save(Object obj) { System.out.println(&quot;save in cache&quot;); } } class FileRepository implements Repository { @Override public void save(Object obj) { System.out.println(&quot;save in file&quot;); } } 简单工厂的使用public class RepositoryFactory { public Repository create(String type) { Repository repository = null; switch (type) { case &quot;db&quot;: repository = new DatabaseRepository(); break; case &quot;cache&quot;: repository = new CacheRepository(); break; case &quot;file&quot;: repository = new FileRepository(); break; } return repository; } public static void main(String[] args) { RepositoryFactory factory = new RepositoryFactory(); factory.create(&quot;db&quot;).save(new Object()); factory.create(&quot;cache&quot;).save(new Object()); factory.create(&quot;file&quot;).save(new Object()); } } 简单工厂的弊端在于每添加一个新的Repository，都必须修改RepositoryFactory中的代码 抽象工厂的使用public interface RepositoryFactoryProvider { Repository create(); } class DatabaseRepositoryFactory implements RepositoryFactoryProvider { @Override public Repository create() { return new DatabaseRepository(); } } class CacheRepositoryFactory implements RepositoryFactoryProvider { @Override public Repository create() { return new CacheRepository(); } } class FileRepositoryFactory implements RepositoryFactoryProvider { @Override public Repository create() { return new FileRepository(); } } 抽象工厂的测试： RepositoryFactoryProvider dbProvider = new DatabaseRepositoryFactory(); dbProvider.create().save(new Object()); RepositoryFactoryProvider cacheProvider = new CacheRepositoryFactory(); cacheProvider.create().save(new Object()); RepositoryFactoryProvider fileProvider = new FileRepositoryFactory(); fileProvider.create().save(new Object()); 抽象工厂把工厂也进行了抽象话，所以添加一个新的Repository的话，只需要新增一个RepositoryFactory即可，原有代码不需要修改。 装饰者模式装饰者模式的作用就在于它可以在不改变原有类的基础上动态地给类添加新的功能。之前写过一篇通过源码分析MyBatis的缓存文章，mybatis中的query就是使用了装饰者设计模式。 用一段简单的代码来模拟一下mybatis中query的实现原理： @Data @AllArgsConstructor @ToString class Result { // 查询结果类，相当于一个domain private Object obj; private String sql; } public interface Query { // 查询接口，有简单查询和缓存查询 Result query(String sql); } public class SimpleQuery implements Query { // 简单查询，相当于直接查询数据库，这里直接返回Result，相当于是数据库查询的结果 @Override public Result query(String sql) { return new Result(new Object(), sql); } } public class CacheQuery implements Query { // 缓存查询，如果查询相同的sql，不直接查询数据库，而是返回map中存在的Result private Query query; private Map&lt;String, Result&gt; cache = new HashMap&lt;&gt;(); public CacheQuery(Query query) { this.query = query; } @Override public Result query(String sql) { if(cache.containsKey(sql)) { return cache.get(sql); } Result result = query.query(sql); cache.put(sql, result); return result; } } 测试： Query simpleQuery = new SimpleQuery(); System.out.println(simpleQuery.query(&quot;select * from t_student&quot;) == simpleQuery.query(&quot;select * from t_student&quot;)); // false Query cacheQuery = new CacheQuery(simpleQuery); System.out.println(cacheQuery.query(&quot;select * from t_student&quot;) == cacheQuery.query(&quot;select * from t_student&quot;)); // true 这里CacheQuery就是一个装饰类，SimpleQuery是一个被装饰者。我们通过装饰者设计模式动态地给SimpleQuery添加了缓存功能，而不需要修改SimpleQuery的代码。 当然，装饰者模式也有缺点，就是会存在太多的类。 如果我们需要添加一个过滤的查询(sql中有敏感字的就直接返回null，而不查询数据库)，只需要可以添加一个FilterQuery装饰者即可： public class FilterQuery implements Query { private Query query; private List&lt;String&gt; words = new ArrayList&lt;&gt;(); public FilterQuery(Query query) { this.query = query; words.add(&quot;fuck&quot;); words.add(&quot;sex&quot;); } @Override public Result query(String sql) { for(String word : words) { if(sql.contains(word)) return null; } return query.query(sql); } } Query filterQuery = new FilterQuery(simpleQuery); System.out.println(filterQuery.query(&quot;select * from t_student where name = &apos;fuck&apos;&quot;)); // null System.out.println(filterQuery.query(&quot;select * from t_student where name = &apos;format&apos;&quot;)); // Result(obj=java.lang.Object@1b4fb997, sql=select * from t_student where name = &apos;format&apos;) 代理模式代理模式的作用是使用一个代理类来代替原先类进行操作。比较常见的就是aop中就是使用代理模式完成事务的处理。 代理模式分静态代理和动态代理，静态代理的原理就是对目标对象进行封装，最后调用目标对象的方法即可。 动态代理跟静态代理的区别就是动态代理中的代理类是程序运行的时候生成的。Spring中对于接口的代理使用jdk内置的Proxy和InvocationHandler实现，对于类的代理使用cglib完成。 以1个UserService为例，使用jdk自带的代理模式完成计算方法调用时间的需求： // UserService接口 public interface IUserService { void printAll(); } // UserService实现类 class UserService implements IUserService { @Override public void printAll() { System.out.println(&quot;print all users&quot;); } } // InvocationHandler策略，这里打印了方法调用前后的时间 @AllArgsConstructor class UserInvocationHandler implements InvocationHandler { private IUserService userService; @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { System.out.println(&quot;start : &quot; + System.currentTimeMillis()); Object result = method.invoke(userService, args); System.out.println(&quot;end : &quot; + System.currentTimeMillis()); return result; } } 测试： IUserService userService = new UserService(); UserInvocationHandler uih = new UserInvocationHandler(userService); IUserService proxy = (IUserService) Proxy.newProxyInstance(userService.getClass().getClassLoader(), new Class[] {IUserService.class}, uih); proxy.printAll(); // 打印出start : 1489665566456 print all users end : 1489665566457 组合模式组合模式经常跟策略模式配合使用，用来组合所有的策略，并遍历这些策略找出满足条件的策略。之前写过一篇SpringMVC关于json、xml自动转换的原理研究文章，里面springmvc把返回的返回值映射给用户的response做了一层抽象，封装到了HandlerMethodReturnValueHandler策略接口中。 在HandlerMethodReturnValueHandlerComposite类中，使用存在的HandlerMethodReturnValueHandler对返回值进行处理，在HandlerMethodReturnValueHandlerComposite内部的代码如下： // 策略集合 private final List&lt;HandlerMethodReturnValueHandler&gt; returnValueHandlers = new ArrayList&lt;HandlerMethodReturnValueHandler&gt;(); @Override public void handleReturnValue(Object returnValue, MethodParameter returnType, ModelAndViewContainer mavContainer, NativeWebRequest webRequest) throws Exception { // 调用selectHandler方法 HandlerMethodReturnValueHandler handler = selectHandler(returnValue, returnType); if (handler == null) { throw new IllegalArgumentException(&quot;Unknown return value type: &quot; + returnType.getParameterType().getName()); } handler.handleReturnValue(returnValue, returnType, mavContainer, webRequest); // 使用找到的handler进行处理 } private HandlerMethodReturnValueHandler selectHandler(Object value, MethodParameter returnType) { boolean isAsyncValue = isAsyncReturnValue(value, returnType); // 遍历存在的HandlerMethodReturnValueHandler for (HandlerMethodReturnValueHandler handler : this.returnValueHandlers) { if (isAsyncValue &amp;&amp; !(handler instanceof AsyncHandlerMethodReturnValueHandler)) { continue; } if (handler.supportsReturnType(returnType)) { // 找到匹配的handler return handler; } } return null; } 模板模式跟策略模式类似，模板模式会先定义好实现的逻辑步骤，但是具体的实现方式由子类完成，跟策略模式的区别就是模板模式是有逻辑步骤的。比如要给院系里的学生排序，并取出排名第一的学生。这里就有2个步骤，分别是排序和取出第一名学生。 一段伪代码： public abstract class AbstractStudentGetter { public final Student getStudent(List&lt;Student&gt; students) { sort(students); // 第一步 if(!CollectionUtils.isEmpty(students)) { return students.get(0); // 第二步 } return null; } abstract public void sort(List&lt;Student&gt; students); } class AgeStudentGetter extends AbstractStudentGetter { // 取出年纪最大的学生 @Override public void sort(List&lt;Student&gt; students) { students.sort(new Comparator&lt;Student&gt;() { @Override public int compare(Student s1, Student s2) { return s2.getAge() - s1.getAge(); } }); } } class NameStudentGetter extends AbstractStudentGetter { // 按照名字字母排序取出第一个学生 @Override public void sort(List&lt;Student&gt; students) { students.sort(new Comparator&lt;Student&gt;() { @Override public int compare(Student s1, Student s2) { return s2.getName().compareTo(s1.getName()); } }); } } 测试： AbstractStudentGetter ageGetter = new AgeStudentGetter(); AbstractStudentGetter nameGetter = new NameStudentGetter(); List&lt;Student&gt; students = new ArrayList&lt;&gt;(); students.add(new Student(&quot;jim&quot;, 22)); students.add(new Student(&quot;format&quot;, 25)); System.out.println(ageGetter.getStudent(students)); // Student(name=format, age=25) System.out.println(nameGetter.getStudent(students)); // Student(name=jim, age=22) 观察者设计模式观察者设计模式主要的使用场景在于一个对象变化之后，依赖该对象的对象会收到通知。典型的例子就是rss的订阅，当订阅了博客的rss之后，当博客更新之后，订阅者就会收到新的订阅信息。 jdk内置提供了Observable和Observer，用来实现观察者模式： // 定义一个Observable public class MetricsObserable extends Observable { private Map&lt;String, Long&gt; counterMap = new HashMap&lt;&gt;(); public void updateCounter(String key, Long value) { counterMap.put(key, value); setChanged(); notifyObservers(counterMap); } } // Observer public class AdminA implements Observer { @Override public void update(Observable o, Object arg) { System.out.println(&quot;adminA: &quot; + arg); } } public class AdminB implements Observer { @Override public void update(Observable o, Object arg) { System.out.println(&quot;adminB: &quot; + arg); } } 测试： MetricsObserable metricsObserable = new MetricsObserable(); metricsObserable.addObserver(new AdminA()); metricsObserable.addObserver(new AdminB()); metricsObserable.updateCounter(&quot;request-count&quot;, 100l); 打印出： adminB: {request-count=100} adminA: {request-count=100} 享元模式线程池中会构造几个核心线程用于处理，这些线程会去取阻塞队列里的任务然后进行执行。这些线程就是会被共享、且被重复使用的。因为线程的创建、销毁、调度都是需要消耗资源的，没有必要每次创建新的线程，而是共用一些线程。这就是享元模式的使用。类似的还有jdbc连接池，对象池等。 之前有一次面试被问到： Integer.valueOf(&quot;1&quot;) == Integer.valueOf(&quot;1&quot;) // true还是false 当时回答的是false，后来翻了下Integer的源码发现Integer里面有个内部类IntegerCache，用于缓存一些共用的Integer。这个缓存的范围可以在jvm启动的时候进行设置。 其实后来想想也应该这么做，我们没有必要每次使用对象的时候都返回新的对象，可以共享这些对象，因为新对象的创建都是需要消耗内存的。 适配器模式适配器模式比较好理解。像生活中插线口的插头有2个口的，也有3个口的。如果电脑的电源插口只有3个口的，但是我们需要一个2个口的插口的话，这个时候就需要使用插座来外接这个3个口的插头，插座上有2个口的插头。 这个例子跟我们编程一样，当用户系统的接口跟我们系统内部的接口不一致时，我们可以使用适配器来完成接口的转换。 使用继承的方式实现类的适配： public class Source { public void method() { System.out.println(&quot;source method&quot;); } } interface Targetable { void method(); void newMethod(); } class Adapter extends Source implements Targetable { @Override public void newMethod() { System.out.println(&quot;new method&quot;); } } 测试： Targetable targetable = new Adapter(); targetable.method(); // source method targetable.newMethod(); // new method 上述方式是用接口和继承的方式实现适配器模式。当然我们也可以使用组合的方式实现(把Source当成属性放到Adapter中)。 单例模式单例模式比较好理解，Spring就是典型的例子。被Spring中的容器管理的对象都有对应的scope，配置成singleton说明这个对象就是单例，也就是在Spring容器的生命周期中，这个类只有1个实例。 java中单例模式的写法也有好多种。比如懒汉式、饿汉式、内部类方式、枚举方式等。 需要注意的如果使用dcl的话需要初始化过程，这篇Java内存模型之从JMM角度分析DCL文章中说明了dcl的正确用法。 Effectice java中推荐的单例方式写法是使用枚举类型的方式。 外观模式外观模式用来包装一组接口用于方便使用。 比如系统中分10个模块，有个功能需要组合使用所有的模块，这个时候就需要一个包装类包装这10个接口，然后进行业务逻辑的调用。","raw":null,"content":null,"categories":[{"name":"architecture","slug":"architecture","permalink":"http://fangjian0423.github.io/categories/architecture/"}],"tags":[{"name":"java","slug":"java","permalink":"http://fangjian0423.github.io/tags/java/"},{"name":"architecture","slug":"architecture","permalink":"http://fangjian0423.github.io/tags/architecture/"}]},{"title":"SpringCloud网关服务zuul介绍","slug":"springcloud-zuul","date":"2017-02-22T13:33:22.000Z","updated":"2017-05-31T13:55:15.000Z","comments":true,"path":"2017/02/22/springcloud-zuul/","link":"","permalink":"http://fangjian0423.github.io/2017/02/22/springcloud-zuul/","excerpt":"Zuul是Netflix开发的一款提供动态路由、监控、弹性、安全的网关服务。\n使用Zuul网关服务带来的好处是统一向外系统提供REST API，并额外提供了权限控制、负载均衡等功能，并且这些功能是从原先的服务中抽离出来并单独存在的。\nZuul提供了不同类型的filter用于处理请求，这些filter可以让我们实现以下功能：\n\n权限控制和安全性：可以识别认证需要的信息和拒绝不满足条件的请求\n监控：监控请求信息\n动态路由：根据需要动态地路由请求到后台的不同集群\n压力测试\n负载均衡\n静态资源处理：直接在zuul处理静态资源的响应而不需要转发这些请求到内部集群中\n","text":"Zuul是Netflix开发的一款提供动态路由、监控、弹性、安全的网关服务。 使用Zuul网关服务带来的好处是统一向外系统提供REST API，并额外提供了权限控制、负载均衡等功能，并且这些功能是从原先的服务中抽离出来并单独存在的。 Zuul提供了不同类型的filter用于处理请求，这些filter可以让我们实现以下功能： 权限控制和安全性：可以识别认证需要的信息和拒绝不满足条件的请求 监控：监控请求信息 动态路由：根据需要动态地路由请求到后台的不同集群 压力测试 负载均衡 静态资源处理：直接在zuul处理静态资源的响应而不需要转发这些请求到内部集群中 Zuul的执行过程介绍Zuul基于Servlet实现，它封装了Servlet提供的相关接口，并提供了一个全新的api。 ZuulFilter是一个基础的抽象类，定义了一些抽象方法： filterType方法: filter的类型，有”pre”, “route”, “post”, “error”, “static” filterOrder方法：优先级，级别越高，越快被执行 shouldFilter方法：开关，如果是true，run方法会执行，否则不会执行 run方法：filter执行的逻辑操作 ZuulServlet是一个继承自HttpServlet的子类，使用Zuul所有的请求都会被这个Servlet接收并处理。 ZuulServlet覆盖了HttpServlet的service方法，所以不论是get/post/put/delete等方法都会执行相同的操作： @Override public void service(javax.servlet.ServletRequest servletRequest, javax.servlet.ServletResponse servletResponse) throws ServletException, IOException { try { init((HttpServletRequest) servletRequest, (HttpServletResponse) servletResponse); // 初始化ZuulRunner，也就是包装request和response，并设置到RequestContext中，RequestContext使用ThreadLocal获得，每个线程独立保存一份，用于存储各种信息，比如request，response，监控信息，异常信息，成功信息，执行时间等等 // Marks this request as having passed through the &quot;Zuul engine&quot;, as opposed to servlets // explicitly bound in web.xml, for which requests will not have the same data attached RequestContext context = RequestContext.getCurrentContext(); context.setZuulEngineRan(); try { preRoute(); // 执行 pre 类型的filter } catch (ZuulException e) { error(e); // pre 类型的filter执行报错的话执行 error 类型的filter postRoute(); // 执行 post 类型的filter return; } try { route(); // pre 类型的filter执行成功后，执行 route 类型的filter } catch (ZuulException e) { error(e); //route 类型的filter执行报错的话执行 error 类型的filter postRoute(); // 执行 post 类型的filter return; } try { postRoute(); // route 类型的filter执行成功后，执行 post 类型的filter } catch (ZuulException e) { error(e); //post 类型的filter执行报错的话执行 error 类型的filter return; } } catch (Throwable e) { error(new ZuulException(e, 500, &quot;UNHANDLED_EXCEPTION_&quot; + e.getClass().getName())); // 发生其他没有catch的错误的话，执行 error 类型的filter } finally { RequestContext.getCurrentContext().unset(); } } 下图是zuul wiki上对filter的执行过程说明。 从上面的service方法中我们也可以得出：先执行pre类型的filter；如果pre filter执行失败那么执行error和post类型的filter，pre filter执行成功的话执行route类型的filter；如果route filter执行失败那么执行error和post类型的filter，route filter执行成功的话执行post filter；如果post filter执行失败那么执行error类型的filter，post filter执行成功的话，结束。上述过程中执行失败指的是ZuulException被catch，如果是其他Exception的话，那么执行error类型的filter，然后结束。 ZuulServlet里的preRoute(), route(), postRoute(), error()方法详情： ZuulRunner.java public void preRoute() throws ZuulException { FilterProcessor.getInstance().preRoute(); } FilterProcessor.java public void preRoute() throws ZuulException { try { runFilters(&quot;pre&quot;); } catch (Throwable e) { if (e instanceof ZuulException) { throw (ZuulException) e; } throw new ZuulException(e, 500, &quot;UNCAUGHT_EXCEPTION_IN_PRE_FILTER_&quot; + e.getClass().getName()); } } public Object runFilters(String sType) throws Throwable { if (RequestContext.getCurrentContext().debugRouting()) { Debug.addRoutingDebug(&quot;Invoking {&quot; + sType + &quot;} type filters&quot;); } boolean bResult = false; List&lt;ZuulFilter&gt; list = FilterLoader.getInstance().getFiltersByType(sType); // 获得对应类型的filter集合 if (list != null) { for (int i = 0; i &lt; list.size(); i++) { // 遍历这些类型的filter集合 ZuulFilter zuulFilter = list.get(i); Object result = processZuulFilter(zuulFilter); // 调用processZuulFilter方法 if (result != null &amp;&amp; result instanceof Boolean) { bResult |= ((Boolean) result); } } } return bResult; } public Object processZuulFilter(ZuulFilter filter) throws ZuulException { RequestContext ctx = RequestContext.getCurrentContext(); boolean bDebug = ctx.debugRouting(); final String metricPrefix = &quot;zuul.filter-&quot;; long execTime = 0; // 执行时间 String filterName = &quot;&quot;; try { long ltime = System.currentTimeMillis(); // 执行前的时间 filterName = filter.getClass().getSimpleName(); // 获取filter名字 RequestContext copy = null; Object o = null; Throwable t = null; if (bDebug) { Debug.addRoutingDebug(&quot;Filter &quot; + filter.filterType() + &quot; &quot; + filter.filterOrder() + &quot; &quot; + filterName); copy = ctx.copy(); } ZuulFilterResult result = filter.runFilter(); // 调用ZuulFilter的runFilter方法得到ZuulFilterResult，这个类是对filter执行结果的包装，包括返回值、异常信息、状态 ExecutionStatus s = result.getStatus(); // 得到ZuulFilterResult的状态信息 execTime = System.currentTimeMillis() - ltime; // 得到filter的执行时间 switch (s) { // 针对不同的ZuulFilterResult的状态做不同处理 case FAILED: // 如果是FAILED状态，说错run方法执行失败了 t = result.getException(); // 得到失败的异常 ctx.addFilterExecutionSummary(filterName, ExecutionStatus.FAILED.name(), execTime); // 失败信息加到RequestContext中 break; case SUCCESS: // 如果是SUCCESS状态，说明run方法正确执行完毕 o = result.getResult(); // 得到run方法返回的结果 ctx.addFilterExecutionSummary(filterName, ExecutionStatus.SUCCESS.name(), execTime); // 成功信息加到RequestContext中 if (bDebug) { Debug.addRoutingDebug(&quot;Filter {&quot; + filterName + &quot; TYPE:&quot; + filter.filterType() + &quot; ORDER:&quot; + filter.filterOrder() + &quot;} Execution time = &quot; + execTime + &quot;ms&quot;); Debug.compareContextState(filterName, copy); } break; default: // 其他状态的话不做处理 break; } if (t != null) throw t; // 如果是FAILED状态，抛出这个Exception usageNotifier.notify(filter, s); // 记录监控信息 return o; } catch (Throwable e) { // 如果发生了一些其它没有catch的异常 if (bDebug) { Debug.addRoutingDebug(&quot;Running Filter failed &quot; + filterName + &quot; type:&quot; + filter.filterType() + &quot; order:&quot; + filter.filterOrder() + &quot; &quot; + e.getMessage()); } usageNotifier.notify(filter, ExecutionStatus.FAILED); // 记录监控信息 if (e instanceof ZuulException) { throw (ZuulException) e; } else { // 封装成ZuulException并抛出 ZuulException ex = new ZuulException(e, &quot;Filter threw Exception&quot;, 500, filter.filterType() + &quot;:&quot; + filterName); ctx.addFilterExecutionSummary(filterName, ExecutionStatus.FAILED.name(), execTime); throw ex; } } } ZuulFilter.java public ZuulFilterResult runFilter() { ZuulFilterResult zr = new ZuulFilterResult(); if (!isFilterDisabled()) { // 如果对应的zuul filter没有被disable if (shouldFilter()) { // shouldFilter开关是否开启 Tracer t = TracerFactory.instance().startMicroTracer(&quot;ZUUL::&quot; + this.getClass().getSimpleName()); // 设置监控信息 try { Object res = run(); // 调用ZuulFilter的run方法 zr = new ZuulFilterResult(res, ExecutionStatus.SUCCESS); // 把结果封装到ZuulFilterResult中，并设置状态为SUCCESS } catch (Throwable e) { // 如果发生了异常 t.setName(&quot;ZUUL::&quot; + this.getClass().getSimpleName() + &quot; failed&quot;); // 完善监控信息 zr = new ZuulFilterResult(ExecutionStatus.FAILED); // 把ZuulFilterResult状态设置为FAILED zr.setException(e); // 设置异常信息 } finally { t.stopAndLog(); } } else { zr = new ZuulFilterResult(ExecutionStatus.SKIPPED); // zuul filter被disable的话，把ZuulFilterResult状态设置为SKIPPED } } return zr; } 在SpringCloud中使用Zuul在SpringCloud中使用Zuul，加上@EnableZuulProxy注解，这个注解会import ZuulProxyConfiguration配置类。ZuulProxyConfiguration配置类继承ZuulConfiguration类，ZuulConfiguration配置类使用zuul开头的配置。 在这个例子中，本地端口2222有了compute-service服务。这个zuul的服务地址暴露在7777端口下。 我们定义了一个规则： zuul.routes.api-a-url.path=/api-a-url/** zuul.routes.api-a-url.url=http://localhost:2222/ 这个routes对应的类型是Map，key为String，value是一个ZuulRoute。ZuulRoute中定义了一些属性，有： private String id; // 标识一个路由规则 private String path; // 拦截路径，比如 /api-a-url/** private String serviceId; // Eureka服务发现中的serviceId private String url; //不使用服务发现中的服务，独立的一个url private boolean stripPrefix = true; private Boolean retryable; // 是否会retry private Set&lt;String&gt; sensitiveHeaders = new LinkedHashSet&lt;&gt;(); 上面的api-a-url就是对应map中的key，path和url对应ZuulRoute中的path和url属性。 在ZuulProxyConfiguration配置类中，构造了很多bean，比如有ZuulController、ZuulHandlerMapping、DiscoveryClientRouteLocator、各种filter等bean。 其中ZuulController内部使用了ZuulServlet处理http请求，DiscoveryClientRouteLocator使用ZuulProperties中的route解析路由规则，然后封装成org.springframework.cloud.netflix.zuul.filters.Route在getRoutes方法中返回，这个方法会在RoutesEndpoint和ZuulHandlerMapping中被调用。 另外DiscoveryClientRouteLocator会基于服务发现中心中的服务信息，再去寻找对应的路由规则。由于例子中有个本地端口为2222的compute-service服务。所以会被解析并放到路由规则里，这样路由规则里就有2个规则： path为/api-a-url/**，url为http://localhost:2222/ path为/compute-service/**，serviceId为compute-service ZuulHandlerMapping是一个HandlerMapping，用于处理请求的映射关系。在SpringMVC中，默认是使用RequestMappingHandlerMapping处理，而在Zuul中，使用ZuulHandlerMapping处理地址映射关系。它内部有个注册handler方法： private void registerHandlers() { Collection&lt;Route&gt; routes = this.routeLocator.getRoutes(); // 得到路由规则 if (routes.isEmpty()) { this.logger.warn(&quot;No routes found from RouteLocator&quot;); } else { for (Route route : routes) { // 注册路由规则中的地址，对应了handler是zuul属性，这个zuul也就是ZuulController registerHandler(route.getFullPath(), this.zuul); } } } 例子中路由规则里对应的路径有2个，分别是/api-a-url/和/compute-service/，它们对应的handler都是ZuulController。 访问地址： http://localhost:7777/api-a-url/add?a=1&amp;b=2 在ZuulHandlerMapping中的规则路径中发现了/api-a-url/**，于是传递给ZuulController处理，ZuulController传递给ZuulServlet处理。 讲到这里，细心的读者可能会发现一个问题：我们前面讲了这么多关于filter的各种细节，但是真正的服务调用是在哪里执行的? Zuul把真正的服务调用也放在了filter中处理，并在产生的结果放在了RequestContext中。 其中有route类型的filter中使用HttpClient执行，执行结果的stream放到了RequestContext。 post类型的filter读取这个stream并使用response write出去。 我们来简单看下这个过程中一些filter的各自实现。 SimpleHostRoutingFilter这个route类型的filter的shouldFilter方法： @Override public boolean shouldFilter() { // 如果对应的地址是使用host方式，才会生效 return RequestContext.getCurrentContext().getRouteHost() != null &amp;&amp; RequestContext.getCurrentContext().sendZuulResponse(); } run方法： @Override public Object run() { RequestContext context = RequestContext.getCurrentContext(); HttpServletRequest request = context.getRequest(); MultiValueMap&lt;String, String&gt; headers = this.helper .buildZuulRequestHeaders(request); MultiValueMap&lt;String, String&gt; params = this.helper .buildZuulRequestQueryParams(request); String verb = getVerb(request); InputStream requestEntity = getRequestBody(request); if (request.getContentLength() &lt; 0) { context.setChunkedRequestBody(); } String uri = this.helper.buildZuulRequestURI(request); this.helper.addIgnoredHeaders(); try { HttpResponse response = forward(this.httpClient, verb, uri, request, headers, params, requestEntity); // 使用HttpClient调用remoteHost setResponse(response); // 设置remoteHost调用的结果 } catch (Exception ex) { context.set(&quot;error.status_code&quot;, HttpServletResponse.SC_INTERNAL_SERVER_ERROR); context.set(&quot;error.exception&quot;, ex); } return null; } // setResponse方法会把response中的stream放到RequestContext中 context.setResponseDataStream(entity); RibbonRoutingFilter跟SimpleHostRoutingFilter类似，区别就是它的shouldFilter方法里不是判断host方式，而是判断路由规则里是否存在serviceId。它的run方法也是使用HttpClient完成服务的调用，但是它是使用ribbon完成的。 访问地址： http://localhost:7777/compute-service/add?a=1&amp;b=2 由于使用了serviceId的方式，所以会触发RibbonRoutingFilter并完成服务的调用。 当使用Eureka服务发现的时候，建议使用serviceId的方式，而不是直接host的方式。因为基于serviceId的方式会使用ribbon完成服务的调用，ribbon中又使用了hystrix和loadbalance等功能，有更好的健壮性。 SendResponseFilter是一个post类型的，它会写回服务调用产生的结果。 @Override public boolean shouldFilter() { // RibbonRoutingFilter和SimpleHostRoutingFilter都会写入stream数据到RequestContext中的responseDataStream中，所以这个filter会生效 return !RequestContext.getCurrentContext().getZuulResponseHeaders().isEmpty() || RequestContext.getCurrentContext().getResponseDataStream() != null || RequestContext.getCurrentContext().getResponseBody() != null; } @Override public Object run() { try { addResponseHeaders(); // 最终在RequestContext中使用response write这个stream writeResponse(); } catch (Exception ex) { ReflectionUtils.rethrowRuntimeException(ex); } return null; } SpringCloud默认还加了其它的一些拦截器，有兴趣的读者可以自行查看源代码。 总结Zuul内部的处理使用ZuulServlet完成，ZuulServlet继承HttpServlet，重写了service方法，service方法内部分别是pre、route、post和error类型的filter进行调用。这里的不同类型的filter执行顺序文中已经说明。 要在SpringCloud中使用Zuul，需要加上@EnableZuulProxy注解。加上这个注解之后SpringCloud会构造一些bean，比如ZuulHandlerMapping、DiscoveryClientRouteLocator、各种filter等。其中DiscoveryClientRouteLocator是一个基于服务发现的路由规则生成器，它会基于zuul的配置构造路由规则。ZuulHandlerMapping是一个HandlerMapping的实现，它跟基于路由规则注册handler，其中key为路由规则对应的路径，handler都是ZuulController，ZuulController内部使用ZuulServlet进行请求的处理。 Zuul把真正的服务调用放在了filter中实现。它提供了SimpleHostRoutingFilter和RibbonRoutingFilter这2个route类型的filter用于执行服务。从名字也可以看出来，SimpleHostRoutingFilter用于执行基于host方式的调用url接口，RibbonRoutingFilter基于服务发现的方式调用服务。一般我们都建议使用RibbonRoutingFilter，因为它内部使用ribbon，更加健壮。 其它RoutesEndpoint这个endpoint使用RouteLocator中提供的所有路由规则。 访问： http://localhost:7777/routes 得到路由规则： { /api-a-url/**: &quot;http://localhost:2222/&quot;, /compute-service/**: &quot;compute-service&quot; } Zuul声称自己可以使用static类型的filter用于处理静态资源，也提供了一个StaticResponseFilter的一个基类，但是查看ZuulServlet的源码发现没有哪段逻辑是处理静态资源的。 上了github发现专门有个issue说明目前还不支持自定义的filter。 参考资料https://github.com/Netflix/zuul/ https://github.com/Netflix/zuul/wiki","raw":null,"content":null,"categories":[{"name":"springcloud","slug":"springcloud","permalink":"http://fangjian0423.github.io/categories/springcloud/"}],"tags":[{"name":"springcloud","slug":"springcloud","permalink":"http://fangjian0423.github.io/tags/springcloud/"},{"name":"springboot","slug":"springboot","permalink":"http://fangjian0423.github.io/tags/springboot/"}]},{"title":"SpringCloud使用Hystrix实现断路器","slug":"springcloud-hystrix","date":"2017-02-19T04:11:39.000Z","updated":"2017-02-21T08:09:52.000Z","comments":true,"path":"2017/02/19/springcloud-hystrix/","link":"","permalink":"http://fangjian0423.github.io/2017/02/19/springcloud-hystrix/","excerpt":"Hystrix是一个供分布式系统使用，提供延迟和容错功能，保证复杂的分布系统在面临不可避免的失败时，仍能有其弹性。\n比如系统中有很多服务，当某些服务不稳定的时候，使用这些服务的用户线程将会阻塞，如果没有隔离机制，系统随时就有可能会挂掉，从而带来很大的风险。\nSpringCloud使用Hystrix组件提供断路器、资源隔离与自我修复功能。下图表示服务B触发了断路器，阻止了级联失败。\n","text":"Hystrix是一个供分布式系统使用，提供延迟和容错功能，保证复杂的分布系统在面临不可避免的失败时，仍能有其弹性。 比如系统中有很多服务，当某些服务不稳定的时候，使用这些服务的用户线程将会阻塞，如果没有隔离机制，系统随时就有可能会挂掉，从而带来很大的风险。 SpringCloud使用Hystrix组件提供断路器、资源隔离与自我修复功能。下图表示服务B触发了断路器，阻止了级联失败。 Hystrix的简单使用Hystrix使用了命令设计模式，只需要编写命令即可： public class CommandHelloWorld extends HystrixCommand&lt;String&gt; { private final String name; public CommandHelloWorld(String name) { super(HystrixCommandGroupKey.Factory.asKey(&quot;HelloWorld&quot;)); this.name = name; } @Override protected String run() throws Exception { // 完成业务逻辑 return &quot;Hello &quot; + name + &quot;!&quot;; } @Override protected String getFallback() { // run方法抛出异常的时候返回备用结果 return &quot;Hello Failure &quot; + name + &quot;!&quot;; } } 测试用例： @Test public void test() { assertEquals(&quot;Hello World!&quot;, new CommandHelloWorld(&quot;World&quot;).execute()); assertEquals(&quot;Hello Format!&quot;, new CommandHelloWorld(&quot;Format&quot;).execute()); } 可能有的人觉得写Command有点麻烦，Hystrix提供了一个类库javanica，可以使用@HystrixCommand注解完成命令的编写。 在SpringCloud中使用Hystrix要在SpringCloud中使用断路器，需要加上@EnableCircuitBreaker注解： ... @EnableCircuitBreaker ... public class RibbonApplication { ... } 然后在对应的方法上加入@HystrixCommand注解实现断路器功能，当service方法对应的服务发生异常的时候，会跳转到serviceFallback方法执行： @HystrixCommand(fallbackMethod = &quot;serviceFallback&quot;) // 加入@HystrixCommand注解实现断路器功能 public String service() { // 原先的方法 return restTemplate.getForEntity(&quot;...&quot;, String.class).getBody(); } public String serviceFallback() { // fallback方法 return &quot;error&quot;; } 工作原理加上@EnableCircuitBreaker注解之后，就可以使用断路器功能，所以SpringCloud内部是如何整合Hystrix的话先从这个注解开始分析。 @EnableCircuitBreaker注解定义如下： @Target(ElementType.TYPE) @Retention(RetentionPolicy.RUNTIME) @Documented @Inherited @Import(EnableCircuitBreakerImportSelector.class) public @interface EnableCircuitBreaker { } import了EnableCircuitBreakerImportSelector这个selector： public class EnableCircuitBreakerImportSelector extends SpringFactoryImportSelector&lt;EnableCircuitBreaker&gt; { @Override protected boolean isEnabled() { return new RelaxedPropertyResolver(getEnvironment()).getProperty( &quot;spring.cloud.circuit.breaker.enabled&quot;, Boolean.class, Boolean.TRUE); } } 在之前的这篇SpringBoot自动化配置的注解开关原理文章中分析过selector的原理，这个EnableCircuitBreakerImportSelector会加载spring.factories属性文件中key为org.springframework.cloud.client.circuitbreaker.EnableCircuitBreaker的类： org.springframework.cloud.client.circuitbreaker.EnableCircuitBreaker=\\ org.springframework.cloud.netflix.hystrix.HystrixCircuitBreakerConfiguration 会加载HystrixCircuitBreakerConfiguration这个配置类。 这个配置类内部构造了一个aspect： @Bean public HystrixCommandAspect hystrixCommandAspect() { return new HystrixCommandAspect(); } 这个aspect对应的pointcut如下，所以使用@HystrixCommand注解修饰的方法会被这个aspect处理： @annotation(com.netflix.hystrix.contrib.javanica.annotation.HystrixCommand) || @annotation(com.netflix.hystrix.contrib.javanica.annotation.HystrixCollapser) 对应的aop处理方法： public Object methodsAnnotatedWithHystrixCommand(final ProceedingJoinPoint joinPoint) throws Throwable { Method method = getMethodFromTarget(joinPoint); // 得到初始的方法 Validate.notNull(method, &quot;failed to get method from joinPoint: %s&quot;, joinPoint); if (method.isAnnotationPresent(HystrixCommand.class) &amp;&amp; method.isAnnotationPresent(HystrixCollapser.class)) { // 如果使用@HystrixCommand注解和@HystrixCollapser注解同时修改，不允许 throw new IllegalStateException(&quot;method cannot be annotated with HystrixCommand and HystrixCollapser &quot; + &quot;annotations at the same time&quot;); } MetaHolderFactory metaHolderFactory = META_HOLDER_FACTORY_MAP.get(HystrixPointcutType.of(method)); MetaHolder metaHolder = metaHolderFactory.create(joinPoint); // 创建一个MetaHolder，这个MetaHolder封装了方法中的一些以及Hystrix的一些信息 HystrixInvokable invokable = HystrixCommandFactory.getInstance().create(metaHolder); // 根据这个metaHolder创建出一个HystrixInvokable，也就是一个HystrixCommand ExecutionType executionType = metaHolder.isCollapserAnnotationPresent() ? metaHolder.getCollapserExecutionType() : metaHolder.getExecutionType(); // 得到执行类型，有3种类型：1. 异步 2. 同步 3. reactive Object result; try { result = CommandExecutor.execute(invokable, executionType, metaHolder); } catch (HystrixBadRequestException e) { throw e.getCause(); } return result; } CommandExecutor的execute方法： public static Object execute(HystrixInvokable invokable, ExecutionType executionType, MetaHolder metaHolder) throws RuntimeException { Validate.notNull(invokable); Validate.notNull(metaHolder); switch (executionType) { case SYNCHRONOUS: { // 同步方式的话，调用HystrixCommand的execute方法 return castToExecutable(invokable, executionType).execute(); } case ASYNCHRONOUS: { // 异步方式的话，调用HystrixCommand的queue方法 HystrixExecutable executable = castToExecutable(invokable, executionType); if (metaHolder.hasFallbackMethodCommand() &amp;&amp; ExecutionType.ASYNCHRONOUS == metaHolder.getFallbackExecutionType()) { return new FutureDecorator(executable.queue()); } return executable.queue(); } case OBSERVABLE: { // reactive方式的话，调用HystrixCommand的observe或者toObservable方法 HystrixObservable observable = castToObservable(invokable); return ObservableExecutionMode.EAGER == metaHolder.getObservableExecutionMode() ? observable.observe() : observable.toObservable(); } default: throw new RuntimeException(&quot;unsupported execution type: &quot; + executionType); } } 根据metaHolder创建出HystrixCommand的过程在HystrixCommandBuilderFactory中： return HystrixCommandBuilder.builder() .setterBuilder(createGenericSetterBuilder(metaHolder)) .commandActions(createCommandActions(metaHolder)) .collapsedRequests(collapsedRequests) .cacheResultInvocationContext(createCacheResultInvocationContext(metaHolder)) .cacheRemoveInvocationContext(createCacheRemoveInvocationContext(metaHolder)) .ignoreExceptions(metaHolder.getHystrixCommand().ignoreExceptions()) .executionType(metaHolder.getExecutionType()) .build(); 所以这个aspect的作用就是把一个普通的Java方法转换成HystrixCommand。 其它HystrixCircuitBreakerConfiguration配置类中有个HystrixWebConfiguration内部配置类，它构造了一个HystrixStreamEndpoint这个endpoint，这个endpoint使用HystrixMetricsStreamServlet暴露出/hystrix.stream地址来获取hystrix的metrics信息。 Hystrix还提供了一个dashboard，这个dashboard可以查看各个断路器的健康状况，要使用这个dashboard，在项目中加入这些依赖： &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-hystrix-dashboard&lt;/artifactId&gt; &lt;/dependency&gt; 然后在代码里加上开关： @EnableHystrixDashboard ... 启动项目，打开： http://localhost:3333/hystrix 输入： http://localhost:3333/hystrix.stream 我们使用wrk模拟请求： wrk -c 10 -t 10 -d 20s http://localhost:3333/add 然后dashboard中发生了变化： 参考资料http://cloud.spring.io/spring-cloud-static/Brixton.SR7/ https://github.com/Netflix/Hystrix https://github.com/Netflix/Hystrix/wiki","raw":null,"content":null,"categories":[{"name":"springcloud","slug":"springcloud","permalink":"http://fangjian0423.github.io/categories/springcloud/"}],"tags":[{"name":"springcloud","slug":"springcloud","permalink":"http://fangjian0423.github.io/tags/springcloud/"},{"name":"springboot","slug":"springboot","permalink":"http://fangjian0423.github.io/tags/springboot/"}]},{"title":"Loadbalance的几种算法以及在ribbon中的使用","slug":"loadbalance","date":"2017-01-29T11:32:16.000Z","updated":"2017-01-29T11:47:10.000Z","comments":true,"path":"2017/01/29/loadbalance/","link":"","permalink":"http://fangjian0423.github.io/2017/01/29/loadbalance/","excerpt":"Load Balance负载均衡是用于解决一台机器(一个进程)无法解决所有请求而产生的一种算法。\n像nginx可以使用负载均衡分配流量，ribbon为客户端提供负载均衡，dubbo服务调用里的负载均衡等等，很多地方都使用到了负载均衡。\n使用负载均衡带来的好处很明显：\n\n当集群里的1台或者多台服务器down的时候，剩余的没有down的服务器可以保证服务的继续使用\n使用了更多的机器保证了机器的良性使用，不会由于某一高峰时刻导致系统cpu急剧上升\n\n负载均衡有好几种实现策略，常见的有：\n\n随机 (Random)\n轮询 (RoundRobin)\n一致性哈希 (ConsistentHash)\n哈希 (Hash)\n加权（Weighted）\n","text":"Load Balance负载均衡是用于解决一台机器(一个进程)无法解决所有请求而产生的一种算法。 像nginx可以使用负载均衡分配流量，ribbon为客户端提供负载均衡，dubbo服务调用里的负载均衡等等，很多地方都使用到了负载均衡。 使用负载均衡带来的好处很明显： 当集群里的1台或者多台服务器down的时候，剩余的没有down的服务器可以保证服务的继续使用 使用了更多的机器保证了机器的良性使用，不会由于某一高峰时刻导致系统cpu急剧上升 负载均衡有好几种实现策略，常见的有： 随机 (Random) 轮询 (RoundRobin) 一致性哈希 (ConsistentHash) 哈希 (Hash) 加权（Weighted） 我们以ribbon的实现为基础，看看其中的一些算法是如何实现的。 ribbon是一个为客户端提供负载均衡功能的服务，它内部提供了一个叫做ILoadBalance的接口代表负载均衡器的操作，比如有添加服务器操作、选择服务器操作、获取所有的服务器列表、获取可用的服务器列表等等。 还提供了一个叫做IRule的接口代表负载均衡策略： public interface IRule{ public Server choose(Object key); public void setLoadBalancer(ILoadBalancer lb); public ILoadBalancer getLoadBalancer(); } IRule接口的实现类有以下几种： 其中RandomRule表示随机策略、RoundRobin表示轮询策略、WeightedResponseTimeRule表示加权策略、BestAvailableRule表示请求数最少策略等等。 随机策略很简单，就是从服务器中随机选择一个服务器，RandomRule的实现代码如下： public Server choose(ILoadBalancer lb, Object key) { if (lb == null) { return null; } Server server = null; while (server == null) { if (Thread.interrupted()) { return null; } List&lt;Server&gt; upList = lb.getReachableServers(); List&lt;Server&gt; allList = lb.getAllServers(); int serverCount = allList.size(); if (serverCount == 0) { return null; } int index = rand.nextInt(serverCount); // 使用jdk内部的Random类随机获取索引值index server = upList.get(index); // 得到服务器实例 if (server == null) { Thread.yield(); continue; } if (server.isAlive()) { return (server); } server = null; Thread.yield(); } return server; } RoundRobin轮询策略表示每次都取下一个服务器，比如一共有5台服务器，第1次取第1台，第2次取第2台，第3次取第3台，以此类推： public Server choose(ILoadBalancer lb, Object key) { if (lb == null) { log.warn(&quot;no load balancer&quot;); return null; } Server server = null; int count = 0; while (server == null &amp;&amp; count++ &lt; 10) { // retry 10 次 List&lt;Server&gt; reachableServers = lb.getReachableServers(); List&lt;Server&gt; allServers = lb.getAllServers(); int upCount = reachableServers.size(); int serverCount = allServers.size(); if ((upCount == 0) || (serverCount == 0)) { log.warn(&quot;No up servers available from load balancer: &quot; + lb); return null; } int nextServerIndex = incrementAndGetModulo(serverCount); // incrementAndGetModulo方法内部使用nextServerCyclicCounter这个AtomicInteger属性原子递增对serverCount取模得到索引值 server = allServers.get(nextServerIndex); // 得到服务器实例 if (server == null) { Thread.yield(); continue; } if (server.isAlive() &amp;&amp; (server.isReadyToServe())) { return (server); } server = null; } if (count &gt;= 10) { log.warn(&quot;No available alive servers after 10 tries from load balancer: &quot; + lb); } return server; } BestAvailableRule策略用来选取最少并发量请求的服务器： public Server choose(Object key) { if (loadBalancerStats == null) { return super.choose(key); } List&lt;Server&gt; serverList = getLoadBalancer().getAllServers(); // 获取所有的服务器列表 int minimalConcurrentConnections = Integer.MAX_VALUE; long currentTime = System.currentTimeMillis(); Server chosen = null; for (Server server: serverList) { // 遍历每个服务器 ServerStats serverStats = loadBalancerStats.getSingleServerStat(server); // 获取各个服务器的状态 if (!serverStats.isCircuitBreakerTripped(currentTime)) { // 没有触发断路器的话继续执行 int concurrentConnections = serverStats.getActiveRequestsCount(currentTime); // 获取当前服务器的请求个数 if (concurrentConnections &lt; minimalConcurrentConnections) { // 比较各个服务器之间的请求数，然后选取请求数最少的服务器并放到chosen变量中 minimalConcurrentConnections = concurrentConnections; chosen = server; } } } if (chosen == null) { // 如果没有选上，调用父类ClientConfigEnabledRoundRobinRule的choose方法，也就是使用RoundRobinRule轮询的方式进行负载均衡 return super.choose(key); } else { return chosen; } } 实例验证Ribbon中的LoadBalance功能ServerList中提供了3个instance，分别是： compute-service:2222 compute-service:2223 compute-service:2224 然后使用不同的IRule策略查看负载均衡的实现。 首先先使用ribbon提供的LoadBalanced注解加在RestTemplate上面，这个注解会自动构造LoadBalancerClient接口的实现类并注册到Spring容器中。 @Bean @LoadBalanced RestTemplate restTemplate() { return new RestTemplate(); } 接下来使用RestTemplate进行rest操作的时候，会自动使用负载均衡策略，它内部会在RestTemplate中加入LoadBalancerInterceptor这个拦截器，这个拦截器的作用就是使用负载均衡。 例子中，我们的实例的name叫做compute-service，里面提供了一个方法add用于相加2个Integer类型的数值。 loadbalance的具体操作： public String loadbalance() { ServiceInstance serviceInstance = loadBalancerClient.choose(&quot;compute-service&quot;); StringBuilder sb = new StringBuilder(); sb.append(&quot;host: &quot;).append(serviceInstance.getHost()).append(&quot;, &quot;); sb.append(&quot;port: &quot;).append(serviceInstance.getPort()).append(&quot;, &quot;); sb.append(&quot;uri: &quot;).append(serviceInstance.getUri()); return sb.toString(); } RandomRule随机策略RandomRule： @Configuration public class RibbonConfiguration { @Autowired private SpringClientFactory springClientFactory; @Bean public IRule ribbonRule() { return new RandomRule(); } } 测试结果如下，确实是随机获取的： host: 192.168.31.113, port: 2222, uri: http://192.168.31.113:2222 host: 192.168.31.113, port: 2222, uri: http://192.168.31.113:2222 host: 192.168.31.113, port: 2222, uri: http://192.168.31.113:2222 host: 192.168.31.113, port: 2224, uri: http://192.168.31.113:2224 host: 192.168.31.113, port: 2224, uri: http://192.168.31.113:2224 host: 192.168.31.113, port: 2222, uri: http://192.168.31.113:2222 host: 192.168.31.113, port: 2223, uri: http://192.168.31.113:2223 host: 192.168.31.113, port: 2222, uri: http://192.168.31.113:2222 host: 192.168.31.113, port: 2222, uri: http://192.168.31.113:2222 host: 192.168.31.113, port: 2223, uri: http://192.168.31.113:2223 RoundRobinRule轮询策略RoundRobinRule： @Bean public IRule ribbonRule() { return new RandomRule(); } 测试结果如下，确实是轮询每个服务器的： host: 192.168.31.113, port: 2223, uri: http://192.168.31.113:2223 host: 192.168.31.113, port: 2222, uri: http://192.168.31.113:2222 host: 192.168.31.113, port: 2224, uri: http://192.168.31.113:2224 host: 192.168.31.113, port: 2223, uri: http://192.168.31.113:2223 host: 192.168.31.113, port: 2222, uri: http://192.168.31.113:2222 host: 192.168.31.113, port: 2224, uri: http://192.168.31.113:2224 host: 192.168.31.113, port: 2223, uri: http://192.168.31.113:2223 host: 192.168.31.113, port: 2222, uri: http://192.168.31.113:2222 host: 192.168.31.113, port: 2224, uri: http://192.168.31.113:2224 host: 192.168.31.113, port: 2223, uri: http://192.168.31.113:2223 host: 192.168.31.113, port: 2222, uri: http://192.168.31.113:2222 host: 192.168.31.113, port: 2224, uri: http://192.168.31.113:2224 BestAvailableRule最少并发数策略BestAvailableRule： @Bean public IRule ribbonRule() { return new BestAvailableRule(); } 如果直接访问浏览器的话，测试结果如下(因为每次访问完请求数都变成0，下次遍历永远都是2223这个端口的实例)： host: 192.168.31.113, port: 2223, uri: http://192.168.31.113:2223 host: 192.168.31.113, port: 2223, uri: http://192.168.31.113:2223 host: 192.168.31.113, port: 2223, uri: http://192.168.31.113:2223 host: 192.168.31.113, port: 2223, uri: http://192.168.31.113:2223 host: 192.168.31.113, port: 2223, uri: http://192.168.31.113:2223 host: 192.168.31.113, port: 2223, uri: http://192.168.31.113:2223 .. 使用wrk模拟并发请求，结果会出现多个实例： wrk -c 1000 -t 10 -d 10s http://localhost:3333/test","raw":null,"content":null,"categories":[{"name":"springcloud","slug":"springcloud","permalink":"http://fangjian0423.github.io/categories/springcloud/"}],"tags":[{"name":"algorithm","slug":"algorithm","permalink":"http://fangjian0423.github.io/tags/algorithm/"},{"name":"springcloud","slug":"springcloud","permalink":"http://fangjian0423.github.io/tags/springcloud/"}]},{"title":"2016总结","slug":"2016_end","date":"2017-01-01T14:28:04.000Z","updated":"2017-01-01T14:28:28.000Z","comments":true,"path":"2017/01/01/2016_end/","link":"","permalink":"http://fangjian0423.github.io/2017/01/01/2016_end/","excerpt":"","text":"2016年总结2016年过去了，转眼已经毕业3年多了，时间过得很快。对自己有点失望，感觉技术还没有达到自己期望的那个3年水平。 有进步的地方： blog从每天的200-300pv增加到了500-600pv。2016年一年达到了9w4 pv，没有突破10w，有点小遗憾。不过这不是重点，写博客的初衷是为了自己对知识点的总结，并不在乎pv 补了一下java的基础知识；包括常用数据结构底层的实现，数组、链表、哈希表、红黑树、跳表等；aqs、juc包下的各个工具类实现原理；jvm的一些知识点。基础好真的很关键，不然看一些东西的时候，里面的一些知识点是基础知识，不了解的话还得重新去补基础知识 把springboot的实现原理看了一下，这样就可以知道官方或者一些第三方提供的starter底层到底做了什么事情。不然只能查阅官方文档，文档里如果又没写相关内容的话，还是得看源码解决。最后自己尝试着写了个starter练练手 熟悉了一下springcloud和dubbo这2个框架。以前rpc相关的只是用过hessian框架，发现dubbo很强大，封装地也很好，而且文档也很详细，是一个值得学习的好框架 学习了一下netty和nio相关的内容，由于工作中没有用到，算是自我补充了知识点 把flume的一些组件的源码看了一遍，发现了有不合理的地方，但是进行改进的话也会引入其他的一些问题。flume的实现还是有缺陷的，需要配合一些额外的操作才能把数据收集这块做得更好。比如实时报错，错误数据重跑，重启策略等 写了spark相关的计算，由于精力有限，并没有深入地研究spark的东西 看了分布式相关的一些内容，比如一致性哈希、心跳检测、分布式锁、cap原理等 懂了一些道理： 自己觉得对的事情，一定要尽早去做，不然可能会来不及，如果错过后，就会很后悔 拥有的时候，一定要珍惜。等到失去后，会很后悔 要有耐心地听别人说话，不能显得很不耐烦 对自己说的话要负责，话说出去就收不回来了 做决定前，要冷静。伤害了别人也等于伤害了自己 换位思考。对别人做的事，别人反过来也对你这么做，自己是什么感受 2017年的计划还没有想过，最近发生了很多事，等稳定下来后再去定计划。","raw":null,"content":null,"categories":[{"name":"总结","slug":"总结","permalink":"http://fangjian0423.github.io/categories/总结/"}],"tags":[{"name":"杂事","slug":"杂事","permalink":"http://fangjian0423.github.io/tags/杂事/"}]},{"title":"SpringBoot编写自定义的starter","slug":"springboot-custom-starter","date":"2016-11-15T17:40:32.000Z","updated":"2016-11-23T07:32:21.000Z","comments":true,"path":"2016/11/16/springboot-custom-starter/","link":"","permalink":"http://fangjian0423.github.io/2016/11/16/springboot-custom-starter/","excerpt":"在之前的文章中，我们分析过SpringBoot内部的自动化配置原理和自动化配置注解开关原理。\n我们先简单分析一下mybatis starter的编写，然后再编写自定义的starter。\nmybatis中的autoconfigure模块中使用了一个叫做MybatisAutoConfiguration的自动化配置类。\n这个MybatisAutoConfiguration需要在这些Condition条件下才会执行：\n\n@ConditionalOnClass({ SqlSessionFactory.class, SqlSessionFactoryBean.class })。需要SqlSessionFactory和SqlSessionFactoryBean在classpath中都存在\n@ConditionalOnBean(DataSource.class)。 spring factory中需要存在一个DataSource的bean\n@AutoConfigureAfter(DataSourceAutoConfiguration.class)。需要在DataSourceAutoConfiguration自动化配置之后进行配置，因为mybatis需要数据源的支持\n\n同时在META-INF目录下有个spring.factories这个properties文件，而且它的key为org.springframework.boot.autoconfigure.EnableAutoConfiguration，这样才会被springboot加载：\n# Auto Configure\norg.springframework.boot.autoconfigure.EnableAutoConfiguration=\\\norg.mybatis.spring.boot.autoconfigure.MybatisAutoConfiguration\n有了这些东西之后，mybatis相关的配置会被自动加入到spring container中，只要在maven中加入starter即可：\n&lt;dependency&gt;\n    &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt;\n    &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt;\n    &lt;version&gt;1.1.1&lt;/version&gt;\n&lt;/dependency&gt;\n","text":"在之前的文章中，我们分析过SpringBoot内部的自动化配置原理和自动化配置注解开关原理。 我们先简单分析一下mybatis starter的编写，然后再编写自定义的starter。 mybatis中的autoconfigure模块中使用了一个叫做MybatisAutoConfiguration的自动化配置类。 这个MybatisAutoConfiguration需要在这些Condition条件下才会执行： @ConditionalOnClass({ SqlSessionFactory.class, SqlSessionFactoryBean.class })。需要SqlSessionFactory和SqlSessionFactoryBean在classpath中都存在 @ConditionalOnBean(DataSource.class)。 spring factory中需要存在一个DataSource的bean @AutoConfigureAfter(DataSourceAutoConfiguration.class)。需要在DataSourceAutoConfiguration自动化配置之后进行配置，因为mybatis需要数据源的支持 同时在META-INF目录下有个spring.factories这个properties文件，而且它的key为org.springframework.boot.autoconfigure.EnableAutoConfiguration，这样才会被springboot加载： # Auto Configure org.springframework.boot.autoconfigure.EnableAutoConfiguration=\\ org.mybatis.spring.boot.autoconfigure.MybatisAutoConfiguration 有了这些东西之后，mybatis相关的配置会被自动加入到spring container中，只要在maven中加入starter即可： &lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.1.1&lt;/version&gt; &lt;/dependency&gt; 编写自定义的starter接下来，我们来编写自定义的starter：log-starter。 这个starter内部定义了一个注解，使用这个注解修饰方法之后，该方法的调用会在日志中被打印并且还会打印出方法的耗时。starter支持exclude配置，在exclude中出现的方法不会进行计算。 pom文件： &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starters&lt;/artifactId&gt; &lt;version&gt;1.3.5.RELEASE&lt;/version&gt; &lt;/parent&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 定义修饰方法的注解@Log： package me.format.springboot.log.annotation; import java.lang.annotation.ElementType; import java.lang.annotation.Retention; import java.lang.annotation.RetentionPolicy; import java.lang.annotation.Target; @Retention(RetentionPolicy.RUNTIME) @Target(ElementType.METHOD) public @interface Log { } 然后是配置类： package me.format.springboot.log.autoconfigure; import org.springframework.boot.context.properties.ConfigurationProperties; import org.springframework.util.StringUtils; import javax.annotation.PostConstruct; @ConfigurationProperties(prefix = &quot;mylog&quot;) public class LogProperties { private String exclude; private String[] excludeArr; @PostConstruct public void init() { this.excludeArr = StringUtils.split(exclude, &quot;,&quot;); } public String getExclude() { return exclude; } public void setExclude(String exclude) { this.exclude = exclude; } public String[] getExcludeArr() { return excludeArr; } } 接下来是AutoConfiguration： package me.format.springboot.log.autoconfigure; import me.format.springboot.log.annotation.Log; import me.format.springboot.log.aop.LogMethodInterceptor; import org.aopalliance.aop.Advice; import org.slf4j.Logger; import org.slf4j.LoggerFactory; import org.springframework.aop.Pointcut; import org.springframework.aop.support.AbstractPointcutAdvisor; import org.springframework.aop.support.annotation.AnnotationMatchingPointcut; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.boot.context.properties.EnableConfigurationProperties; import org.springframework.context.annotation.Configuration; import javax.annotation.PostConstruct; @Configuration @EnableConfigurationProperties(LogProperties.class) public class LogAutoConfiguration extends AbstractPointcutAdvisor { private Logger logger = LoggerFactory.getLogger(LogAutoConfiguration.class); private Pointcut pointcut; private Advice advice; @Autowired private LogProperties logProperties; @PostConstruct public void init() { logger.info(&quot;init LogAutoConfiguration start&quot;); this.pointcut = new AnnotationMatchingPointcut(null, Log.class); this.advice = new LogMethodInterceptor(logProperties.getExcludeArr()); logger.info(&quot;init LogAutoConfiguration end&quot;); } @Override public Pointcut getPointcut() { return this.pointcut; } @Override public Advice getAdvice() { return this.advice; } } 由于计算方法调用的时候需要使用aop相关的lib，所以我们的AutoConfiguration继承了AbstractPointcutAdvisor。这样就有了Pointcut和Advice。Pointcut是一个支持注解的修饰方法的Pointcut，Advice则自己实现： package me.format.springboot.log.aop; import org.aopalliance.intercept.MethodInterceptor; import org.aopalliance.intercept.MethodInvocation; import org.slf4j.Logger; import org.slf4j.LoggerFactory; import java.util.Arrays; import java.util.List; public class LogMethodInterceptor implements MethodInterceptor { private Logger logger = LoggerFactory.getLogger(LogMethodInterceptor.class); private List&lt;String&gt; exclude; public LogMethodInterceptor(String[] exclude) { this.exclude = Arrays.asList(exclude); } @Override public Object invoke(MethodInvocation invocation) throws Throwable { String methodName = invocation.getMethod().getName(); if(exclude.contains(methodName)) { return invocation.proceed(); } long start = System.currentTimeMillis(); Object result = invocation.proceed(); long end = System.currentTimeMillis(); logger.info(&quot;====method({}), cost({}) &quot;, methodName, (end - start)); return result; } } 最后resources/META-INF/spring.factories中加入这个AutoConfiguration： org.springframework.boot.autoconfigure.EnableAutoConfiguration=\\ me.format.springboot.log.autoconfigure.LogAutoConfiguration 我们在项目中使用这个log-starter： &lt;dependency&gt; &lt;groupId&gt;me.format.springboot&lt;/groupId&gt; &lt;artifactId&gt;log-starter&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; 使用配置： mylog.exclude=core,log 然后编写一个简单的Service： @Service public class SimpleService { @Log public void test(int num) { System.out.println(&quot;----test---- &quot; + num); } @Log public void core(int num) { System.out.println(&quot;----core---- &quot; + num); } public void work(int num) { System.out.println(&quot;----work---- &quot; + num); } } 使用单元测试分别调用这3个方法，由于work方法没有加上@Log注解，core方法虽然加上了@Log注解，但是在配置中被exclude了，只有test方法可以正常计算耗时： ----test---- 666 2016-11-16 01:29:32.255 INFO 41010 --- [ main] m.f.s.log.aop.LogMethodInterceptor : ====method(test), cost(36) ----work---- 666 ----core---- 666 总结： 自定义springboot的starter，注意这两点。 如果自动化配置类需要在程序启动的时候就加载，可以在META-INF/spring.factories文件中定义。如果本次加载还需要其他一些lib的话，可以使用ConditionalOnClass注解协助 如果自动化配置类要在使用自定义注解后才加载，可以使用自定义注解+@Import注解或@ImportSelector注解完成 参考： http://www.jianshu.com/p/85460c1d835a http://docs.spring.io/spring-boot/docs/current/reference/html/boot-features-developing-auto-configuration.html","raw":null,"content":null,"categories":[{"name":"springboot","slug":"springboot","permalink":"http://fangjian0423.github.io/categories/springboot/"}],"tags":[{"name":"springboot","slug":"springboot","permalink":"http://fangjian0423.github.io/tags/springboot/"}]},{"title":"SpringBoot自动化配置的注解开关原理","slug":"springboot-enable-annotation","date":"2016-11-13T08:22:51.000Z","updated":"2016-11-23T07:29:21.000Z","comments":true,"path":"2016/11/13/springboot-enable-annotation/","link":"","permalink":"http://fangjian0423.github.io/2016/11/13/springboot-enable-annotation/","excerpt":"在之前我们分析SpringBoot的自动化配置原理的时候，分析了freemarker的自动化配置类FreeMarkerAutoConfiguration，这个自动化配置类需要classloader中的一些类需要存在并且在其他的一些配置类之后进行加载。\n但是还存在一些自动化配置类，它们需要在使用一些注解开关的情况下才会生效。比如spring-boot-starter-batch里的@EnableBatchProcessing注解、@EnableCaching等。","text":"在之前我们分析SpringBoot的自动化配置原理的时候，分析了freemarker的自动化配置类FreeMarkerAutoConfiguration，这个自动化配置类需要classloader中的一些类需要存在并且在其他的一些配置类之后进行加载。 但是还存在一些自动化配置类，它们需要在使用一些注解开关的情况下才会生效。比如spring-boot-starter-batch里的@EnableBatchProcessing注解、@EnableCaching等。 一个需求在分析这些开关的原理之前，我们来看一个需求： 定义一个Annotation，让使用了这个Annotaion的应用程序自动化地注入一些类或者做一些底层的事情。 我们会使用Spring提供的@Import注解配合一个配置类来完成。 我们以一个最简单的例子来完成这个需求：定义一个注解EnableContentService，使用了这个注解的程序会自动注入ContentService这个bean。 @Retention(RetentionPolicy.RUNTIME) @Target(ElementType.TYPE) @Import(ContentConfiguration.class) public @interface EnableContentService {} public interface ContentService { void doSomething(); } public class SimpleContentService implements ContentService { @Override public void doSomething() { System.out.println(&quot;do some simple things&quot;); } } 然后在应用程序的入口加上@EnableContentService注解。 这样的话，ContentService就被注入进来了。 SpringBoot也就是用这个完成的。只不过它用了更加高级点的ImportSelector。 ImportSelector的使用用了ImportSelector之后，我们可以在Annotation上添加一些属性，然后根据属性的不同加载不同的bean。 我们在@EnableContentService注解添加属性policy，同时Import一个Selector。 @Retention(RetentionPolicy.RUNTIME) @Target(ElementType.TYPE) @Import(ContentImportSelector.class) public @interface EnableContentService { String policy() default &quot;simple&quot;; } 这个ContentImportSelector根据EnableContentService注解里的policy加载不同的bean。 public class ContentImportSelector implements ImportSelector { @Override public String[] selectImports(AnnotationMetadata importingClassMetadata) { Class&lt;?&gt; annotationType = EnableContentService.class; AnnotationAttributes attributes = AnnotationAttributes.fromMap(importingClassMetadata.getAnnotationAttributes( annotationType.getName(), false)); String policy = attributes.getString(&quot;policy&quot;); if (&quot;core&quot;.equals(policy)) { return new String[] { CoreContentConfiguration.class.getName() }; } else { return new String[] { SimpleContentConfiguration.class.getName() }; } } } CoreContentService和CoreContentConfiguration如下： public class CoreContentService implements ContentService { @Override public void doSomething() { System.out.println(&quot;do some import things&quot;); } } public class CoreContentConfiguration { @Bean public ContentService contentService() { return new CoreContentService(); } } 这样的话，如果在@EnableContentService注解的policy中使用core的话，应用程序会自动加载CoreContentService，否则会加载SimpleContentService。 ImportSelector在SpringBoot中的使用SpringBoot里的ImportSelector是通过SpringBoot提供的@EnableAutoConfiguration这个注解里完成的。 这个@EnableAutoConfiguration注解可以显式地调用，否则它会在@SpringBootApplication注解中隐式地被调用。 @EnableAutoConfiguration注解中使用了EnableAutoConfigurationImportSelector作为ImportSelector。下面这段代码就是EnableAutoConfigurationImportSelector中进行选择的具体代码： @Override public String[] selectImports(AnnotationMetadata metadata) { try { AnnotationAttributes attributes = getAttributes(metadata); List&lt;String&gt; configurations = getCandidateConfigurations(metadata, attributes); configurations = removeDuplicates(configurations); // 删除重复的配置 Set&lt;String&gt; exclusions = getExclusions(metadata, attributes); // 去掉需要exclude的配置 configurations.removeAll(exclusions); configurations = sort(configurations); // 排序 recordWithConditionEvaluationReport(configurations, exclusions); return configurations.toArray(new String[configurations.size()]); } catch (IOException ex) { throw new IllegalStateException(ex); } } 其中getCandidateConfigurations方法将获取配置类： protected List&lt;String&gt; getCandidateConfigurations(AnnotationMetadata metadata, AnnotationAttributes attributes) { return SpringFactoriesLoader.loadFactoryNames( getSpringFactoriesLoaderFactoryClass(), getBeanClassLoader()); } SpringFactoriesLoader.loadFactoryNames方法会根据FACTORIES_RESOURCE_LOCATION这个静态变量从所有的jar包中读取META-INF/spring.factories文件信息： public static List&lt;String&gt; loadFactoryNames(Class&lt;?&gt; factoryClass, ClassLoader classLoader) { String factoryClassName = factoryClass.getName(); try { Enumeration&lt;URL&gt; urls = (classLoader != null ? classLoader.getResources(FACTORIES_RESOURCE_LOCATION) : ClassLoader.getSystemResources(FACTORIES_RESOURCE_LOCATION)); List&lt;String&gt; result = new ArrayList&lt;String&gt;(); while (urls.hasMoreElements()) { URL url = urls.nextElement(); Properties properties = PropertiesLoaderUtils.loadProperties(new UrlResource(url)); String factoryClassNames = properties.getProperty(factoryClassName); // 只会过滤出key为factoryClassNames的值 result.addAll(Arrays.asList(StringUtils.commaDelimitedListToStringArray(factoryClassNames))); } return result; } catch (IOException ex) { throw new IllegalArgumentException(&quot;Unable to load [&quot; + factoryClass.getName() + &quot;] factories from location [&quot; + FACTORIES_RESOURCE_LOCATION + &quot;]&quot;, ex); } } getCandidateConfigurations方法中的getSpringFactoriesLoaderFactoryClass方法返回的是EnableAutoConfiguration.class，所以会过滤出key为org.springframework.boot.autoconfigure.EnableAutoConfiguration的值。 下面这段配置代码就是autoconfigure这个jar包里的spring.factories文件的一部分内容(有个key为org.springframework.boot.autoconfigure.EnableAutoConfiguration，所以会得到这些AutoConfiguration)： # Initializers org.springframework.context.ApplicationContextInitializer=\\ org.springframework.boot.autoconfigure.logging.AutoConfigurationReportLoggingInitializer # Application Listeners org.springframework.context.ApplicationListener=\\ org.springframework.boot.autoconfigure.BackgroundPreinitializer # Auto Configure org.springframework.boot.autoconfigure.EnableAutoConfiguration=\\ org.springframework.boot.autoconfigure.admin.SpringApplicationAdminJmxAutoConfiguration,\\ org.springframework.boot.autoconfigure.aop.AopAutoConfiguration,\\ org.springframework.boot.autoconfigure.amqp.RabbitAutoConfiguration,\\ org.springframework.boot.autoconfigure.MessageSourceAutoConfiguration,\\ 当然了，这些AutoConfiguration不是所有都会加载的，会根据AutoConfiguration上的@ConditionalOnClass等条件判断是否加载。 上面这个例子说的读取properties文件的时候只会过滤出key为org.springframework.boot.autoconfigure.EnableAutoConfiguration的值。 SpringBoot内部还有一些其他的key用于过滤得到需要加载的类： org.springframework.test.context.TestExecutionListener org.springframework.beans.BeanInfoFactory org.springframework.context.ApplicationContextInitializer org.springframework.context.ApplicationListener org.springframework.boot.SpringApplicationRunListener org.springframework.boot.env.EnvironmentPostProcessor org.springframework.boot.env.PropertySourceLoader","raw":null,"content":null,"categories":[{"name":"springboot","slug":"springboot","permalink":"http://fangjian0423.github.io/categories/springboot/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://fangjian0423.github.io/tags/spring/"},{"name":"springboot","slug":"springboot","permalink":"http://fangjian0423.github.io/tags/springboot/"}]},{"title":"SpringBatch中的retry和skip机制实现分析","slug":"springbatch-retry-skip","date":"2016-11-09T00:15:22.000Z","updated":"2016-11-09T04:22:25.000Z","comments":true,"path":"2016/11/09/springbatch-retry-skip/","link":"","permalink":"http://fangjian0423.github.io/2016/11/09/springbatch-retry-skip/","excerpt":"SpringBatch是spring框架下的一个子模块，用于处理批处理的批次框架。\n本文主要分析SpringBatch中的retry和skip机制的实现。\n先简单说明下SpringBatch在SpringBoot中的使用。\n如果要在springboot中使用batch的话，直接加入以下依赖即可：\n&lt;dependency&gt;\n    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-boot-starter-batch&lt;/artifactId&gt;\n&lt;/dependency&gt;\n然后使用注解开启Batch模块：\n...\n@EnableBatchProcessing\npublic class Application { ... }\n之后就可以注入JobBuilderFactory和StepBuilderFactory：\n@Autowired\nprivate JobBuilderFactory jobs;\n\n@Autowired\nprivate StepBuilderFactory steps;\n有了这2个factory之后，就可以build job。","text":"SpringBatch是spring框架下的一个子模块，用于处理批处理的批次框架。 本文主要分析SpringBatch中的retry和skip机制的实现。 先简单说明下SpringBatch在SpringBoot中的使用。 如果要在springboot中使用batch的话，直接加入以下依赖即可： &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-batch&lt;/artifactId&gt; &lt;/dependency&gt; 然后使用注解开启Batch模块： ... @EnableBatchProcessing public class Application { ... } 之后就可以注入JobBuilderFactory和StepBuilderFactory： @Autowired private JobBuilderFactory jobs; @Autowired private StepBuilderFactory steps; 有了这2个factory之后，就可以build job。 SpringBatch中的相关基础概念比如ItemReader、ItemWriter、Chunk等本文就不介绍了。 我们以FlatFileItemReader作为reader，一个自定义Writer用于打印reader中读取出来的数据。 这个定义的writer遇到good job这条数据的时候会报错，具体逻辑如下： @Override public void write(List&lt;? extends String&gt; items) throws Exception { System.out.println(&quot;handle start =====&quot; + items); for(String a : items) { if(a.equals(&quot;good job&quot;)) { throw new Exception(&quot;custom exception&quot;); } } System.out.println(&quot;handle end.. -----&quot; + items); } 其中reader中读取的文件中的数据如下： hello world hello coder good job cool 66666 我们使用StepBuilderFactory构造Step，chunkSize设置为2。然后在job1中使用并执行： stepBuilderFactory.get(&quot;test-step&quot;).chunk(2).reader(reader).writer(writer).build(); 执行job1后console打印如下： handle start =====[hello world, hello coder] handle end.. -----[hello world, hello coder] handle start =====[good job, cool] job1遇到了good job这条数据，writer抛出了异常，由于没有使用skip或者retry机制，导致整个流程停止。job1的处理流程底层在SimpleChunkProcessor这个类中完成，包括processor、writer的使用。 接下里我们构造一个job2，job2使用skip机制(其中skipLimit必须要和skip(Class&lt;? extends Throwable&gt; type)一起使用)，skip机制可以防止writer发生异常后不停止整个job，但是需要同时满足skip的限制次数和skip对应的Exception是发生异常的父类或自身关系条件才不会停止整个job，这里我们使用Exception作为异常和Integer.MAX_VALUE作为skip的限制次数为例： stepBuilderFactory.get.get(&quot;test-step&quot;).chunk(2).reader(reader).writer(writer).faultTolerant().skipLimit(Integer.MAX_VALUE).skip(Exception.class).build(); 执行job2 后console打印如下： handle start =====[hello world, hello coder] handle end.. -----[hello world, hello coder] handle start =====[good job, cool] handle start =====[good job] handle start =====[cool] handle end.. -----[cool] handle start =====[66666] handle end.. -----[66666] 我们看到good job这条数据发生的异常被skip掉了，job完整的执行。 但是发现了另外一个问题，那就是处理 [good job, cool] 这批数据的时候，发生了异常，但是接下来执行了 [good job] 和 [cool] 这两批chunk为1的批次。这是在ItemWriter中执行的，它也会在ItemWriteListener中执行多次。 换句话说，如果使用了skip功能，那么对于需要被skip的批次数据中会进行scan操作找出具体是哪1条数据的原因，这里的scan操作指的是一条一条数据的遍历。 这个过程为什么叫scan呢? 在源码中，FaultTolerantChunkProcessor类(处理带有skip或者retry机制的处理器，跟SimpleChunkProcessor类似，只不过SimpleChunkProcessor处理简单的Job)里有个私有方法scan： private void scan(final StepContribution contribution, final Chunk&lt;I&gt; inputs, final Chunk&lt;O&gt; outputs, ChunkMonitor chunkMonitor, boolean recovery) throws Exception { ... Chunk&lt;I&gt;.ChunkIterator inputIterator = inputs.iterator(); Chunk&lt;O&gt;.ChunkIterator outputIterator = outputs.iterator(); List&lt;O&gt; items = Collections.singletonList(outputIterator.next()); // 拿出需要写的数据中的每一条数据 inputIterator.next(); try { writeItems(items); // 写每条数据 doAfterWrite(items); contribution.incrementWriteCount(1); inputIterator.remove(); outputIterator.remove(); } catch (Exception e) { // 写的时候如果发生了异常 doOnWriteError(e, items); if (!shouldSkip(itemWriteSkipPolicy, e, -1) &amp;&amp; !rollbackClassifier.classify(e)) { inputIterator.remove(); outputIterator.remove(); } else { // 具体的skip策略 checkSkipPolicy(inputIterator, outputIterator, e, contribution, recovery); } if (rollbackClassifier.classify(e)) { throw e; } } chunkMonitor.incrementOffset(); if (outputs.isEmpty()) { // 批次里的所有数据处理完毕之后 scanning 设置为false data.scanning(false); inputs.setBusy(false); chunkMonitor.resetOffset(); } } 这个scan方法触发的条件是UserData这个内部类里的scanning被设置为true，这里被设置为true是在处理批次数据出现异常后并且不能retry的情况下才会被设置的。 try { batchRetryTemplate.execute(retryCallback, recoveryCallback, new DefaultRetryState(inputs, rollbackClassifier)); } catch (Exception e) { RetryContext context = contextHolder.get(); if (!batchRetryTemplate.canRetry(context)) { // 设置scanning为true data.scanning(true); } throw e; } 这就是为什么skip机制在skip数据的时候会去scan批次中的每条数据，然后并找出需要被skip的数据的原理。 job3带有retry功能，retry的功能在于出现某个异常并且这个异常可以被retry所接受的话会进行retry，retry的次数可以进行配置，我们配置了3次retry： stepBuilderFactory.get.get(&quot;test-step&quot;).chunk(2).reader(reader).writer(writer).faultTolerant().skipLimit(Integer.MAX_VALUE).skip(Exception.class).retryLimit(3).retry(Exception.class).build(); 执行 job3后console打印如下： handle start =====[hello world, hello coder] handle end.. -----[hello world, hello coder] handle start =====[good job, cool] handle start =====[good job, cool] handle start =====[good job, cool] handle start =====[good job] handle start =====[cool] handle end.. -----[cool] handle start =====[66666] handle end.. -----[66666] [good job, cool] 这批数据retry了3次，而且都失败了。失败之后进行了skip操作。 SpringBatch中的retry和skip都有对应的policy实现，默认的retry policy是SimpleRetryPolicy，可以设置retry次数和接收的exception。比如可以使用NeverRetryPolicy： .retryPolicy(new NeverRetryPolicy()) 使用NeverRetryPolicy之后，便不再retry了，只会skip。SpringBatch内部的retry是使用Spring的retry模块完成的。执行的时候可以设置RetryCallback和RecoveryCallback。 SpringBatch中默认的skip policy是LimitCheckingItemSkipPolicy。 参考资料: http://stackoverflow.com/questions/16567432/how-is-the-skipping-implemented-in-spring-batch http://docs.spring.io/spring-batch/reference/html/retry.html https://github.com/spring-projects/spring-retry","raw":null,"content":null,"categories":[{"name":"spring","slug":"spring","permalink":"http://fangjian0423.github.io/categories/spring/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://fangjian0423.github.io/tags/spring/"},{"name":"springboot","slug":"springboot","permalink":"http://fangjian0423.github.io/tags/springboot/"},{"name":"springbatch","slug":"springbatch","permalink":"http://fangjian0423.github.io/tags/springbatch/"}]},{"title":"java Annotation的RetentionPolicy介绍","slug":"java-annotation-retentionpolicy","date":"2016-11-03T16:01:06.000Z","updated":"2016-11-03T16:03:49.000Z","comments":true,"path":"2016/11/04/java-annotation-retentionpolicy/","link":"","permalink":"http://fangjian0423.github.io/2016/11/04/java-annotation-retentionpolicy/","excerpt":"Java Annotation对应的Retention有3种，在RetentionPolicy中定义，有3种：\n\nSOURCE. 注解保留在源代码中，但是编译的时候会被编译器所丢弃。比如@Override, @SuppressWarnings\nCLASS. 这是默认的policy。注解会被保留在class文件中，但是在运行时期间就不会识别这个注解。\nRUNTIME. 注解会被保留在class文件中，同时运行时期间也会被识别。所以可以使用反射机制获取注解信息。比如@Deprecated\n","text":"Java Annotation对应的Retention有3种，在RetentionPolicy中定义，有3种： SOURCE. 注解保留在源代码中，但是编译的时候会被编译器所丢弃。比如@Override, @SuppressWarnings CLASS. 这是默认的policy。注解会被保留在class文件中，但是在运行时期间就不会识别这个注解。 RUNTIME. 注解会被保留在class文件中，同时运行时期间也会被识别。所以可以使用反射机制获取注解信息。比如@Deprecated RUNTIME大部分情况下，我们都是使用RUNTIME这个Policy。 下面就是一个RUNTIME Annotation的例子。 先定义Annotation： @Retention(RetentionPolicy.RUNTIME) @Target(ElementType.TYPE) public @interface MyClassRuntimeAnno { String name(); int level() default 1; } 然后在CLASS前面使用这个Annotation。 @MyClassRuntimeAnno(name = &quot;simple&quot;, level = 10) public class SimpleObj { } 最后写一个testcase通过反射可以获取这个类的Annotation进行后续操作。 @Test public void testGetAnnotation() { Annotation[] annotations = SimpleObj.class.getAnnotations(); System.out.println(Arrays.toString(annotations)); MyClassRuntimeAnno myClassAnno = SimpleObj.class.getAnnotation(MyClassRuntimeAnno.class); System.out.println(myClassAnno.name() + &quot;, &quot; + myClassAnno.level()); System.out.println(myClassAnno == annotations[0]); } SOURCESOURCE这个policy表示注解保留在源代码中，但是编译的时候会被编译器所丢弃。 由于在编译的过程中这个注解还被保留着，所以在编译过程中可以针对这个policy进行一些操作。比如在自动生成java代码的场景下使用。最常见的就是lombok的使用了，可以自动生成field的get和set方法以及toString方法，构造器等；消除了冗长的java代码。 SOURCE这个policy可以使用jdk中的javax.annotation.processing.*包中的processor处理器进行注解的处理过程。 以1个编译过程中会打印类中的方法的例子来说明SOUCRE这个policy的作用： 首先定义一个Printer注解： @Retention(RetentionPolicy.SOURCE) @Target(ElementType.METHOD) public @interface Printer { } 然后一个类的方法使用这个注解： public class SimpleObject { @Printer public void methodA() { } public void methodB() { } } 创建对应的Processor： @SupportedAnnotationTypes({&quot;me.format.annotaion.Printer&quot;}) public class PrintProcessor extends AbstractProcessor { @Override public boolean process(Set&lt;? extends TypeElement&gt; annotations, RoundEnvironment roundEnv) { Messager messager = processingEnv.getMessager(); messager.printMessage(Diagnostic.Kind.NOTE, &quot;start to use PrintProcessor ..&quot;); Set&lt;? extends Element&gt; rootElements = roundEnv.getRootElements(); messager.printMessage(Diagnostic.Kind.NOTE, &quot;root classes: &quot;); for(Element root : rootElements) { messager.printMessage(Diagnostic.Kind.NOTE, &quot;&gt;&gt; &quot; + root.toString()); } messager.printMessage(Diagnostic.Kind.NOTE, &quot;annotation: &quot;); for(TypeElement te : annotations) { messager.printMessage(Diagnostic.Kind.NOTE, &quot;&gt;&gt;&gt; &quot; + te.toString()); Set&lt;? extends Element&gt; elements = roundEnv.getElementsAnnotatedWith(te); for(Element ele : elements) { messager.printMessage(Diagnostic.Kind.NOTE, &quot;&gt;&gt;&gt;&gt; &quot; + ele.toString()); } } return true; } @Override public SourceVersion getSupportedSourceVersion() { return SourceVersion.latestSupported(); } } 然后先使用javac编译Printer和PrintProcessor： javac -d classes src/main/java/me/format/annotation/Printer.java src/main/java/me/format/annotation/PrintProcessor.java 最后再使用javac中的processor参数处理： javac -cp classes -processor me.format.annotation.PrintProcessor -d classes src/main/java/me/format/annotation/SimpleObject.java 控制台打印出： 注: start to use PrintProcessor .. 注: root classes: 注: &gt;&gt; hello.annotation.SimpleObject 注: annotation: 注: &gt;&gt;&gt; hello.annotation.Printer 注: &gt;&gt;&gt;&gt; methodA() CLASSCLASS和RUNTIME的唯一区别是RUNTIME在运行时期间注解是存在的，而CLASS则不存在。 我们通过asm来获取class文件里的annotation。 首先定义注解： policy为CLASS的注解。 @Retention(RetentionPolicy.CLASS) @Target(ElementType.TYPE) public @interface Meta { String name(); } policy为RUNTIME的注解。 @Retention(RetentionPolicy.RUNTIME) @Target(ElementType.TYPE) public @interface Header { int code(); } 使用注解： @Meta(name = &quot;obj&quot;) @Header(code = 200) public class AnnotationObject { private String val; public String getVal() { return val; } public void setVal(String val) { this.val = val; } } 编译这3个java文件得到字节码文件AnnotationObject.class： javac -d classes src/main/java/me/format/annotaion/AnnotationObject.java src/main/java/me/format/annotation/Meta.java src/main/java/me/format/annotation/Header.java 使用asm获取字节码文件中的注解： ClassNode classNode = new ClassNode(); ClassReader cr = new ClassReader(new FileInputStream(&quot;classes/me/format/annotation/AnnotationObject.class&quot;)); cr.accept(classNode, 0); System.out.println(&quot;Class Name: &quot; + classNode.name); System.out.println(&quot;Source File: &quot; + classNode.sourceFile); System.out.println(&quot;invisible: &quot;); AnnotationNode anNode = null; for (Object annotation : classNode.invisibleAnnotations) { anNode = (AnnotationNode) annotation; System.out.println(&quot;Annotation Descriptor : &quot; + anNode.desc); System.out.println(&quot;Annotation attribute pairs : &quot; + anNode.values); } System.out.println(&quot;visible: &quot;); for (Object annotation : classNode.visibleAnnotations) { anNode = (AnnotationNode) annotation; System.out.println(&quot;Annotation Descriptor : &quot; + anNode.desc); System.out.println(&quot;Annotation attribute pairs : &quot; + anNode.values); } 打印出： Class Name: me/format/annotation/AnnotationObject Source File: AnnotationObject.java invisible: Annotation Descriptor : Lme/format/annotation/Meta; Annotation attribute pairs : [name, obj] visible: Annotation Descriptor : Lme/format/annotation/Header; Annotation attribute pairs : [code, 200] 其中policy为CLASS的注解编译完后不可见，而policy为RUNTIME的注解编译后可见。 同样，我们可以使用javap查看编译后的信息： javap -v me.format.annotation.AnnotationObject 会打印出注解的visible信息： #16 = Utf8 AnnotationObject.java #17 = Utf8 RuntimeVisibleAnnotations #18 = Utf8 Lhello/annotation/Header; #19 = Utf8 code #20 = Integer 200 #21 = Utf8 RuntimeInvisibleAnnotations #22 = Utf8 Lhello/annotation/Meta; #23 = Utf8 name #24 = Utf8 obj","raw":null,"content":null,"categories":[{"name":"java","slug":"java","permalink":"http://fangjian0423.github.io/categories/java/"}],"tags":[{"name":"java","slug":"java","permalink":"http://fangjian0423.github.io/tags/java/"}]},{"title":"Swagger在SpringBoot中的使用","slug":"springboot-swagger","date":"2016-10-09T14:11:23.000Z","updated":"2016-10-09T13:50:09.000Z","comments":true,"path":"2016/10/09/springboot-swagger/","link":"","permalink":"http://fangjian0423.github.io/2016/10/09/springboot-swagger/","excerpt":"Swagger是一个规范和完整的框架，用于生成、描述、调用和可视化 RESTful 风格的 Web 服务。\n在SpringBoot中要使用Swagger的话，可以使用springfox。\n在sbt中添加依赖即可：\nlibraryDependencies += &quot;io.springfox&quot; % &quot;springfox-swagger2&quot; % &quot;2.6.0&quot;\nlibraryDependencies += &quot;io.springfox&quot; % &quot;springfox-swagger-ui&quot; % &quot;2.6.0&quot;\n查看springfox-swagger2中的源码，发现springfox并不是通过autoconfigure实现和swagger的整合的，而是基于Spring的方式Import各种bean构造Swagger。\n所以本文指的Swagger在SpringBoot中的使用同样也可以在Spring中使用。","text":"Swagger是一个规范和完整的框架，用于生成、描述、调用和可视化 RESTful 风格的 Web 服务。 在SpringBoot中要使用Swagger的话，可以使用springfox。 在sbt中添加依赖即可： libraryDependencies += &quot;io.springfox&quot; % &quot;springfox-swagger2&quot; % &quot;2.6.0&quot; libraryDependencies += &quot;io.springfox&quot; % &quot;springfox-swagger-ui&quot; % &quot;2.6.0&quot; 查看springfox-swagger2中的源码，发现springfox并不是通过autoconfigure实现和swagger的整合的，而是基于Spring的方式Import各种bean构造Swagger。 所以本文指的Swagger在SpringBoot中的使用同样也可以在Spring中使用。 要整合Spring和Swagger的话需要这么做。 加上@EnableSwagger2注解 构造一个Docket 以下代码就是例子： @EnableSwagger2 @Configuration public class SwaggerConfiguration { @Bean public Docket userDocket() { ApiInfo apiInfo = new ApiInfo(&quot;A Simple of SpringBoot-Swagger&quot;,// 大标题 &quot;two controllers: UserController and DeptController&quot;,// 小标题 &quot;1.0&quot;,// 版本 &quot;NO terms of service&quot;, new Contact(&quot;format&quot;, &quot;fangjian0423.github.io&quot;, &quot;fangjian0423@gmail.com&quot;), // 作者信息 &quot;The Apache License, Version 2.0&quot;,// 开源许可证 &quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;// 许可证详情 ); return new Docket(DocumentationType.SWAGGER_2) .select() .paths(Predicates.or(PathSelectors.regex(&quot;/user/.*&quot;), PathSelectors.regex(&quot;/dept/.*&quot;))) .build() .apiInfo(apiInfo); } } 需要注意的是path方法表示要构造的url，这里使用了or连接了 /user/.* 和 /dept/.* 这2个地址，这里用的是正则的匹配方式。 UserController： @RestController @RequestMapping(Array(&quot;/user&quot;)) class UserController { @ApiOperation(nickname = &quot;test method&quot;, value = &quot;just a test method&quot;) @RequestMapping(value = Array(&quot;/demo&quot;), method = Array(RequestMethod.POST, RequestMethod.GET)) def demo(): String = { &quot;Hello Swagger&quot; } @RequestMapping(value = Array(&quot;/get/{id}&quot;), method = Array(RequestMethod.GET)) def get(@PathVariable id: String): String = { s&quot;get ${id}&quot; } @RequestMapping(value = Array(&quot;/delete/{id}&quot;), method = Array(RequestMethod.POST)) def delete( @ApiParam(name = &quot;id&quot;, value = &quot;the identity of user&quot;, required = true) @PathVariable id: String ): String = { s&quot;delete ${id}&quot; } @ApiImplicitParams( Array( new ApiImplicitParam(name = &quot;name&quot;, value = &quot;the name of user&quot;, required = true, paramType = &quot;form&quot;, dataType = &quot;string&quot;), new ApiImplicitParam(name = &quot;age&quot;, value = &quot;the age of user&quot;, required = true, paramType = &quot;form&quot;, dataType = &quot;int&quot;) ) ) @RequestMapping(value = Array(&quot;/add&quot;), method = Array(RequestMethod.POST)) def add(req: HttpServletRequest): String = { s&quot;${req.getParameter(&quot;name&quot;)}-${req.getParameter(&quot;age&quot;)}&quot; } } Swagger默认会去找被@RequestMapping注解的方法。 @ApiOperation注解用于说明接口的作用，作用在方法上，如果没有使用这个注解，会去@RequestMapping中的value和method等属性。 @ApiParam注解用来额外说明参数的meta data。 @ApiImplicitParams注解也用来额外说明参数，当参数不在方法里声明的时候，可以使用这个注解。需要注意的是这个注解只能作用在方法上。 Swagger还提供了@ApiModel、@ApiResponse、@Example等诸多注解用于说明接口的作用。 另外一个Controller: @RestController @RequestMapping(Array(&quot;/dept&quot;)) class DeptController { @ApiOperation(nickname = &quot;dept test method&quot;, value = &quot;dept just a test method&quot;) @RequestMapping(value = Array(&quot;/demo&quot;), method = Array(RequestMethod.POST, RequestMethod.GET)) def demo(): String = { &quot;Hello Swagger&quot; } } Swagger展示如下：","raw":null,"content":null,"categories":[{"name":"springboot","slug":"springboot","permalink":"http://fangjian0423.github.io/categories/springboot/"}],"tags":[{"name":"springboot","slug":"springboot","permalink":"http://fangjian0423.github.io/tags/springboot/"},{"name":"swagger","slug":"swagger","permalink":"http://fangjian0423.github.io/tags/swagger/"}]},{"title":"SpringBoot的事务管理","slug":"springboot-transaction","date":"2016-10-06T17:28:36.000Z","updated":"2016-10-06T17:33:10.000Z","comments":true,"path":"2016/10/07/springboot-transaction/","link":"","permalink":"http://fangjian0423.github.io/2016/10/07/springboot-transaction/","excerpt":"Springboot内部提供的事务管理器是根据autoconfigure来进行决定的。\n比如当使用jpa的时候，也就是pom中加入了spring-boot-starter-data-jpa这个starter之后(之前我们分析过springboot的自动化配置原理)。\nSpringboot会构造一个JpaTransactionManager这个事务管理器。\n而当我们使用spring-boot-starter-jdbc的时候，构造的事务管理器则是DataSourceTransactionManager。\n这2个事务管理器都实现了spring中提供的PlatformTransactionManager接口，这个接口是spring的事务核心接口。\n这个核心接口有以下这几个常用的实现策略：\n\nHibernateTransactionManager\nDataSourceTransactionManager\nJtaTransactionManager\nJpaTransactionManager\n\n具体的PlatformTransactionManager继承关系如下：\n","text":"Springboot内部提供的事务管理器是根据autoconfigure来进行决定的。 比如当使用jpa的时候，也就是pom中加入了spring-boot-starter-data-jpa这个starter之后(之前我们分析过springboot的自动化配置原理)。 Springboot会构造一个JpaTransactionManager这个事务管理器。 而当我们使用spring-boot-starter-jdbc的时候，构造的事务管理器则是DataSourceTransactionManager。 这2个事务管理器都实现了spring中提供的PlatformTransactionManager接口，这个接口是spring的事务核心接口。 这个核心接口有以下这几个常用的实现策略： HibernateTransactionManager DataSourceTransactionManager JtaTransactionManager JpaTransactionManager 具体的PlatformTransactionManager继承关系如下： spring-boot-starter-data-jpa这个starter会触发HibernateJpaAutoConfiguration这个自动化配置类，HibernateJpaAutoConfiguration继承了JpaBaseConfiguration基础类。 在JpaBaseConfiguration中构造了事务管理器： @Bean @ConditionalOnMissingBean(PlatformTransactionManager.class) public PlatformTransactionManager transactionManager() { return new JpaTransactionManager(); } spring-boot-starter-jdbc会触发DataSourceTransactionManagerAutoConfiguration这个自动化配置类，也会构造事务管理器： @Bean @ConditionalOnMissingBean(PlatformTransactionManager.class) @ConditionalOnBean(DataSource.class) public DataSourceTransactionManager transactionManager() { return new DataSourceTransactionManager(this.dataSource); } Spring的事务管理器PlatformTransactionManager接口中定义了3个方法： // 基于事务的传播特性，返回一个已经存在的事务或者创建一个新的事务 TransactionStatus getTransaction(TransactionDefinition definition) throws TransactionException; // 提交事务 void commit(TransactionStatus status) throws TransactionException; // 回滚事务 void rollback(TransactionStatus status) throws TransactionException; 其中TransactionDefinition接口表示跟spring兼容的事务属性，比如传播行为、隔离级别、超时时间、是否只读等属性。 DefaultTransactionDefinition类是一个默认的TransactionDefinition实现，它的传播行为是PROPAGATION_REQUIRED(如果当前没事务，则创建一个，否则加入到当前事务中)，隔离级别是数据库默认级别。 TransactionStatus接口表示事务的状态，比如事务是否是一个刚构造的事务、事务是否已经完成等状态。 下面这段代码就是传统事务的常见写法： transaction.begin(); try { ... transaction.commit(); } catch(Exception e) { ... transaction.rollback(); } finally { } 由于spring的事务操作被封装到了PlatformTransactionManager接口中，commit和rollback方法对应接口中的方法，begin方法在getTransaction方法中会被调用。 细心的读者发现文章前面构造事务管理器的时候都会加上这段注解： @ConditionalOnMissingBean(PlatformTransactionManager.class) 也就是说如果我们手动配置了事务管理器，Springboot就不会再为我们自动配置事务管理器。 如果要使用多个事务管理器的话，那么需要手动配置多个： @Configuration public class DatabaseConfiguration { @Bean public PlatformTransactionManager transactionManager1(EntityManagerFactory entityManagerFactory) { return new JpaTransactionManager(entityManagerFactory); } @Bean public PlatformTransactionManager transactionManager2(DataSource dataSource) { return new DataSourceTransactionManager(dataSource); } } 然后使用Transactional注解的时候需要声明是哪个事务管理器： @Transactional(value=&quot;transactionManager1&quot;) public void save() { doSave(); } Spring给我们提供了一个TransactionManagementConfigurer接口，该接口只有一个方法返回PlatformTransactionManager。其中返回的PlatformTransactionManager就表示这是默认的事务处理器，这样在Transactional注解上就不需要声明是使用哪个事务管理器了。 参考资料： http://www.cnblogs.com/davidwang456/p/4309038.html http://blog.csdn.net/chjttony/article/details/6528344","raw":null,"content":null,"categories":[{"name":"springboot","slug":"springboot","permalink":"http://fangjian0423.github.io/categories/springboot/"}],"tags":[{"name":"springboot","slug":"springboot","permalink":"http://fangjian0423.github.io/tags/springboot/"}]},{"title":"Netty的单元测试","slug":"netty-unittest","date":"2016-09-03T13:04:52.000Z","updated":"2016-09-03T12:47:41.000Z","comments":true,"path":"2016/09/03/netty-unittest/","link":"","permalink":"http://fangjian0423.github.io/2016/09/03/netty-unittest/","excerpt":"我们在编写完Netty程序之后，会需要进行ChannelHandler的一些测试。\n在初学Netty的时候，我们都是直接起一个Server，然后再写一个Client去连接Server，看传输的数据和接收到的数据是否正确。\n在Netty in Action书中的Transport和Unit-test your code这两章就提出了可以使用EmbeddedChannel进行ChanndlHandler的单元测试。","text":"我们在编写完Netty程序之后，会需要进行ChannelHandler的一些测试。 在初学Netty的时候，我们都是直接起一个Server，然后再写一个Client去连接Server，看传输的数据和接收到的数据是否正确。 在Netty in Action书中的Transport和Unit-test your code这两章就提出了可以使用EmbeddedChannel进行ChanndlHandler的单元测试。 EmbeddedChannel介绍EmbeddedChannel中提供了一些方法： 方法名 说明 writeInbound 写数据到Channel中，但是这些数据只会被ChannelPipeline中的inbound handler处理。如果可以从EmbeddedChannel的readInbound方法中读取出数据的话，就返回true readInbound 读取在EmbeddedChannel上被所有inbound handler处理过的数据，如果已经没有数据可以读取就返回null writeInbound 写数据到Channel中，但是这些数据只会被ChannelPipeline中的outbound handler处理。如果可以从EmbeddedChannel的readOutbound方法中读取出数据的话，就返回true readOutbound 读取在EmbeddedChannel上被所有outbound handler处理过的数据，如果已经没有数据可以读取就返回null finish 标记EmbeddedChannel已经完成。如果可以从inbound或者outbound中返回数据，该方法就返回true。这个方法还会关闭Channel 下图就是EmbeddedChannel的处理流程图： 使用writeInbound方法写入的数据，并经过Pipeline中所有的inbound handler，之后可以使用readInbound方法读取经过inbound handler之后的数据。 使用writeOutbound方法写入的数据，并经过Pipeline中所有的outbound handler，之后可以使用readOutbound方法读取经过outbound handler之后的数据。 调用finish方法可以标记EmbeddedChannel已经完成。 Inbound Handler的测试Netty内部提供了一个FixedLengthFrameDecoder解码器用于把长度不固定的字节转换成固定长度的字节，处理流程图如下： 针对这个Decoder编写单元测试。 由于是个Decoder，针对的是inbound中的数据，所以需要使用的方法是writeInbound和readInbound： ByteBuf buf = Unpooled.buffer(); // 构造heap buffer for(int i = 0; i &lt; 9; i ++) { // 写入9个字节 buf.writeByte(i); } ByteBuf input = buf.copy(); // 构造EmbeddedChannel，并在Pipeline中加入FixedLengthFrameDecoder EmbeddedChannel channel = new EmbeddedChannel(new FixedLengthFrameDecoder(3)); // 使用writeInbound方法写入数据 Assert.assertTrue(channel.writeInbound(input)); // 标记EmbeddedChannel状态已经complete Assert.assertTrue(channel.finish()); // 读取经过FixedLengthFrameDecoder处理过后的字节 Assert.assertEquals(buf.readBytes(3), channel.readInbound()); Assert.assertEquals(buf.readBytes(3), channel.readInbound()); Assert.assertEquals(buf.readBytes(3), channel.readInbound()); Assert.assertNull(channel.readInbound()); 我们在使用Netty编写自定义的协议文章中编写的自定义协议CustomProtocol的解码器，并最后通过一个server和client的编写完成了测试。 现在我们可以使用EmbeddedChannel进行Decoder的unit test： EmbeddedChannel channel = new EmbeddedChannel(new CustomProtocolDecoder()); String uuid = UUID.randomUUID().toString(); channel.writeInbound(new CustomProtocol(1024l, uuid, &quot;content content&quot;)); Assert.assertTrue(channel.finish()); CustomProtocol customProtocol = (CustomProtocol) channel.readInbound(); // 判断是否正确 Assert.assertEquals(1024l, customProtocol.getVersion()); Assert.assertEquals(uuid, customProtocol.getHeader()); Assert.assertEquals(&quot;content content&quot;, customProtocol.getContent()); Assert.assertNull(channel.readInbound()); Outbound Handler的测试AbsIntegerEncoder对所有的int数据取绝对值。处理流过图如下： public class AbsIntegerEncoder extends MessageToMessageEncoder&lt;ByteBuf&gt; { @Override protected void encode(ChannelHandlerContext ctx, ByteBuf msg, List&lt;Object&gt; out) throws Exception { while(msg.readableBytes() &gt;= 4) { int value = Math.abs(msg.readInt()); out.add(value); } } } 针对这个Encoder编写单元测试。 由于是个Encoder，针对的是outbound中的数据，所以需要使用的方法是writeOutbound和readOutbound： ByteBuf buf = Unpooled.buffer(); // 构造heap buffer for(int i = 1; i &lt; 10; i ++) { // 写入10个负数 buf.writeInt(i * -1); } // 构造EmbeddedChannel，并在Pipeline中加入AbsIntegerEncoder EmbeddedChannel channel = new EmbeddedChannel(new AbsIntegerEncoder()); // 使用writeOutbound方法写入数据 Assert.assertTrue(channel.writeOutbound(buf)); // 标记EmbeddedChannel状态已经complete Assert.assertTrue(channel.finish()); // 测试是否所有的int数据都取了绝对值 for(int i = 1; i &lt; 10; i ++) { Assert.assertEquals(i, (int)channel.readOutbound()); } Assert.assertNull(channel.readOutbound()); 同理我们可以使用EmbeddedChannel进行CustomProtocol的Encoder的unit test： EmbeddedChannel channel = new EmbeddedChannel(new CustomProtocolEncoder()); String uuid = UUID.randomUUID().toString(); channel.writeOutbound(new CustomProtocol(1024l, uuid, &quot;content content&quot;)); Assert.assertTrue(channel.finish()); ByteBuf buf = (ByteBuf) channel.readOutbound(); Assert.assertEquals(1024l, buf.readLong()); byte[] headerBytes = new byte[36]; buf.readBytes(headerBytes); Assert.assertEquals(uuid, new String(headerBytes)); byte[] contentBytes = new byte[buf.readableBytes()]; buf.readBytes(contentBytes); Assert.assertEquals(&quot;content content&quot;, new String(contentBytes)); Assert.assertNull(channel.readOutbound());","raw":null,"content":null,"categories":[{"name":"netty","slug":"netty","permalink":"http://fangjian0423.github.io/categories/netty/"}],"tags":[{"name":"nio","slug":"nio","permalink":"http://fangjian0423.github.io/tags/nio/"},{"name":"netty","slug":"netty","permalink":"http://fangjian0423.github.io/tags/netty/"}]},{"title":"使用Netty编写自定义的协议","slug":"netty-custom-protocol","date":"2016-08-30T14:04:52.000Z","updated":"2016-08-30T13:28:26.000Z","comments":true,"path":"2016/08/30/netty-custom-protocol/","link":"","permalink":"http://fangjian0423.github.io/2016/08/30/netty-custom-protocol/","excerpt":"Netty内部提供的Codec用于处理消息的编解码问题，比如Http，Ftp，SMTP这些协议，都需要进行编解码。\nNetty内部直接提供了Http协议的Codec组件用于编解码Http协议。\n在开发过程中，有时候我们需要构建一些适应自己业务的应用层协议，Netty可以很方便地实现自定义协议。\n比如我们的自定义协议CucstomProtocol结构如下：\n| version | header | content |\n其中version表示版本号，long类型；header是一个uuid，string类型，content则是具体的内容，string类型。","text":"Netty内部提供的Codec用于处理消息的编解码问题，比如Http，Ftp，SMTP这些协议，都需要进行编解码。 Netty内部直接提供了Http协议的Codec组件用于编解码Http协议。 在开发过程中，有时候我们需要构建一些适应自己业务的应用层协议，Netty可以很方便地实现自定义协议。 比如我们的自定义协议CucstomProtocol结构如下： | version | header | content | 其中version表示版本号，long类型；header是一个uuid，string类型，content则是具体的内容，string类型。 对应的POJO如下： public class CustomProtocol { private long version; // 版本 private String header; // 头信息(UUID) private String content; // 具体内容 // GET SET ... @Override public String toString() { return &quot;CustomProtocol{&quot; + &quot;version=&quot; + version + &quot;, header=&apos;&quot; + header + &apos;\\&apos;&apos; + &quot;, content=&apos;&quot; + content + &apos;\\&apos;&apos; + &apos;}&apos;; } } 解码器把byte转换成CustomProtocol，在server中使用： public class CustomProtocolDecoder extends ByteToMessageDecoder { @Override protected void decode(ChannelHandlerContext ctx, ByteBuf in, List&lt;Object&gt; out) throws Exception { long version = in.readLong(); // 读取version byte[] headerBytes = new byte[36]; in.readBytes(headerBytes); // 读取header String header = new String(headerBytes); byte[] contentBytes = new byte[in.readableBytes()]; // 读取content in.readBytes(contentBytes); out.add(new CustomProtocol(version, header, new String(contentBytes))); } } 编码器把CustomProtocol转换成byte，在client中使用： public class CustomProtocolEncoder extends MessageToByteEncoder&lt;CustomProtocol&gt; { @Override protected void encode(ChannelHandlerContext ctx, CustomProtocol msg, ByteBuf out) throws Exception { out.writeLong(msg.getVersion()); out.writeBytes(msg.getHeader().getBytes()); out.writeBytes(msg.getContent().getBytes()); } } server代码： ServerBootstrap serverBootstrap = new ServerBootstrap(); EventLoopGroup eventLoopGroup = new NioEventLoopGroup(); EventLoopGroup childEventLoopGroup = new NioEventLoopGroup(); try { serverBootstrap .group(eventLoopGroup, childEventLoopGroup) .channel(NioServerSocketChannel.class) .childHandler(new ChannelInitializer&lt;Channel&gt;() { @Override protected void initChannel(Channel ch) throws Exception { ch.pipeline().addLast(new CustomProtocolDecoder()); // 解码器 ch.pipeline().addLast(new ServerHandler()); // 打印数据 } }); ChannelFuture future = serverBootstrap.bind(&quot;localhost&quot;, 9999).sync(); future.channel().closeFuture().sync(); } catch (Exception e) { e.printStackTrace(); } finally { try { eventLoopGroup.shutdownGracefully().sync(); childEventLoopGroup.shutdownGracefully().sync(); } catch (Exception e1) { e1.printStackTrace(); } } ServerHandler用于打印接收到的数据，并写数据回去给客户端表示接收到了数据： public class ServerHandler extends SimpleChannelInboundHandler&lt;CustomProtocol&gt; { @Override protected void channelRead0(ChannelHandlerContext ctx, CustomProtocol msg) throws Exception { System.out.println(&quot;server receive: &quot; + msg); ctx.writeAndFlush(Unpooled.copiedBuffer(&quot;server get&quot;, CharsetUtil.UTF_8)).addListener(ChannelFutureListener.CLOSE); } } client代码： Bootstrap bootstrap = new Bootstrap(); EventLoopGroup eventLoopGroup = new NioEventLoopGroup(); try { bootstrap .group(eventLoopGroup) .channel(NioSocketChannel.class) .handler(new ChannelInitializer&lt;Channel&gt;() { @Override protected void initChannel(Channel ch) throws Exception { ch.pipeline().addLast(new CustomProtocolEncoder()); // 编码器 ch.pipeline().addLast(new ClientHandler()); // 接收服务端数据 } }); ChannelFuture channelFuture = bootstrap.connect(&quot;localhost&quot;, 9999).sync(); channelFuture.channel().writeAndFlush(new CustomProtocol(1024l, UUID.randomUUID().toString(), &quot;content detail&quot;)); channelFuture.channel().closeFuture().sync(); } catch (Exception e) { e.printStackTrace(); } finally { try { eventLoopGroup.shutdownGracefully().sync(); } catch (InterruptedException e) { e.printStackTrace(); } } ClientHandler用于接收服务端返回回来的数据： public class ClientHandler extends SimpleChannelInboundHandler&lt;ByteBuf&gt; { @Override protected void channelRead0(ChannelHandlerContext ctx, ByteBuf msg) throws Exception { System.out.println(&quot;send success, response is: &quot; + msg.toString(CharsetUtil.UTF_8)); } } 启动服务端和客户端之后，服务端收到客户端发来的CustomProtocol： server receive: CustomProtocol{version=1024, header=&apos;6ed7fa3d-7d54-4add-9081-d659d4b37d3f&apos;, content=&apos;content detail&apos;} 之后客户端也收到服务端成功接收数据的反馈： send success, response is: server get","raw":null,"content":null,"categories":[{"name":"netty","slug":"netty","permalink":"http://fangjian0423.github.io/categories/netty/"}],"tags":[{"name":"nio","slug":"nio","permalink":"http://fangjian0423.github.io/tags/nio/"},{"name":"netty","slug":"netty","permalink":"http://fangjian0423.github.io/tags/netty/"}]},{"title":"Netty in Action笔记(二)","slug":"netty-in-action-note2","date":"2016-08-29T12:36:52.000Z","updated":"2016-08-29T12:07:20.000Z","comments":true,"path":"2016/08/29/netty-in-action-note2/","link":"","permalink":"http://fangjian0423.github.io/2016/08/29/netty-in-action-note2/","excerpt":"主要记录ChannelHandler、Codec以及Bootstrap的作用，还有Netty内置的一些ChannelHandler、Codec，以及Bootstrap的分类。","text":"主要记录ChannelHandler、Codec以及Bootstrap的作用，还有Netty内置的一些ChannelHandler、Codec，以及Bootstrap的分类。 第六章介绍Netty中的Channel、ChannelHandler、ChannelHandlerContext以及ChannelPipeline。 Channel定义：一个Channel表示一个通道，跟io中的stream的角色一样，但是有几点不同。 1. stream是单向的，只支持读或者写，channel是双向的，既支持读也支持写。2. stream是阻塞的，channel是并行的。 其中Channel的生命周期状态如下： 状态说明 说明 channelUnregistered channel创建之后，还未注册到EventLoop channelRegistered channel注册到了对应的EventLoop channelActive channel处于活跃状态，活跃状态表示已经连接到了远程服务器，现在可以接收和发送数据 channelInactive channel未连接到远程服务器 一个Channel正常的生命周期如下： channelRegistered -&gt; channelActice -&gt; channelInactive -&gt; channelUnregistered 在另外一种特殊情况下，会发生多次channelRegistered和channelUnregistered，这是因为用户可以从EventLoop上取消注册Channel来阻止事件的执行并在之后重新注册。状态变化如下： ChannelHandlerChannelHandler有2种类型： Inbound Handler: 处理inbound数据(接收到的数据)以及所有类型的channel状态修改事件 Outbound Handler: 处理outbound数据(发送出去的数据)并且可以拦截各种操作，比如bind、connect、disconnect、close、write等操作 Inbound和Outbound Handler都属于ChannelHandler，它们都可以被添加到ChannelPipeline中，它们内部也提供了handlerAdded、handlerRemoved这两种方法分别在ChannelHandler添加到ChannelPipeline和ChannelHandler从ChannelPipeline中被删除的时候触发。 ChannelInboundHandlerChannelInboundHandler方法在两种情况下触发：channel状态的改变和channel接收到数据。 一些方法说明： 方法名 描述 channelRegistered(..) Channel注册到EventLoop，并且可以处理IO请求 channelUnregistered(…) Channel从EventLoop中被取消注册，并且不能处理任何IO请求 channelActive(…) Channel已经连接到远程服务器，并准备好了接收数据 channelInactive(…) Channel还没有连接到远程服务器 channelReadComplete(…) Channel的读取操作已经完成 channelRead(…) 有数据可以读取的时候触发 userEventTriggered(…) 当用户调用Channel.fireUserEventTriggered方法的时候触发，用户可以传递一个自定义的对象当这个方法里 ChannelInboundHandler有一个实现ChannelInboundHandlerAdapter，它实现了所有的方法，我们只需要继承这个类然后复写需要的方法即可。 ChannelInboundHandler中的channelRead方法中有读取的ByteBuf。由于Netty在ByteBuf的使用上使用了池的概念，当不需要这个ByteBuf的时候需要进行资源的释放以减少内存的消耗。 @Override public void channelRead(ChannelHandlerContext ctx, Object msg) { // do something ReferenceCountUtil.release(msg); } Netty内部提供了一个SimpleChannelInboundHandler类，这个类读取数据会自动释放资源。它继承ChannelInboundHandlerAdapter并复写了channelRead方法，在channelRead方法里面finally代码里会自动release资源，并提供了channelRead0方法： @Override public void channelRead0(ChannelHandlerContext ctx, Object msg) { // do something, do not need release } 所以一般使用ChannelInboundHandler的话，有3种方法。 直接实现ChannelInBoundHandler接口 继承ChannelInboundHandlerAdapter 继承SimpleChannelInboundHandler 第1种基本不用，第3种用来处理接收消息，第2种用来处理事件状态的改变 ChannelOutboundHandlerChannelOutboundHandler方法在两种情况下触发：发送数据和拦截操作。 ChannelOutboundHandler有一个强大的功能，可以按需推迟一个操作，这使得处理请求可以用到更为复杂的策略。比如，如果写数据到远端被暂停，你可以推迟flush操作，稍后再试。 一些方法说明： 方法名 描述 bind(..) 请求绑定channel到本地地址 connect(…) channel连接到远程地址 disconnect(…) channel从远程服务器上断开 close(…) 关闭channel deregister(…) 取消channel在eventloop上的注册 read(…) 在channel中读数据 flush(…) flush数据到远程服务器 write(…) 写数据到远程服务器 ChannelOutboundHandler有一个实现ChannelOutboundHandlerAdapter，它实现了所有的方法，我们只需要继承这个类然后复写需要的方法即可。 在outboundhandler中有时候也需要释放资源，当消息被消费并且不再需要传递给下一个outbound handler的时候，调用ReferenceCountUtil.release(message)释放消息。 当消息被写回去或者channel关闭的时候，这个消息的资源会被自动释放，所以没有一个类似SimpleChannelInboundHandler的概念。 ChannelHandlerContext当ChannelHandler被添加到ChannelPipeline中的时候，ChannelHandlerContext会被创建。所以说ChannelHandlerContext属于ChannelHandler。 可以通过ChannelHandlerContext的channel方法得到Channel和pipeline方法得到ChannelPipeline。 ChannelHandlerContext可以被保留下来并且在其他地方进行调用，比如在其他线程，或者在handler外部进行调用。 可以使用以下方法保留ChannelHandlerContext： class WriterHandler extends ChannelHandlerAdapter { private ChannelHandlerContext ctx; @Override public void handlerAdded(ChannelHandlerContext ctx) throws Exception { this.ctx = ctx; } public void send(String msg) { ctx.write(msg); } } Netty中提供了一个@Sharable注解用来将一个实例的ChannelHandler添加到多个ChannelPipeline中，如果不加上这个注解，被多个ChannelPipeline使用的话会抛出异常。 ChannelPipeline多个ChannelHandler可以组成一个ChannelPipeline，里面的每个ChannelHandler可以转发给下一个ChannelHandler。 ChannelPipeline内部的所有ChannelHandler的处理流程图： ChannelPipeline提供了多种方法用于添加或删除或代替ChannelHandler，比如addLast, addFirst, remove, replace等方法。 第七章介绍Netty中的编码器、解码器。我们知道网络中传输的是字节-ByteBuf。我们需要对ByteBuf进行一些解码用于解码成熟悉的POJO，对ByteBuf进行编码用于网络传输。 Decoder解码器，针对的是inbound的数据，也就是读取数据的解码。 2种类型： bytes到message的解码(ByteToMessageDecoder和ReplayingDecoder) message到message的解码(MessageToMessageDecoder) decoders的作用是把inbound中读取的数据从一种格式转换成另一种格式，由于decoders处理的是inbound中的数据，所以它也是ChannelInboundHandler的一种实现类。 ByteToMessageDecoder和ReplayingDecoder属于bytes到message的解码。 一个ByteToMessageDecoder例子，将byte转换成integer： public class ToIntegerDecoder extends ByteToMessageDecoder { @Override protected void decode(ChannelHandlerContext ctx, ByteBuf in, List&lt;Object&gt; out) throws Exception { if(in.readableBytes() &gt;= 4) { out.add(in.readInt()); } } } 处理流程图如下： 如果使用ReplayingDecoder，不需要进行可读字节的判断，直接添加到List里即可。跟ToIntegerDecoder实现一样的功能，ReplayingDecoder只需要这样即可。(但是有一定的局限性：1.不是所有的操作都被ByteBuf支持 2.ByteBuf.readableBytes方法大部分时间不会返回期望的值) public class ToIntegerDecoder2 extends ReplayingDecoder&lt;Void&gt; { @Override protected void decode(ChannelHandlerContext ctx, ByteBuf in, List&lt;Object&gt; out) throws Exception { out.add(in.readInt()); } } 一个MessageToMessageDecoder例子，将integer转换成string： public class IntegerToStringDecoder extends MessageToMessageDecoder&lt;Integer&gt; { @Override protected void decode(ChannelHandlerContext ctx, Integer msg, List&lt;Object&gt; out) throws Exception { out.add(String.valueOf(msg)); } } 处理流程图如下： Encoder编码器，针对的是outbound的数据，也就是发送出去的数据的编码。 2种类型： message到message的编码(MessageToMessageEncoder) message到byte的编码(MessageToByteEncoder) decoders的作用是把outbound中发送出去的数据从一种格式转换成另一种格式，由于eecoders处理的是outbound中的数据，所以它也是ChannelOutboundHandler的一种实现类。 一个MessageToByteEncoder例子，将integer转换成byte： public class IntegerToByteEncoder extends MessageToByteEncoder&lt;Integer&gt; { @Override protected void encode(ChannelHandlerContext ctx, Integer msg, ByteBuf out) throws Exception { out.writeInt(msg); } } 处理流程图如下： 一个MessageToMessageEncoder例子，将integer转换成string： public class IntegerToStringEncoder extends MessageToMessageEncoder&lt;Integer&gt; { @Override protected void encode(ChannelHandlerContext ctx, Integer msg, List&lt;Object&gt; out) throws Exception { out.add(String.valueOf(msg)); } } 处理流程图如下： Codec编解码器，既支持编码器的功能，也支持解码器的功能。 2种类型： ByteToMessageCodec：message到byte的编解码 MessageToMessageCodec：message到message的编解码 CombinedChannelDuplexHandler结合解码器和编码器在一起可能会牺牲可重用性。为了避免这种方式，可以使用CombinedChannelDuplexHandler。 CombinedChannelDuplexHandler也就是codec的另外一种方式，如果已经有个encoder和decoder，不需要重新写了codec，直接使用CombinedChannelDuplexHandler整合这个encoder和decoder即可。 上面的ToIntegerDecoder和IntegerToByteEncoder就可以构成一个编解码器，直接使用CombinedChannelDuplexHandler即可。 public class CombinedByteIntegerCodec extends CombinedChannelDuplexHandler&lt;ToIntegerDecoder, IntegerToByteEncoder&gt; { public CombinedByteIntegerCodec() { super(new ToIntegerDecoder(), new IntegerToByteEncoder()); } } 第八章主要说明jetty内置的一些ChannelHandler和Codec。 使用SSL/TLS加密Netty程序的话，可以使用内置的SslHandler。 要构建Http应用的话，可以使用HttpClientCodec/HttpServerCodec(http客户端和服务端的编解码)以及HttpObjectAggregator(Http的消息聚合)。 可以使用HttpContentDecompressor和HttpContentCompressor对http的内容进行解压和压缩。 还有一些WebSocket、SPDY，一些空置链接、超时链接等处理的内置解决方案。 第九章主要讲解Bootstrap中Netty中的作用。 之前分析过各种ChannelHandler，各种Codec，以及把这两个东西添加到Channel 的ChannelPipeline中。有了这些东西之后，该用什么把他们整合起来呢，那就是Bootstrap。 Bootstrap分别ServerBootstrap(服务端)和Bootstrap(客户端)，它们都继承AbstractBootstrap。 Bootstrap客户端服务启动类，内部提供的一些方法如下： 方法名 描述 group(..) 设置Bootstrap使用的EventLoopGroup，用来处理Channel的IO操作 channel(..) Channel的类型，比如有NioSocketChannel, OioSocketChannel等 channelFactory(..) 如果Channel没有没有参数的构造函数，需要使用ChannelFactory构造Channel localAddress(..) Channel需要绑定的地址和端口，可以不调用，使用connect或者bind再进行设置 option(..) 一些可选的设置，使用ChannelOptions完成。比如keep-alive时间，超时时间 handler(..) 设置ChannelHandler处理事件 clone(..) 复制一个新的Bootstrap，AbstractBootstrap实现了Cloneable接口 remoteAddress()..) 设置远程地址，也可以在调用connect方法的时候设置 connect()..) 链接到远程地址 bind()..) 绑定本地端口 需要注意的是如果EventLoopGroup选择的是NioEventLoopGroup，那么对应的channel需要选择NioSocketChannel，否则会抛出兼容性的错误异常。 ServerBootstrap服务端服务启动类，内部提供的一些方法如下： 方法名 描述 group(..) 设置ServerBootstrap使用的EventLoopGroup，用来处理ServerChannel的IO操作并接收Channel channel(..) ServerChannel的类型，比如有NioServerSocketChannel, OioServerSocketChannel等 channelFactory(..) 如果ServerChannel没有没有参数的构造函数，需要使用ChannelFactory构造ServerChannel localAddress(..) ServerChannel需要绑定的地址和端口，可以不调用，使用connect或者bind再进行设置 option(..) 一些ServerChannel可选的设置，使用ChannelOptions完成。比如keep-alive时间，超时时间 childOption(..) 被ServerChannel接收的Channel的可选的设置，使用ChannelOptions完成 handler(..) 设置ServerChannel的ChannelHandler处理事件 childHandler(..) 设置被ServerChannel接收的Channel的ChannelHandler处理事件 clone(..) 复制一个新的ServerBootstrap，AbstractBootstrap实现了Cloneable接口 bind()..) 绑定本地端口 ServerBootstrap的处理过程： ServerBootstrap调用bind绑定地址和端口的时候，会创建ServerChannel。这个ServerChannel会接收客户的各个链接，针对各个链接创建Channel。 handler方法就是为ServerChannel服务的，而childHandler是给被ServerChannel接收的Channel服务器的。所以说只要服务器已起来，handler中的ChannelHandler就会触发，而有链接被ServerChannel接收之后childHandler中的ChannelHandler才会触发。 从一个已经存在的Channel中使用Bootstrap启动客户端在ServerBootstrap接收到新的Channel的时候准备启动Bootstrap客户端的时候，可以使用一个全新的EventLoop用于处理Channel的IO模型。 但是没有必要，可以跟ServerBootstrap共享同一个EventLoop，因为一个EventLoop是跟一个线程绑定的，如果使用了多个EventLoop的话，那就相当于需要进行线程的上下文切换，需要消耗一定的资源。 1个在ServerBootstrap接收到链接之后，使用Bootstrap链接另外一个地址的处理过程： 其中第3点就是ServerChannel接收到的新的Channel，第5点是Bootstrap创建的连接到远程服务器的Channel，它们使用同一个EventLoop。","raw":null,"content":null,"categories":[{"name":"netty","slug":"netty","permalink":"http://fangjian0423.github.io/categories/netty/"}],"tags":[{"name":"nio","slug":"nio","permalink":"http://fangjian0423.github.io/tags/nio/"},{"name":"netty","slug":"netty","permalink":"http://fangjian0423.github.io/tags/netty/"}]},{"title":"Netty in Action笔记(一)","slug":"netty-in-action-note1","date":"2016-08-18T17:36:52.000Z","updated":"2016-08-18T17:41:37.000Z","comments":true,"path":"2016/08/19/netty-in-action-note1/","link":"","permalink":"http://fangjian0423.github.io/2016/08/19/netty-in-action-note1/","excerpt":"一篇读书笔记，根据章节进行下总结。已经看了5章，先对这5个章节做个小总结。","text":"一篇读书笔记，根据章节进行下总结。已经看了5章，先对这5个章节做个小总结。 第一章然后介绍了一下Netty出来的背景以及Netty所拥有的一些强大的特性，比如Netty的设计高可用，可扩展，性能高，使用NIO这种非阻塞式的IO模型等特点。 然后介绍了一下异步编程的设计，有两种方式： 基于Callback 基于Future 基于Callback的跟javascript相似，这种方式比较大的问题就是当callback多的时候，代码就变得难以阅读。 基于Future的方式就是jdk里的concurrent包里的Future接口一样，代表着未来的一个值。 Netty内部这2种异步编程方式都有使用。 之后对IO阻塞模型和NIO非阻塞模型进行了一番比较。 其中IO阻塞模型对于每一个Socket都会创建一个线程进行处理，虽然可以使用线程池解决线程过多的问题，但是底层还是使用线程处理每一个请求，系统的瓶颈在于线程的个数，并且线程多了会导致频繁的线程切换，导致CPU利用效率不高。 NIO非阻塞模型采用Reactor模式，一个Reactor线程内部使用一个多路复用器selector，这个selector可以同时注册、监听和轮询成百上千个Channel，一个IO线程可以同时并发处理多个客户端连接，就不会存在频繁的IO线程之间的切换，CPU利用率高。 之后使用JDK的NIO Api编写了一个demo，发现JDK的NIO Api比较难用，比如在ByteBuffer中进行读操作后又要进行写操作，需要调用flip进行切换，api很难用，而netty很好地解决了这个问题，这也是netty的一个优点。 第二章第二章主要就是使用netty的api编写了一个server端和client端，让读者先简单熟悉一下netty中的一些api的使用。 第三章第三章主要对netty中的一些主要的组件做一个简单的介绍。 Channel: 一个Channel表示一个通道，跟io中的stream的角色一样，但是有几点不同。 1. stream是单向的，只支持读或者写，channel是双向的，既支持读也支持写。2. stream是阻塞的，channel是并行的。 EventLoop: 当一个channel注册之后，会将这个channel绑定到EventLoop里来处理channel的IO操作，一个EventLoop对应一个线程。 EventLoopGroup: 多个EventLoop的集合 Bootstrap: 用于配置客户端netty程序，比如连接的host和port，EventLoop等 ServerBootstrap: 用于配置服务端netty程序，比如绑定的port，EventLoop等 ChannelHandler: 主要用于处理关于Channel的业务逻辑。比如转换数据格式、当channel状态改变的时候被通知到，当channel注册到EventLoop的时候被通知到以及通知一些用户执行的特殊事件等。ChannelHandler有很多很多的实现类 ChannelPipeline: 把很多ChannelHandler整合在一起并进行处理 Future or ChannelFuture: 由于所有的netty都是异步操作。异步操作的结果就是ChannelFuture ChannelInBoundHandler和ChannelOutBoundHandler是ChannelHandler的两种很常见的实现类，他们分别用于读取socket中的数据和写数据到socket中，他们存储在ChannelPipeline中用于处理socket，有一张很经典关于ChannelInboundHandler和ChannelOutboundHandler的图如下： 最后讲解了关于java对象的编码和解码，因为使用netty开发的时候需要传输字节数据，而这些字节数据可以跟java对象之间进行互相转换使编程更加方便。 第四章第四章主要讲解了netty中的4种transport，分别是OIO、NIO、Local和Embedded。 其中OIO这种transport在netty中的io.netty.channel.socket.oio包里，底层使用jdk的java.net包里的类，io模型为阻塞io模型 NIO在netty中的io.netty.channel.socket.nio包里，底层使用jdk的java.nio.channels包里的类，netty提供了2种nio的实现，分别是基于selector和基于epoll的实现。 Local在io.netty.channel.local包里，这是一种在同一个JVM中客户端与服务器进行通信的一种transport。 Embedded在io.netty.channel.embedded包里，主要用于测试，可以在不需要网络的情况下进行ChannelHandler的单元测试。 这4种transport的使用场景如下： OIO: 低连接数，低延迟，需要阻塞时使用NIO: 搞链接数Local: 同一个JVM中进行通信时使用Embedded: ChannelHandler单元测试时使用 第五章第五章主要讲解netty中的ByteBuf的使用。 Jdk的NIO中的ByteBuffer使用成本过多，netty发现了这一缺点并进行了改造，设计出了ByteBuf这个类来代替ByteBuffer，相比ByteBuffer，ByteBuf有如下几个特点： 可以定义自己的buffer类型，比如heap buffer, direct buffer等 可以使用内置的composite buffer类型完成零拷贝 buffer容量可以扩展 不需要调用flip来切换读写模式 区分readerIndex和writerIndex 方法链式调用 使用引用计数 可以使用pool来创建buffer ByteBuf工作原理：内部有2个索引，分别是readerIndex和writerIndex，初始化时，这2个值都是0，当写入数据到buffer中，writerIndex会增加；当读取数据时，readerIndex会增加。当readerIndex = writerIndex的时候，再进行读取将会抛出IndexOutOfBoundsException异常。 ByteBuf是有容量概念的，默认情况下的最大的容量是Integer.MAX_VALUE。当writerIndex超过这个容量大小时，将会抛出异常。 下图就是一个ByteBuf的结构： ByteBuf共有3种类型： heap buffer: 存储在JVM的堆中，内部使用一个字节数组存储字节。可以使用ByteBuf的hasArray方法判断是否是heap buffer direct buffer：在堆外直接内存中分配，效率高，相比于堆内分配的buffer，在堆外直接内存中分配的buffer少了一次缓冲区的内存拷贝(实际上，当使用一个非直接内存buffer的时候，在发送buffer数据出去之前jvm内部会拷贝这个buffer到堆外直接内存中) composite buffer：组合型的buffer，比如一个ByteBuf由2个ByteBuf构成，可以使用Composite Buffer完成，可以避免创建一个新的ByteBuf来整合这2个ByteBuf导致的内存拷贝 以上2、3点再加上netty文件传输采用的transferTo方法(可以直接将文件缓存区的数据发送到目标Channel，不需要通过循环write方式导致的内存拷贝问题)构成了Netty的零拷贝。 接下来就是ByteBuf的一些常用方法介绍，比如copy, slice, duplicate, readInt, writeInt, indexOf, clear, discardReadBytes方法等等。 一些参考资料： http://www.open-open.com/news/view/1d31d83 http://www.infoq.com/cn/articles/netty-high-performance http://www.infoq.com/cn/author/%E6%9D%8E%E6%9E%97%E9%94%8B#%E6%96%87%E7%AB%A0","raw":null,"content":null,"categories":[{"name":"netty","slug":"netty","permalink":"http://fangjian0423.github.io/categories/netty/"}],"tags":[{"name":"nio","slug":"nio","permalink":"http://fangjian0423.github.io/tags/nio/"},{"name":"netty","slug":"netty","permalink":"http://fangjian0423.github.io/tags/netty/"}]},{"title":"SpringBoot的actuator模块","slug":"springboot-actuator","date":"2016-06-25T06:52:52.000Z","updated":"2016-06-25T07:38:28.000Z","comments":true,"path":"2016/06/25/springboot-actuator/","link":"","permalink":"http://fangjian0423.github.io/2016/06/25/springboot-actuator/","excerpt":"springboot内部提供了一个模块spring-boot-actuator用于监控和管理springboot应用。\n这个模块内部提供了很多功能，endpoint就是其中一块功能。\n我们在sbt中加入这个模块的依赖：\nlibraryDependencies += &quot;org.springframework.boot&quot; % &quot;spring-boot-starter-actuator&quot; % &quot;1.3.5.RELEASE&quot;\n然后启动项目，访问地址 http://localhost:8080/health，看到以下页面：\n{\n    status: &quot;UP&quot;,\n    diskSpace: {\n        status: &quot;UP&quot;,\n        total: 249779191808,\n        free: 22231089152,\n        threshold: 10485760\n    },\n    db: {\n        status: &quot;UP&quot;,\n        database: &quot;H2&quot;,\n        hello: 1\n    }\n}\n","text":"springboot内部提供了一个模块spring-boot-actuator用于监控和管理springboot应用。 这个模块内部提供了很多功能，endpoint就是其中一块功能。 我们在sbt中加入这个模块的依赖： libraryDependencies += &quot;org.springframework.boot&quot; % &quot;spring-boot-starter-actuator&quot; % &quot;1.3.5.RELEASE&quot; 然后启动项目，访问地址 http://localhost:8080/health，看到以下页面： { status: &quot;UP&quot;, diskSpace: { status: &quot;UP&quot;, total: 249779191808, free: 22231089152, threshold: 10485760 }, db: { status: &quot;UP&quot;, database: &quot;H2&quot;, hello: 1 } } 这个/health endpoint显示了目前应用的一些健康情况。 Endpointspring-boot-autoconfigure模块中还提供了其他很多endpoint，比如 /beans(查看spring工厂信息，里面存在哪些bean)、/dump(应用中的所有线程状态)、/env(应用环境信息，比如jvm环境信息、配置文件内容、应用端口等信息)、/mappings(SpringMVC的RequestMapping映射信息)、/configprops(框架配置信息，比如数据源、freemarker、spring的配置信息)、/metrics(度量信息)等等。 具体其他的endpoint可以查看springboot官方文档上的信息： http://docs.spring.io/spring-boot/docs/current-SNAPSHOT/reference/htmlsingle/#production-ready-endpoints 这些endpoint是如何暴露出来的呢，是通过SpringBoot内部的一个Endpoint接口完成的。 public interface Endpoint&lt;T&gt; { String getId(); // endpoint的唯一标识 boolean isEnabled(); // 是否可用 boolean isSensitive(); // 是否对一般用户可见 T invoke(); // 具体的执行过程，返回值会被解析成json暴露出口 } 这个接口的实现类有DumpEndpoint、BeansEndpoint、InfoEndpoint、HealthEndpoint、RequestMappingEndpoint等等。这些EndPoint实现类就是对应对外暴露的endpoint。BeansEndpoint的代码如下： @ConfigurationProperties(prefix = &quot;endpoints.beans&quot;) // 配置以endpoints.beans开头，可以覆盖 public class BeansEndpoint extends AbstractEndpoint&lt;List&lt;Object&gt;&gt; implements ApplicationContextAware { private final LiveBeansView liveBeansView = new LiveBeansView(); private final JsonParser parser = JsonParserFactory.getJsonParser(); public BeansEndpoint() { super(&quot;beans&quot;); // id为beans } @Override public void setApplicationContext(ApplicationContext context) throws BeansException { if (context.getEnvironment() .getProperty(LiveBeansView.MBEAN_DOMAIN_PROPERTY_NAME) == null) { this.liveBeansView.setApplicationContext(context); } } @Override public List&lt;Object&gt; invoke() { return this.parser.parseList(this.liveBeansView.getSnapshotAsJson()); // 返回值就是最后展示的json数组 } } 可以使用配置覆盖默认的beans endpoint中的信息： endpoints.beans.enabled= # Enable the endpoint. endpoints.beans.id= # Endpoint identifier. endpoints.beans.path= # Endpoint path. endpoints.beans.sensitive= # Mark if the endpoint exposes sensitive information. Health IndicatorHealthEndpoint这个endpoint是暴露通过扫描出的HealthIndicator接口的实现类完成的。 用于查看应用的健康状况。 在我们的应用中只使用了h2数据库，最终/health显示出来的内容如下(只有硬盘容量和数据库健康状况)： { status: &quot;UP&quot;, diskSpace: { status: &quot;UP&quot;, total: 249779191808, free: 20595720192, threshold: 10485760 }, db: { status: &quot;UP&quot;, database: &quot;H2&quot;, hello: 1 } } springboot内置的HealthIndicator有这些：SolrHealthIndicator、RedisHealthIndicator、RabbitHealthIndicator、MongoHealthIndicator、ElasticsearchHealthIndicator、CassandraHealthIndicator、DiskSpaceHealthIndicator、DataSourceHealthIndicator等。 这些都是springboot内置的，我们也可以编写自定义的health indicator。 以服务器中某个目录中的文件个数不能超过某个值为健康指标作为需求进行编写health indicator。 编写HealthIndicator： @Component public class TempDirHealthIndicator implements HealthIndicator { @Override public Health health() { Health.Builder builder = new Health.Builder(); File file = new File(&quot;youdirpath&quot;); File[] fileList = file.listFiles(); if(fileList.length &gt; 10) { builder.down().withDetail(&quot;file num&quot;,fileList.length); } else { builder.up(); } return builder.build(); } } 访问 http://localhost:8080/health { status: &quot;DOWN&quot;, tempDir: { status: &quot;DOWN&quot;, file num: 34 }, diskSpace: { status: &quot;UP&quot;, total: 249779191808, free: 20690649088, threshold: 10485760 }, db: { status: &quot;UP&quot;, database: &quot;H2&quot;, hello: 1 } } MetricsMetrics服务用来做一些度量支持，springboot提供了两种Metrics，分别是gauge(单一的值)和counter(计数器，自增或自减)。springboot提供了PublicMetrics接口用来支持Metrics服务。 metrics这个endpoint中使用的metrics都是由SystemPublicMetrics完成的： { mem: 388503, mem.free: 199992, processors: 4, instance.uptime: 58260089, uptime: 21843805, systemload.average: 3.28369140625, heap.committed: 320000, heap.init: 131072, heap.used: 120007, heap: 1864192, nonheap.committed: 69528, nonheap.init: 2496, nonheap.used: 68501, nonheap: 0, threads.peak: 15, threads.daemon: 13, threads.totalStarted: 20, threads: 15, classes: 9483, classes.loaded: 9484, classes.unloaded: 1, gc.ps_scavenge.count: 9, gc.ps_scavenge.time: 152, gc.ps_marksweep.count: 2, gc.ps_marksweep.time: 167, httpsessions.max: -1, httpsessions.active: 0, datasource.primary.active: 0, datasource.primary.usage: 0, gauge.response.health: 218, gauge.response.star-star.favicon.ico: 29, counter.status.200.star-star.favicon.ico: 1, counter.status.503.health: 1 } gauge和counter度量通过GaugeService和CounterService完成。 比如我们要查看各个Controller中的接口被调用的次数话，可以使用CounterService和aop完成： @Aspect @Component class ControllerAspect @Autowired() ( counterService: CounterService ) { @Before(&quot;execution(* me.format.controller.*.*(..))&quot;) def controllerCounter(joinPoint: JoinPoint): Unit = { counterService.increment(joinPoint.getSignature.toString + &quot;-invokeNum&quot;) } } 启动应用，调用getPersons接口4次，调用get/{long} 2次，查看metrics endpoint，得到以下信息： counter.List me.format.controller.PersonController.get(HttpServletRequest)-invokeNum: 4, counter.Person me.format.controller.PersonController.get(long)-invokeNum: 2, 比如我们要查看各个Controller中的接口的延迟情况，可以使用GaugeService和aop完成： @Aspect @Component class ControllerAspect @Autowired() ( counterService: CounterService, gaugeService: GaugeService ) { @Before(&quot;execution(* me.format.controller.*.*(..))&quot;) def controllerCounter(joinPoint: JoinPoint): Unit = { counterService.increment(joinPoint.getSignature.toString + &quot;-invokeNum&quot;) } @Around(&quot;execution(* me.format.controller.*.*(..))&quot;) def controllerGauge(proceedingJoinPoint: ProceedingJoinPoint): AnyRef = { val st = System.currentTimeMillis() val result = proceedingJoinPoint.proceed() val et = System.currentTimeMillis() gaugeService.submit(proceedingJoinPoint.getSignature.toString + &quot;-invokeTime&quot;, (et - st)) result } } 启动应用，调用getPersons接口4次，调用get/{long} 2次，查看metrics endpoint，得到以下信息： gauge.List me.format.controller.PersonController.get(HttpServletRequest)-invokeTime: 4, gauge.Person me.format.controller.PersonController.get(long)-invokeTime: 11, counter.List me.format.controller.PersonController.get(HttpServletRequest)-invokeNum: 4, counter.Person me.format.controller.PersonController.get(long)-invokeNum: 2, 当然，actuator模块提供的功能远不止这些，更多的信息可以查看官方文档。 http://docs.spring.io/spring-boot/docs/current-SNAPSHOT/reference/htmlsingle/#production-ready","raw":null,"content":null,"categories":[{"name":"springboot","slug":"springboot","permalink":"http://fangjian0423.github.io/categories/springboot/"}],"tags":[{"name":"scala","slug":"scala","permalink":"http://fangjian0423.github.io/tags/scala/"},{"name":"springboot","slug":"springboot","permalink":"http://fangjian0423.github.io/tags/springboot/"}]},{"title":"SpringBoot with Scala","slug":"springboot-with-scala","date":"2016-06-22T15:22:39.000Z","updated":"2016-06-23T06:06:31.000Z","comments":true,"path":"2016/06/22/springboot-with-scala/","link":"","permalink":"http://fangjian0423.github.io/2016/06/22/springboot-with-scala/","excerpt":"一般情况下，我们使用java开发springboot应用，可以使用scala开发springboot应用吗？\n答案当然是可以的。\n今天参考了阿福老师的这篇Scala开发者的SpringBoot快速入门指南以及一位国际友人的一个spring-boot-scala-web demo。\n自己尝试地搭建了一下环境，发现用scala编写springboot应用这种体验也是非常赞的。\n下面是具体的环境搭建流程：","text":"一般情况下，我们使用java开发springboot应用，可以使用scala开发springboot应用吗？ 答案当然是可以的。 今天参考了阿福老师的这篇Scala开发者的SpringBoot快速入门指南以及一位国际友人的一个spring-boot-scala-web demo。 自己尝试地搭建了一下环境，发现用scala编写springboot应用这种体验也是非常赞的。 下面是具体的环境搭建流程： 1.使用sbt作为构建工具，由于springboot官方只有基于maven或者gradle的构建方法，所以我们只能自己写了。 参考了springboot的maven配置，比如要开发web应用，需要1个spring-boot-starter-web模块，这个spring-boot-starter-web模块内地又引用了spring-boot-starter模块，spring-boot-starter模块是一个基础的starter模块，内部的依赖如下： &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-autoconfigure&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-logging&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-core&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;commons-logging&lt;/groupId&gt; &lt;artifactId&gt;commons-logging&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.yaml&lt;/groupId&gt; &lt;artifactId&gt;snakeyaml&lt;/artifactId&gt; &lt;scope&gt;runtime&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 其中springb-boot模块内部使用了spring的依赖，spring-boot-autoconfigure模块内部有很多自动化配置的类，spring-boot-starter-web模块内部使用了spring的web模块，一些tomcat模块等。 我们将这3个模块加入到sbt中： libraryDependencies += &quot;org.springframework.boot&quot; % &quot;spring-boot&quot; % &quot;1.3.5.RELEASE&quot; libraryDependencies += &quot;org.springframework.boot&quot; % &quot;spring-boot-autoconfigure&quot; % &quot;1.3.5.RELEASE&quot; libraryDependencies += &quot;org.springframework.boot&quot; % &quot;spring-boot-starter-web&quot; % &quot;1.3.5.RELEASE&quot; 再加入jpa的依赖： libraryDependencies += &quot;org.springframework.boot&quot; % &quot;spring-boot-starter-data-jpa&quot; % &quot;1.3.5.RELEASE&quot; libraryDependencies += &quot;com.h2database&quot; % &quot;h2&quot; % &quot;1.4.192&quot; 2.编写domain，使用BeanProperty可以自动给field加入get，set方法。 @Entity class Person(pName: String, pAge: Int) { @Id @GeneratedValue @BeanProperty var id: Long = _ @BeanProperty var name: String = pName @BeanProperty var age: Int = pAge def this() = this(&quot;unknown&quot;, -1) } object Person { def apply(name: String, age: Int) = new Person(name, age) } 3.编写Repository，由于scala中没有接口这个概念，我们使用trait代替。 @Repository trait PersonRepository extends CrudRepository[Person, Long] 4.编写Controller。利用构造器依赖注入将PersonRepository注入到属性里。 @RestController class PersonController @Autowired() ( private val personRepository: PersonRepository ) { @RequestMapping(value = Array(&quot;/index&quot;)) def index(): String = { &quot;hello springboot&quot; } @RequestMapping(value = Array(&quot;/add&quot;)) def add(req: HttpServletRequest): String = { val p = Person(req.getParameter(&quot;name&quot;), Some(req.getParameter(&quot;age&quot;).toInt).getOrElse(-1)) personRepository.save(p) &quot;ok&quot; } @RequestMapping(value = Array(&quot;/getAll&quot;)) def get(req: HttpServletRequest): java.util.List[Person] = { personRepository.findAll().toList } @RequestMapping(value = Array(&quot;/get/{id}&quot;)) def get(@PathVariable() id: Long): Person = { personRepository.findOne(id) } } 5.使用CommandLineRunner做一些初始化工作。 @SpringBootApplication class MainClass { @Bean def init(personRepository: PersonRepository): CommandLineRunner = { return new CommandLineRunner { override def run(strings: String*): Unit = { personRepository.save(Person(&quot;jim&quot;, 1)) personRepository.save(Person(&quot;tom&quot;, 2)) personRepository.save(Person(&quot;jerry&quot;, 3)) } } } } 6.配置文件application.properties中加入以下配置。 spring.datasource.url=jdbc:h2:mem:AZ;DB_CLOSE_DELAY=-1;DB_CLOSE_ON_EXIT=FALSE spring.datasource.driverClassName=org.h2.Driver spring.datasource.username=sa spring.datasource.password= spring.jpa.database-platform=org.hibernate.dialect.H2Dialect 7.写一个入口对象启动应用。 object Application extends App { SpringApplication.run(classOf[MainClass]); } 测试： curl http://localhost:8080/index hello springboot curl http://localhost:8080/getPersons [{&quot;id&quot;:1,&quot;name&quot;:&quot;jim&quot;,&quot;age&quot;:1},{&quot;id&quot;:2,&quot;name&quot;:&quot;tom&quot;,&quot;age&quot;:2},{&quot;id&quot;:3,&quot;name&quot;:&quot;jerry&quot;,&quot;age&quot;:3}] curl http://localhost:8080/get/1 {&quot;id&quot;:1,&quot;name&quot;:&quot;jim&quot;,&quot;age&quot;:1} curl http://localhost:8080/add\\?name\\=format\\&amp;age\\=4 ok curl http://localhost:8080/getPersons [{&quot;id&quot;:1,&quot;name&quot;:&quot;jim&quot;,&quot;age&quot;:1},{&quot;id&quot;:2,&quot;name&quot;:&quot;tom&quot;,&quot;age&quot;:2},{&quot;id&quot;:3,&quot;name&quot;:&quot;jerry&quot;,&quot;age&quot;:3},{&quot;id&quot;:4,&quot;name&quot;:&quot;format&quot;,&quot;age&quot;:4}]","raw":null,"content":null,"categories":[{"name":"springboot","slug":"springboot","permalink":"http://fangjian0423.github.io/categories/springboot/"}],"tags":[{"name":"scala","slug":"scala","permalink":"http://fangjian0423.github.io/tags/scala/"},{"name":"springboot","slug":"springboot","permalink":"http://fangjian0423.github.io/tags/springboot/"}]},{"title":"SpringBoot内部的一些自动化配置原理","slug":"springboot-autoconfig-analysis","date":"2016-06-12T11:30:57.000Z","updated":"2016-09-23T17:07:18.000Z","comments":true,"path":"2016/06/12/springboot-autoconfig-analysis/","link":"","permalink":"http://fangjian0423.github.io/2016/06/12/springboot-autoconfig-analysis/","excerpt":"springboot用来简化Spring框架带来的大量XML配置以及复杂的依赖管理，让开发人员可以更加关注业务逻辑的开发。\n比如不使用springboot而使用SpringMVC作为web框架进行开发的时候，需要配置相关的SpringMVC配置以及对应的依赖，比较繁琐；而使用springboot的话只需要以下短短的几行代码就可以使用SpringMVC，可谓相当地方便：\n@RestController\nclass App {\n  @RequestMapping(&quot;/&quot;)\n  String home() {\n    &quot;hello&quot;\n  }\n}\n其中maven配置如下：\n&lt;parent&gt;\n    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;\n    &lt;version&gt;1.3.5.RELEASE&lt;/version&gt;\n&lt;/parent&gt;\n&lt;dependencies&gt;\n    &lt;dependency&gt;\n        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\n        &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;\n    &lt;/dependency&gt;\n&lt;/dependencies&gt;\n","text":"springboot用来简化Spring框架带来的大量XML配置以及复杂的依赖管理，让开发人员可以更加关注业务逻辑的开发。 比如不使用springboot而使用SpringMVC作为web框架进行开发的时候，需要配置相关的SpringMVC配置以及对应的依赖，比较繁琐；而使用springboot的话只需要以下短短的几行代码就可以使用SpringMVC，可谓相当地方便： @RestController class App { @RequestMapping(&quot;/&quot;) String home() { &quot;hello&quot; } } 其中maven配置如下： &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;1.3.5.RELEASE&lt;/version&gt; &lt;/parent&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 我们以使用SpringMVC并且视图使用freemarker为例，分析springboot内部是如何解析freemarker视图的。 如果要在springboot中使用freemarker视图框架，并且使用maven构建项目的时候，还需要加入以下依赖： &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-freemarker&lt;/artifactId&gt; &lt;version&gt;1.3.5.RELEASE&lt;/version&gt; &lt;/dependency&gt; 这个spring-boot-starter-freemarker依赖对应的jar包里的文件如下： META-INF ├── MANIFEST.MF ├── maven │ └── org.springframework.boot │ └── spring-boot-starter-freemarker │ ├── pom.properties │ └── pom.xml └── spring.provides 内部的pom.xml里需要的依赖如下： &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.freemarker&lt;/groupId&gt; &lt;artifactId&gt;freemarker&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context-support&lt;/artifactId&gt; &lt;/dependency&gt; 我们可以看到这个spring-boot-starter-freemarker依赖内部并没有freemarker的ViewResolver，而是仅仅加入了freemarker的依赖，还有3个依赖，分别是spring-boot-starter、spring-boot-starter-web和spring-context-support。 接下来我们来分析一下为什么在springboot中加入了freemarker的依赖spring-boot-starter-freemarker后，SpringMVC自动地构造了一个freemarker的ViewResolver？ 在分析之前，首先我们先看下maven配置，看到了一个parent配置： &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;1.3.5.RELEASE&lt;/version&gt; &lt;/parent&gt; 这个spring-boot-starter-parent的pom文件在 http://central.maven.org/maven2/org/springframework/boot/spring-boot-starter-parent/1.3.5.RELEASE/spring-boot-starter-parent-1.3.5.RELEASE.pom 里。 它内部也有一个parent： &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-dependencies&lt;/artifactId&gt; &lt;version&gt;1.3.5.RELEASE&lt;/version&gt; &lt;relativePath&gt;../../spring-boot-dependencies&lt;/relativePath&gt; &lt;/parent&gt; 这个spring-boot-dependencies的pom文件在http://central.maven.org/maven2/org/springframework/boot/spring-boot-dependencies/1.3.5.RELEASE/spring-boot-dependencies-1.3.5.RELEASE.pom，内部有很多依赖。 比如spring-boot-starter-web、spring-boot-starter-websocket、spring-boot-starter-data-solrspring-boot-starter-freemarker等等，基本上所有的依赖都在这个parent里。 我们的例子中使用了parent依赖里的两个依赖，分别是spring-boot-starter-web和spring-boot-starter-freemarker。 其中spring-boot-starter-web内部依赖了spring的两个spring web依赖：spring-web和spring-webmvc。 spring-boot-starter-web内部还依赖spring-boot-starter，这个spring-boot-starter依赖了spring核心依赖spring-core；还依赖了spring-boot和spring-boot-autoconfigure这两个。 spring-boot定义了很多基础功能类，像运行程序的SpringApplication，Logging系统，一些tomcat或者jetty这些EmbeddedServlet容器，配置属性loader等等。 包括了这些包： spring-boot-autoconfigure定义了很多自动配置的类，比如jpa，solr，redis，elasticsearch、mongo、freemarker、velocity，thymeleaf等等自动配置的类。 以freemarker为例，看一下它的自动化配置类： @Configuration // 使用Configuration注解，自动构造一些内部定义的bean @ConditionalOnClass({ freemarker.template.Configuration.class, FreeMarkerConfigurationFactory.class }) // 需要freemarker.template.Configuration和FreeMarkerConfigurationFactory这两个类存在在classpath中才会进行自动配置 @AutoConfigureAfter(WebMvcAutoConfiguration.class) // 本次自动配置需要依赖WebMvcAutoConfiguration这个配置类配置之后触发。这个WebMvcAutoConfiguration内部会配置很多Wen基础性的东西，比如RequestMappingHandlerMapping、RequestMappingHandlerAdapter等 @EnableConfigurationProperties(FreeMarkerProperties.class) // 使用FreeMarkerProperties类中的配置 public class FreeMarkerAutoConfiguration { private static final Log logger = LogFactory .getLog(FreeMarkerAutoConfiguration.class); @Autowired private ApplicationContext applicationContext; @Autowired private FreeMarkerProperties properties; @PostConstruct // 构造之后调用的方法，组要检查模板位置是否存在 public void checkTemplateLocationExists() { if (this.properties.isCheckTemplateLocation()) { TemplateLocation templatePathLocation = null; List&lt;TemplateLocation&gt; locations = new ArrayList&lt;TemplateLocation&gt;(); for (String templateLoaderPath : this.properties.getTemplateLoaderPath()) { TemplateLocation location = new TemplateLocation(templateLoaderPath); locations.add(location); if (location.exists(this.applicationContext)) { templatePathLocation = location; break; } } if (templatePathLocation == null) { logger.warn(&quot;Cannot find template location(s): &quot; + locations + &quot; (please add some templates, &quot; + &quot;check your FreeMarker configuration, or set &quot; + &quot;spring.freemarker.checkTemplateLocation=false)&quot;); } } } protected static class FreeMarkerConfiguration { @Autowired protected FreeMarkerProperties properties; protected void applyProperties(FreeMarkerConfigurationFactory factory) { factory.setTemplateLoaderPaths(this.properties.getTemplateLoaderPath()); factory.setPreferFileSystemAccess(this.properties.isPreferFileSystemAccess()); factory.setDefaultEncoding(this.properties.getCharsetName()); Properties settings = new Properties(); settings.putAll(this.properties.getSettings()); factory.setFreemarkerSettings(settings); } } @Configuration @ConditionalOnNotWebApplication // 非Web项目的自动配置 public static class FreeMarkerNonWebConfiguration extends FreeMarkerConfiguration { @Bean @ConditionalOnMissingBean public FreeMarkerConfigurationFactoryBean freeMarkerConfiguration() { FreeMarkerConfigurationFactoryBean freeMarkerFactoryBean = new FreeMarkerConfigurationFactoryBean(); applyProperties(freeMarkerFactoryBean); return freeMarkerFactoryBean; } } @Configuration // 自动配置的类 @ConditionalOnClass(Servlet.class) // 需要运行在Servlet容器下 @ConditionalOnWebApplication // 需要在Web项目下 public static class FreeMarkerWebConfiguration extends FreeMarkerConfiguration { @Bean @ConditionalOnMissingBean(FreeMarkerConfig.class) public FreeMarkerConfigurer freeMarkerConfigurer() { FreeMarkerConfigurer configurer = new FreeMarkerConfigurer(); applyProperties(configurer); return configurer; } @Bean public freemarker.template.Configuration freeMarkerConfiguration( FreeMarkerConfig configurer) { return configurer.getConfiguration(); } @Bean @ConditionalOnMissingBean(name = &quot;freeMarkerViewResolver&quot;) // 没有配置freeMarkerViewResolver这个bean的话，会自动构造一个freeMarkerViewResolver @ConditionalOnProperty(name = &quot;spring.freemarker.enabled&quot;, matchIfMissing = true) // 配置文件中开关开启的话，才会构造 public FreeMarkerViewResolver freeMarkerViewResolver() { // 构造了freemarker的ViewSolver，这就是一开始我们分析的为什么没有设置ViewResolver，但是最后却还是存在的原因 FreeMarkerViewResolver resolver = new FreeMarkerViewResolver(); this.properties.applyToViewResolver(resolver); return resolver; } } } freemarker对应的配置类： @ConfigurationProperties(prefix = &quot;spring.freemarker&quot;) // 使用配置文件中以spring.freemarker开头的配置 public class FreeMarkerProperties extends AbstractTemplateViewResolverProperties { public static final String DEFAULT_TEMPLATE_LOADER_PATH = &quot;classpath:/templates/&quot;; // 默认路径 public static final String DEFAULT_PREFIX = &quot;&quot;; // 默认前缀 public static final String DEFAULT_SUFFIX = &quot;.ftl&quot;; // 默认后缀 ... } 下面是官网上的freemarker配置： # FREEMARKER (FreeMarkerAutoConfiguration) spring.freemarker.allow-request-override=false # Set whether HttpServletRequest attributes are allowed to override (hide) controller generated model attributes of the same name. spring.freemarker.allow-session-override=false # Set whether HttpSession attributes are allowed to override (hide) controller generated model attributes of the same name. spring.freemarker.cache=false # Enable template caching. spring.freemarker.charset=UTF-8 # Template encoding. spring.freemarker.check-template-location=true # Check that the templates location exists. spring.freemarker.content-type=text/html # Content-Type value. spring.freemarker.enabled=true # Enable MVC view resolution for this technology. spring.freemarker.expose-request-attributes=false # Set whether all request attributes should be added to the model prior to merging with the template. spring.freemarker.expose-session-attributes=false # Set whether all HttpSession attributes should be added to the model prior to merging with the template. spring.freemarker.expose-spring-macro-helpers=true # Set whether to expose a RequestContext for use by Spring&apos;s macro library, under the name &quot;springMacroRequestContext&quot;. spring.freemarker.prefer-file-system-access=true # Prefer file system access for template loading. File system access enables hot detection of template changes. spring.freemarker.prefix= # Prefix that gets prepended to view names when building a URL. spring.freemarker.request-context-attribute= # Name of the RequestContext attribute for all views. spring.freemarker.settings.*= # Well-known FreeMarker keys which will be passed to FreeMarker&apos;s Configuration. spring.freemarker.suffix= # Suffix that gets appended to view names when building a URL. spring.freemarker.template-loader-path=classpath:/templates/ # Comma-separated list of template paths. spring.freemarker.view-names= # White list of view names that can be resolved. 所以说一开始我们加入了一个spring-boot-starter-freemarker依赖，这个依赖中存在freemarker的lib，满足了FreeMarkerAutoConfiguration中的ConditionalOnClass里写的freemarker.template.Configuration.class这个类存在于classpath中。 所以就构造了FreeMarkerAutoConfiguration里的ViewResolver，这个ViewResolver被自动加入到SpringMVC中。 同样地，如果我们要使用velocity模板，springboot内部也有velocity的自动配置类VelocityAutoConfiguration，原理是跟freemarker一样的。 其他： Mybatis的autoconfigure是Mybatis提供的springboot的自动配置模块，由于springboot官方没有提供mybatis的自动化配置模块，所以mybatis自己写了这么一个模块，观察它的源码，发现基本上跟freemarker的autoconfigure模块一样，只需要构造对应的实例即可。 总结： springboot内部提供了很多自动化配置的类，这些类会判断classpath中是否存在自己需要的那个类，如果存在则会自动配置相关的配置，否则就不会自动配置。 如果我们需要使用一些框架，只需要加入依赖即可，这些依赖内部是没有代码的，只是一些对应框架需要的lib，有了这些lib就会触发自动化配置，于是就能使用框架了。 这一点跟当时看springmvc的时候对response进行json或xml渲染的原理相同。springmvc中的requestmapping注解加上responsebody注解后会返回xml或者json，如果依赖中加入jackson依赖就会转换成json，如果依赖中加入xstream依赖就会转换成xml。当然，前提是springmvc中有了这两种依赖的HttpMessageConverter代码，这个HttpMessageConverter代码就相当于springboot中的各种AutoConfiguration。","raw":null,"content":null,"categories":[{"name":"springboot","slug":"springboot","permalink":"http://fangjian0423.github.io/categories/springboot/"}],"tags":[{"name":"springboot","slug":"springboot","permalink":"http://fangjian0423.github.io/tags/springboot/"}]},{"title":"Java线程状态分析","slug":"java-thread-state","date":"2016-06-04T15:53:35.000Z","updated":"2016-06-04T15:53:47.000Z","comments":true,"path":"2016/06/04/java-thread-state/","link":"","permalink":"http://fangjian0423.github.io/2016/06/04/java-thread-state/","excerpt":"Java线程的生命周期中，存在几种状态。在Thread类里有一个枚举类型State，定义了线程的几种状态，分别有：\n\nNEW: 线程创建之后，但是还没有启动(not yet started)。这时候它的状态就是NEW\nRUNNABLE: 正在Java虚拟机下跑任务的线程的状态。在RUNNABLE状态下的线程可能会处于等待状态， 因为它正在等待一些系统资源的释放，比如IO\nBLOCKED: 阻塞状态，等待锁的释放，比如线程A进入了一个synchronized方法，线程B也想进入这个方法，但是这个方法的锁已经被线程A获取了，这个时候线程B就处于BLOCKED状态\nWAITING: 等待状态，处于等待状态的线程是由于执行了3个方法中的任意方法。 1. Object的wait方法，并且没有使用timeout参数; 2. Thread的join方法，没有使用timeout参数 3. LockSupport的park方法。  处于waiting状态的线程会等待另外一个线程处理特殊的行为。 再举个例子，如果一个线程调用了一个对象的wait方法，那么这个线程就会处于waiting状态直到另外一个线程调用这个对象的notify或者notifyAll方法后才会解除这个状态\nTIMED_WAITING: 有等待时间的等待状态，比如调用了以下几个方法中的任意方法，并且指定了等待时间，线程就会处于这个状态。 1. Thread.sleep方法 2. Object的wait方法，带有时间 3. Thread.join方法，带有时间 4. LockSupport的parkNanos方法，带有时间 5. LockSupport的parkUntil方法，带有时间\nTERMINATED: 线程中止的状态，这个线程已经完整地执行了它的任务\n","text":"Java线程的生命周期中，存在几种状态。在Thread类里有一个枚举类型State，定义了线程的几种状态，分别有： NEW: 线程创建之后，但是还没有启动(not yet started)。这时候它的状态就是NEW RUNNABLE: 正在Java虚拟机下跑任务的线程的状态。在RUNNABLE状态下的线程可能会处于等待状态， 因为它正在等待一些系统资源的释放，比如IO BLOCKED: 阻塞状态，等待锁的释放，比如线程A进入了一个synchronized方法，线程B也想进入这个方法，但是这个方法的锁已经被线程A获取了，这个时候线程B就处于BLOCKED状态 WAITING: 等待状态，处于等待状态的线程是由于执行了3个方法中的任意方法。 1. Object的wait方法，并且没有使用timeout参数; 2. Thread的join方法，没有使用timeout参数 3. LockSupport的park方法。 处于waiting状态的线程会等待另外一个线程处理特殊的行为。 再举个例子，如果一个线程调用了一个对象的wait方法，那么这个线程就会处于waiting状态直到另外一个线程调用这个对象的notify或者notifyAll方法后才会解除这个状态 TIMED_WAITING: 有等待时间的等待状态，比如调用了以下几个方法中的任意方法，并且指定了等待时间，线程就会处于这个状态。 1. Thread.sleep方法 2. Object的wait方法，带有时间 3. Thread.join方法，带有时间 4. LockSupport的parkNanos方法，带有时间 5. LockSupport的parkUntil方法，带有时间 TERMINATED: 线程中止的状态，这个线程已经完整地执行了它的任务 下面通过几个例子再次说明一下在什么情况下，线程会处于这几种状态： NEW状态NEW状态比较简单，实例化一个线程之后，并且这个线程没有开始执行，这个时候的状态就是NEW： Thread thread = new Thread(); System.out.println(thread.getState()); // NEW RUNNABLE状态正在运行的状态。 Thread thread = new Thread(new Runnable() { @Override public void run() { for(int i = 0; i &lt; Integer.MAX_VALUE; i ++) { System.out.println(i); } } }, &quot;RUNNABLE-Thread&quot;); thread.start(); 使用jstack查看线程状态： &quot;RUNNABLE-Thread&quot; #10 prio=5 os_prio=31 tid=0x00007f8e04981000 nid=0x4f03 runnable [0x000070000124c000] java.lang.Thread.State: RUNNABLE at java.io.FileOutputStream.writeBytes(Native Method) at java.io.FileOutputStream.write(FileOutputStream.java:315) at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82) at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140) - locked &lt;0x000000079764cc50&gt; (a java.io.BufferedOutputStream) at java.io.PrintStream.write(PrintStream.java:482) - locked &lt;0x0000000797604dc0&gt; (a java.io.PrintStream) at sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:221) at sun.nio.cs.StreamEncoder.implFlushBuffer(StreamEncoder.java:291) at sun.nio.cs.StreamEncoder.flushBuffer(StreamEncoder.java:104) - locked &lt;0x0000000797604d78&gt; (a java.io.OutputStreamWriter) at java.io.OutputStreamWriter.flushBuffer(OutputStreamWriter.java:185) at java.io.PrintStream.write(PrintStream.java:527) - eliminated &lt;0x0000000797604dc0&gt; (a java.io.PrintStream) at java.io.PrintStream.print(PrintStream.java:597) at java.io.PrintStream.println(PrintStream.java:736) - locked &lt;0x0000000797604dc0&gt; (a java.io.PrintStream) at study.thread.ThreadStateTest$1.run(ThreadStateTest.java:23) at java.lang.Thread.run(Thread.java:745) BLOCKED状态线程A和线程B都需要持有lock对象的锁才能调用方法。如果线程A持有锁，那么线程B处于BLOCKED状态；如果线程B持有锁，那么线程A处于BLOCKED状态。例子中使用Thread.sleep方法主要是用于调试方便： final Object lock = new Object(); Thread threadA = new Thread(new Runnable() { @Override public void run() { synchronized (lock) { System.out.println(Thread.currentThread().getName() + &quot; invoke&quot;); try { Thread.sleep(20000l); } catch (InterruptedException e) { e.printStackTrace(); } } } }, &quot;BLOCKED-Thread-A&quot;); Thread threadB = new Thread(new Runnable() { @Override public void run() { synchronized (lock) { System.out.println(Thread.currentThread().getName() + &quot; invoke&quot;); try { Thread.sleep(20000l); } catch (InterruptedException e) { e.printStackTrace(); } } } }, &quot;BLOCKED-Thread-B&quot;); threadA.start(); threadB.start(); 使用jstack查看线程状态。由于线程A先执行，线程B后执行，而且线程A执行后调用了Thread.sleep方法，所以线程A会处于TIMED_WAITING状态，线程B处于BLOCKED状态： &quot;BLOCKED-Thread-B&quot; #11 prio=5 os_prio=31 tid=0x00007fa7db8ff000 nid=0x5103 waiting for monitor entry [0x000070000134f000] java.lang.Thread.State: BLOCKED (on object monitor) at study.thread.ThreadStateTest$3.run(ThreadStateTest.java:50) - waiting to lock &lt;0x0000000795a03bf8&gt; (a java.lang.Object) at java.lang.Thread.run(Thread.java:745) &quot;BLOCKED-Thread-A&quot; #10 prio=5 os_prio=31 tid=0x00007fa7db15a000 nid=0x4f03 waiting on condition [0x000070000124c000] java.lang.Thread.State: TIMED_WAITING (sleeping) at java.lang.Thread.sleep(Native Method) at study.thread.ThreadStateTest$2.run(ThreadStateTest.java:39) - locked &lt;0x0000000795a03bf8&gt; (a java.lang.Object) at java.lang.Thread.run(Thread.java:745) WAITING状态Object的wait方法、Thread的join方法以及Conditon的await方法都会产生WAITING状态。 1.没有时间参数的Object的wait方法 final Object lock = new Object(); Thread threadA = new Thread(new Runnable() { @Override public void run() { synchronized (lock) { try { lock.wait(); System.out.println(&quot;wait over&quot;); } catch (InterruptedException e) { e.printStackTrace(); } } } }, &quot;WAITING-Thread-A&quot;); Thread threadB = new Thread(new Runnable() { @Override public void run() { synchronized (lock) { try { Thread.sleep(20000); } catch (InterruptedException e) { e.printStackTrace(); } lock.notifyAll(); } } }, &quot;WAITING-Thread-B&quot;); threadA.start(); threadB.start(); WAITING-Thread-A调用了lock的wait，处于WAITING状态： &quot;WAITING-Thread-B&quot; #11 prio=5 os_prio=31 tid=0x00007f8de992d800 nid=0x5103 waiting on condition [0x000070000134f000] java.lang.Thread.State: TIMED_WAITING (sleeping) at java.lang.Thread.sleep(Native Method) at study.thread.ThreadStateTest$5.run(ThreadStateTest.java:84) - locked &lt;0x0000000795a03e40&gt; (a java.lang.Object) at java.lang.Thread.run(Thread.java:745) &quot;WAITING-Thread-A&quot; #10 prio=5 os_prio=31 tid=0x00007f8dea193000 nid=0x4f03 in Object.wait() [0x000070000124c000] java.lang.Thread.State: WAITING (on object monitor) at java.lang.Object.wait(Native Method) - waiting on &lt;0x0000000795a03e40&gt; (a java.lang.Object) at java.lang.Object.wait(Object.java:502) at study.thread.ThreadStateTest$4.run(ThreadStateTest.java:71) - locked &lt;0x0000000795a03e40&gt; (a java.lang.Object) at java.lang.Thread.run(Thread.java:745) 2.Thread的join方法 Thread threadA = new Thread(new Runnable() { @Override public void run() { try { Thread.sleep(20000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(&quot;Thread-A over&quot;); } }, &quot;WAITING-Thread-A&quot;); threadA.start(); try { threadA.join(); } catch (InterruptedException e) { e.printStackTrace(); } 主线程main处于WAITING状态： &quot;WAITING-Thread-A&quot; #10 prio=5 os_prio=31 tid=0x00007fd2d5100000 nid=0x4e03 waiting on condition [0x000070000124c000] java.lang.Thread.State: TIMED_WAITING (sleeping) at java.lang.Thread.sleep(Native Method) at study.thread.ThreadStateTest$6.run(ThreadStateTest.java:103) at java.lang.Thread.run(Thread.java:745) &quot;main&quot; #1 prio=5 os_prio=31 tid=0x00007fd2d3815000 nid=0x1003 in Object.wait() [0x0000700000182000] java.lang.Thread.State: WAITING (on object monitor) at java.lang.Object.wait(Native Method) - waiting on &lt;0x0000000795a03ec0&gt; (a java.lang.Thread) at java.lang.Thread.join(Thread.java:1245) - locked &lt;0x0000000795a03ec0&gt; (a java.lang.Thread) at java.lang.Thread.join(Thread.java:1319) at study.thread.ThreadStateTest.WAITING_join(ThreadStateTest.java:118) at study.thread.ThreadStateTest.main(ThreadStateTest.java:13) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:483) at com.intellij.rt.execution.application.AppMain.main(AppMain.java:140) 3.没有时间参数的Condition的await方法 Condition的await方法跟Obejct的wait方法原理是一样的，故也是WAITING状态 TIMED_WAITING状态TIMED_WAITING状态跟TIMEING状态类似，是一个有等待时间的等待状态，不会一直等待下去。 最简单的TIMED_WAITING状态例子就是Thread的sleep方法： Thread threadA = new Thread(new Runnable() { @Override public void run() { try { Thread.sleep(20000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(&quot;Thread-A over&quot;); } }, &quot;WAITING-Thread-A&quot;); threadA.start(); try { Thread.sleep(5000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(threadA.getState()); // TIMED_WAITING 或者是Object的wait方法带有时间参数、Thread的join方法带有时间参数也会让线程的状态处于TIMED_WAITING状态。 TERMINATED线程终止的状态，线程执行完成，结束生命周期。 Thread threadA = new Thread(); threadA.start(); try { Thread.sleep(5000l); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(threadA.getState()); // TERMINATED 总结了解线程的状态可以分析一些问题。 比如线程处于BLOCKED状态，这个时候可以分析一下是不是lock加锁的时候忘记释放了，或者释放的时机不对。导致另外的线程一直处于BLOCKED状态。 比如线程处于WAITING状态，这个时候可以分析一下notifyAll或者signalAll方法的调用时机是否不对。 java自带的jstack工具可以分析查看线程的状态、优先级、描述等具体信息。","raw":null,"content":null,"categories":[{"name":"java","slug":"java","permalink":"http://fangjian0423.github.io/categories/java/"}],"tags":[{"name":"java","slug":"java","permalink":"http://fangjian0423.github.io/tags/java/"}]},{"title":"Java阻塞队列ArrayBlockingQueue和LinkedBlockingQueue实现原理分析","slug":"java-arrayblockingqueue-linkedblockingqueue-analysis","date":"2016-05-10T12:30:57.000Z","updated":"2016-05-10T14:59:25.000Z","comments":true,"path":"2016/05/10/java-arrayblockingqueue-linkedblockingqueue-analysis/","link":"","permalink":"http://fangjian0423.github.io/2016/05/10/java-arrayblockingqueue-linkedblockingqueue-analysis/","excerpt":"Java中的阻塞队列接口BlockingQueue继承自Queue接口。\nBlockingQueue接口提供了3个添加元素方法。\n\nadd：添加元素到队列里，添加成功返回true，由于容量满了添加失败会抛出IllegalStateException异常\noffer：添加元素到队列里，添加成功返回true，添加失败返回false\nput：添加元素到队列里，如果容量满了会阻塞直到容量不满\n\n3个删除方法。\n\npoll：删除队列头部元素，如果队列为空，返回null。否则返回元素。\nremove：基于对象找到对应的元素，并删除。删除成功返回true，否则返回false\ntake：删除队列头部元素，如果队列为空，一直阻塞到队列有元素并删除\n\n常用的阻塞队列具体类有ArrayBlockingQueue、LinkedBlockingQueue、PriorityBlockingQueue、LinkedBlockingDeque等。\n本文以ArrayBlockingQueue和LinkedBlockingQueue为例，分析它们的实现原理。","text":"Java中的阻塞队列接口BlockingQueue继承自Queue接口。 BlockingQueue接口提供了3个添加元素方法。 add：添加元素到队列里，添加成功返回true，由于容量满了添加失败会抛出IllegalStateException异常 offer：添加元素到队列里，添加成功返回true，添加失败返回false put：添加元素到队列里，如果容量满了会阻塞直到容量不满 3个删除方法。 poll：删除队列头部元素，如果队列为空，返回null。否则返回元素。 remove：基于对象找到对应的元素，并删除。删除成功返回true，否则返回false take：删除队列头部元素，如果队列为空，一直阻塞到队列有元素并删除 常用的阻塞队列具体类有ArrayBlockingQueue、LinkedBlockingQueue、PriorityBlockingQueue、LinkedBlockingDeque等。 本文以ArrayBlockingQueue和LinkedBlockingQueue为例，分析它们的实现原理。 ArrayBlockingQueueArrayBlockingQueue的原理就是使用一个可重入锁和这个锁生成的两个条件对象进行并发控制(classic two-condition algorithm)。 ArrayBlockingQueue是一个带有长度的阻塞队列，初始化的时候必须要指定队列长度，且指定长度之后不允许进行修改。 它带有的属性如下： // 存储队列元素的数组，是个循环数组 final Object[] items; // 拿数据的索引，用于take，poll，peek，remove方法 int takeIndex; // 放数据的索引，用于put，offer，add方法 int putIndex; // 元素个数 int count; // 可重入锁 final ReentrantLock lock; // notEmpty条件对象，由lock创建 private final Condition notEmpty; // notFull条件对象，由lock创建 private final Condition notFull; 数据的添加ArrayBlockingQueue有不同的几个数据添加方法，add、offer、put方法。 add方法： public boolean add(E e) { if (offer(e)) return true; else throw new IllegalStateException(&quot;Queue full&quot;); } add方法内部调用offer方法如下： public boolean offer(E e) { checkNotNull(e); // 不允许元素为空 final ReentrantLock lock = this.lock; lock.lock(); // 加锁，保证调用offer方法的时候只有1个线程 try { if (count == items.length) // 如果队列已满 return false; // 直接返回false，添加失败 else { insert(e); // 数组没满的话调用insert方法 return true; // 返回true，添加成功 } } finally { lock.unlock(); // 释放锁，让其他线程可以调用offer方法 } } insert方法如下： private void insert(E x) { items[putIndex] = x; // 元素添加到数组里 putIndex = inc(putIndex); // 放数据索引+1，当索引满了变成0 ++count; // 元素个数+1 notEmpty.signal(); // 使用条件对象notEmpty通知，比如使用take方法的时候队列里没有数据，被阻塞。这个时候队列insert了一条数据，需要调用signal进行通知 } put方法： public void put(E e) throws InterruptedException { checkNotNull(e); // 不允许元素为空 final ReentrantLock lock = this.lock; lock.lockInterruptibly(); // 加锁，保证调用put方法的时候只有1个线程 try { while (count == items.length) // 如果队列满了，阻塞当前线程，并加入到条件对象notFull的等待队列里 notFull.await(); // 线程阻塞并被挂起，同时释放锁 insert(e); // 调用insert方法 } finally { lock.unlock(); // 释放锁，让其他线程可以调用put方法 } } ArrayBlockingQueue的添加数据方法有add，put，offer这3个方法，总结如下： add方法内部调用offer方法，如果队列满了，抛出IllegalStateException异常，否则返回true offer方法如果队列满了，返回false，否则返回true add方法和offer方法不会阻塞线程，put方法如果队列满了会阻塞线程，直到有线程消费了队列里的数据才有可能被唤醒。 这3个方法内部都会使用可重入锁保证原子性。 数据的删除ArrayBlockingQueue有不同的几个数据删除方法，poll、take、remove方法。 poll方法： public E poll() { final ReentrantLock lock = this.lock; lock.lock(); // 加锁，保证调用poll方法的时候只有1个线程 try { return (count == 0) ? null : extract(); // 如果队列里没元素了，返回null，否则调用extract方法 } finally { lock.unlock(); // 释放锁，让其他线程可以调用poll方法 } } poll方法内部调用extract方法： private E extract() { final Object[] items = this.items; E x = this.&lt;E&gt;cast(items[takeIndex]); // 得到取索引位置上的元素 items[takeIndex] = null; // 对应取索引上的数据清空 takeIndex = inc(takeIndex); // 取数据索引+1，当索引满了变成0 --count; // 元素个数-1 notFull.signal(); // 使用条件对象notFull通知，比如使用put方法放数据的时候队列已满，被阻塞。这个时候消费了一条数据，队列没满了，就需要调用signal进行通知 return x; // 返回元素 } take方法： public E take() throws InterruptedException { final ReentrantLock lock = this.lock; lock.lockInterruptibly(); // 加锁，保证调用take方法的时候只有1个线程 try { while (count == 0) // 如果队列空，阻塞当前线程，并加入到条件对象notEmpty的等待队列里 notEmpty.await(); // 线程阻塞并被挂起，同时释放锁 return extract(); // 调用extract方法 } finally { lock.unlock(); // 释放锁，让其他线程可以调用take方法 } } remove方法： public boolean remove(Object o) { if (o == null) return false; final Object[] items = this.items; final ReentrantLock lock = this.lock; lock.lock(); // 加锁，保证调用remove方法的时候只有1个线程 try { for (int i = takeIndex, k = count; k &gt; 0; i = inc(i), k--) { // 遍历元素 if (o.equals(items[i])) { // 两个对象相等的话 removeAt(i); // 调用removeAt方法 return true; // 删除成功，返回true } } return false; // 删除成功，返回false } finally { lock.unlock(); // 释放锁，让其他线程可以调用remove方法 } } removeAt方法： void removeAt(int i) { final Object[] items = this.items; if (i == takeIndex) { // 如果要删除数据的索引是取索引位置，直接删除取索引位置上的数据，然后取索引+1即可 items[takeIndex] = null; takeIndex = inc(takeIndex); } else { // 如果要删除数据的索引不是取索引位置，移动元素元素，更新取索引和放索引的值 for (;;) { int nexti = inc(i); if (nexti != putIndex) { items[i] = items[nexti]; i = nexti; } else { items[i] = null; putIndex = i; break; } } } --count; // 元素个数-1 notFull.signal(); // 使用条件对象notFull通知，比如使用put方法放数据的时候队列已满，被阻塞。这个时候消费了一条数据，队列没满了，就需要调用signal进行通知 } ArrayBlockingQueue的删除数据方法有poll，take，remove这3个方法，总结如下： poll方法对于队列为空的情况，返回null，否则返回队列头部元素。 remove方法取的元素是基于对象的下标值，删除成功返回true，否则返回false。 poll方法和remove方法不会阻塞线程。 take方法对于队列为空的情况，会阻塞并挂起当前线程，直到有数据加入到队列中。 这3个方法内部都会调用notFull.signal方法通知正在等待队列满情况下的阻塞线程。 LinkedBlockingQueueLinkedBlockingQueue是一个使用链表完成队列操作的阻塞队列。链表是单向链表，而不是双向链表。 内部使用放锁和拿锁，这两个锁实现阻塞(“two lock queue” algorithm)。 它带有的属性如下： // 容量大小 private final int capacity; // 元素个数，因为有2个锁，存在竞态条件，使用AtomicInteger private final AtomicInteger count = new AtomicInteger(0); // 头结点 private transient Node&lt;E&gt; head; // 尾节点 private transient Node&lt;E&gt; last; // 拿锁 private final ReentrantLock takeLock = new ReentrantLock(); // 拿锁的条件对象 private final Condition notEmpty = takeLock.newCondition(); // 放锁 private final ReentrantLock putLock = new ReentrantLock(); // 放锁的条件对象 private final Condition notFull = putLock.newCondition(); ArrayBlockingQueue只有1个锁，添加数据和删除数据的时候只能有1个被执行，不允许并行执行。 而LinkedBlockingQueue有2个锁，放锁和拿锁，添加数据和删除数据是可以并行进行的，当然添加数据和删除数据的时候只能有1个线程各自执行。 数据的添加LinkedBlockingQueue有不同的几个数据添加方法，add、offer、put方法。 add方法内部调用offer方法： public boolean offer(E e) { if (e == null) throw new NullPointerException(); // 不允许空元素 final AtomicInteger count = this.count; if (count.get() == capacity) // 如果容量满了，返回false return false; int c = -1; Node&lt;E&gt; node = new Node(e); // 容量没满，以新元素构造节点 final ReentrantLock putLock = this.putLock; putLock.lock(); // 放锁加锁，保证调用offer方法的时候只有1个线程 try { if (count.get() &lt; capacity) { // 再次判断容量是否已满，因为可能拿锁在进行消费数据，没满的话继续执行 enqueue(node); // 节点添加到链表尾部 c = count.getAndIncrement(); // 元素个数+1 if (c + 1 &lt; capacity) // 如果容量还没满 notFull.signal(); // 在放锁的条件对象notFull上唤醒正在等待的线程，表示可以再次往队列里面加数据了，队列还没满 } } finally { putLock.unlock(); // 释放放锁，让其他线程可以调用offer方法 } if (c == 0) // 由于存在放锁和拿锁，这里可能拿锁一直在消费数据，count会变化。这里的if条件表示如果队列中还有1条数据 signalNotEmpty(); // 在拿锁的条件对象notEmpty上唤醒正在等待的1个线程，表示队列里还有1条数据，可以进行消费 return c &gt;= 0; // 添加成功返回true，否则返回false } put方法： public void put(E e) throws InterruptedException { if (e == null) throw new NullPointerException(); // 不允许空元素 int c = -1; Node&lt;E&gt; node = new Node(e); // 以新元素构造节点 final ReentrantLock putLock = this.putLock; final AtomicInteger count = this.count; putLock.lockInterruptibly(); // 放锁加锁，保证调用put方法的时候只有1个线程 try { while (count.get() == capacity) { // 如果容量满了 notFull.await(); // 阻塞并挂起当前线程 } enqueue(node); // 节点添加到链表尾部 c = count.getAndIncrement(); // 元素个数+1 if (c + 1 &lt; capacity) // 如果容量还没满 notFull.signal(); // 在放锁的条件对象notFull上唤醒正在等待的线程，表示可以再次往队列里面加数据了，队列还没满 } finally { putLock.unlock(); // 释放放锁，让其他线程可以调用put方法 } if (c == 0) // 由于存在放锁和拿锁，这里可能拿锁一直在消费数据，count会变化。这里的if条件表示如果队列中还有1条数据 signalNotEmpty(); // 在拿锁的条件对象notEmpty上唤醒正在等待的1个线程，表示队列里还有1条数据，可以进行消费 } LinkedBlockingQueue的添加数据方法add，put，offer跟ArrayBlockingQueue一样，不同的是它们的底层实现不一样。 ArrayBlockingQueue中放入数据阻塞的时候，需要消费数据才能唤醒。 而LinkedBlockingQueue中放入数据阻塞的时候，因为它内部有2个锁，可以并行执行放入数据和消费数据，不仅在消费数据的时候进行唤醒插入阻塞的线程，同时在插入的时候如果容量还没满，也会唤醒插入阻塞的线程。 数据的删除LinkedBlockingQueue有不同的几个数据删除方法，poll、take、remove方法。 poll方法： public E poll() { final AtomicInteger count = this.count; if (count.get() == 0) // 如果元素个数为0 return null; // 返回null E x = null; int c = -1; final ReentrantLock takeLock = this.takeLock; takeLock.lock(); // 拿锁加锁，保证调用poll方法的时候只有1个线程 try { if (count.get() &gt; 0) { // 判断队列里是否还有数据 x = dequeue(); // 删除头结点 c = count.getAndDecrement(); // 元素个数-1 if (c &gt; 1) // 如果队列里还有元素 notEmpty.signal(); // 在拿锁的条件对象notEmpty上唤醒正在等待的线程，表示队列里还有数据，可以再次消费 } } finally { takeLock.unlock(); // 释放拿锁，让其他线程可以调用poll方法 } if (c == capacity) // 由于存在放锁和拿锁，这里可能放锁一直在添加数据，count会变化。这里的if条件表示如果队列中还可以再插入数据 signalNotFull(); // 在放锁的条件对象notFull上唤醒正在等待的1个线程，表示队列里还能再次添加数据 return x; } take方法： public E take() throws InterruptedException { E x; int c = -1; final AtomicInteger count = this.count; final ReentrantLock takeLock = this.takeLock; takeLock.lockInterruptibly(); // 拿锁加锁，保证调用take方法的时候只有1个线程 try { while (count.get() == 0) { // 如果队列里已经没有元素了 notEmpty.await(); // 阻塞并挂起当前线程 } x = dequeue(); // 删除头结点 c = count.getAndDecrement(); // 元素个数-1 if (c &gt; 1) // 如果队列里还有元素 notEmpty.signal(); // 在拿锁的条件对象notEmpty上唤醒正在等待的线程，表示队列里还有数据，可以再次消费 } finally { takeLock.unlock(); // 释放拿锁，让其他线程可以调用take方法 } if (c == capacity) // 由于存在放锁和拿锁，这里可能放锁一直在添加数据，count会变化。这里的if条件表示如果队列中还可以再插入数据 signalNotFull(); // 在放锁的条件对象notFull上唤醒正在等待的1个线程，表示队列里还能再次添加数据 return x; } remove方法： public boolean remove(Object o) { if (o == null) return false; fullyLock(); // remove操作要移动的位置不固定，2个锁都需要加锁 try { for (Node&lt;E&gt; trail = head, p = trail.next; // 从链表头结点开始遍历 p != null; trail = p, p = p.next) { if (o.equals(p.item)) { // 判断是否找到对象 unlink(p, trail); // 修改节点的链接信息，同时调用notFull的signal方法 return true; } } return false; } finally { fullyUnlock(); // 2个锁解锁 } } LinkedBlockingQueue的take方法对于没数据的情况下会阻塞，poll方法删除链表头结点，remove方法删除指定的对象。 需要注意的是remove方法由于要删除的数据的位置不确定，需要2个锁同时加锁。","raw":null,"content":null,"categories":[{"name":"java","slug":"java","permalink":"http://fangjian0423.github.io/categories/java/"}],"tags":[{"name":"java","slug":"java","permalink":"http://fangjian0423.github.io/tags/java/"}]},{"title":"CountDownLatch和CyclicBarrier的区别","slug":"countdownlatch-cyclicbarrier-difference","date":"2016-04-30T17:59:39.000Z","updated":"2016-05-10T14:59:25.000Z","comments":true,"path":"2016/05/01/countdownlatch-cyclicbarrier-difference/","link":"","permalink":"http://fangjian0423.github.io/2016/05/01/countdownlatch-cyclicbarrier-difference/","excerpt":"CountDownLatch和CyclicBarrier都是java并发包下的工具类。\nCountDownLatch用于处理一个或多个线程等待其他所有线程完毕之后再继续进行操作。\n比如要处理一个非常耗时的任务，处理完之后需要更新这个任务的状态，需要开多线程去分批次处理任务中的各个子任务，当所有的子任务全部执行完毕之后，就可以更新任务状态了。这个时候就需要使用CountDownLatch。\nCyclicBarrier用于N个线程相互等待，当到达条件之后所有线程继续执行。\n比如一个抽奖活动，每个线程进行抽奖，当奖品全部抽完之后对各个线程中的用户进行后续操作。\n个人理解的两者之间的区别有3点：\n\nCountDownLatch可以阻塞1个或N个线程，CyclicBarrier必须要阻塞N个线程\nCountDownLatch用完之后就不能再次使用了，CyclicBarrier用完之后可以再次使用，CyclicBarrier还可以做reset操作\nCountDownLatch底层使用的是共享锁，CyclicBarrier底层使用的是ReentrantLock和这个lock的条件对象Condition\n","text":"CountDownLatch和CyclicBarrier都是java并发包下的工具类。 CountDownLatch用于处理一个或多个线程等待其他所有线程完毕之后再继续进行操作。 比如要处理一个非常耗时的任务，处理完之后需要更新这个任务的状态，需要开多线程去分批次处理任务中的各个子任务，当所有的子任务全部执行完毕之后，就可以更新任务状态了。这个时候就需要使用CountDownLatch。 CyclicBarrier用于N个线程相互等待，当到达条件之后所有线程继续执行。 比如一个抽奖活动，每个线程进行抽奖，当奖品全部抽完之后对各个线程中的用户进行后续操作。 个人理解的两者之间的区别有3点： CountDownLatch可以阻塞1个或N个线程，CyclicBarrier必须要阻塞N个线程 CountDownLatch用完之后就不能再次使用了，CyclicBarrier用完之后可以再次使用，CyclicBarrier还可以做reset操作 CountDownLatch底层使用的是共享锁，CyclicBarrier底层使用的是ReentrantLock和这个lock的条件对象Condition 例子1个CountDownLatch例子，这个例子阻塞3个线程，分别是主线程，Thread1和Thread2。这3个线程会在调用await方法之后阻塞，直到计数器变成0： public class CountDownLatchTest { public static void main(String[] args) { System.out.println(&quot;主任务开始，一共需要进行7个子任务。第1和第2个子任务需要进行后续操作 &quot; + DateFormatUtils.format(new Date(), &quot;yyyy-MM-dd HH:mm:ss&quot;)); CountDownLatch countDownLatch = new CountDownLatch(5); for(int i = 0; i &lt; 7; i ++) { final int index = i; new Thread(new Runnable() { @Override public void run() { System.out.println(&quot;子任务在线程 &quot; + Thread.currentThread().getName() + &quot; 中运行 &quot; + DateFormatUtils.format(new Date(), &quot;yyyy-MM-dd HH:mm:ss&quot;)); if(index == 1 || index == 2) { try { countDownLatch.await(); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(&quot;子任务在线程 &quot; + Thread.currentThread().getName() + &quot; 中进行后续操作 &quot; + DateFormatUtils.format(new Date(), &quot;yyyy-MM-dd HH:mm:ss&quot;)); } if(index != 1 &amp;&amp; index != 2) { try { Thread.sleep(5000l); } catch (InterruptedException e) { e.printStackTrace(); } countDownLatch.countDown(); } } }, &quot;Thread-&quot; + i).start(); } try { countDownLatch.await(); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(&quot;主任务结束 &quot; + DateFormatUtils.format(new Date(), &quot;yyyy-MM-dd HH:mm:ss&quot;)); } } 输出： 主任务开始，一共需要进行7个子任务。第1和第2个子任务需要进行后续操作 2016-05-01 00:41:57 子任务在线程 Thread-1 中运行 2016-05-01 00:41:57 子任务在线程 Thread-2 中运行 2016-05-01 00:41:57 子任务在线程 Thread-0 中运行 2016-05-01 00:41:57 子任务在线程 Thread-3 中运行 2016-05-01 00:41:57 子任务在线程 Thread-4 中运行 2016-05-01 00:41:57 子任务在线程 Thread-5 中运行 2016-05-01 00:41:57 子任务在线程 Thread-6 中运行 2016-05-01 00:41:57 子任务在线程 Thread-2 中进行后续操作 2016-05-01 00:42:02 主任务结束 2016-05-01 00:42:02 子任务在线程 Thread-1 中进行后续操作 2016-05-01 00:42:02 1个CyclicBarrier例子，模拟抽奖，每个用户都可以抽奖，当所有的用户抽完奖之后才能开始颁发奖项： public class CyclicBarrierTest { public static void main(String[] args) throws InterruptedException { System.out.println(&quot;5个用户开始抽奖&quot; + DateFormatUtils.format(new Date(), &quot;yyyy-MM-dd HH:mm:ss&quot;)); CyclicBarrier cyclicBarrier = new CyclicBarrier(5); for(int i = 0; i &lt; 5; i ++) { final int index = i; new Thread(new Runnable() { @Override public void run() { System.out.println(Thread.currentThread().getName() + &quot; 用户开始抽奖，持续&quot;+(index+1)+&quot;秒&quot; + DateFormatUtils.format(new Date(), &quot;yyyy-MM-dd HH:mm:ss&quot;)); try { Thread.sleep((index + 1) * 1000); cyclicBarrier.await(); } catch (InterruptedException e) { e.printStackTrace(); } catch (BrokenBarrierException e) { e.printStackTrace(); } System.out.println(&quot;所有用户抽奖完毕，颁发奖项。为用户&quot; + Thread.currentThread().getName() + &quot;颁奖。&quot; + DateFormatUtils.format(new Date(), &quot;yyyy-MM-dd HH:mm:ss&quot;)); } }, &quot;Thread-&quot; + i).start(); } } } 输出： 5个用户开始抽奖2016-05-01 00:57:16 Thread-1 用户开始抽奖，持续2秒2016-05-01 00:57:16 Thread-2 用户开始抽奖，持续3秒2016-05-01 00:57:16 Thread-3 用户开始抽奖，持续4秒2016-05-01 00:57:16 Thread-4 用户开始抽奖，持续5秒2016-05-01 00:57:16 Thread-0 用户开始抽奖，持续1秒2016-05-01 00:57:16 所有用户抽奖完毕，颁发奖项。为用户Thread-0颁奖。2016-05-01 00:57:21 所有用户抽奖完毕，颁发奖项。为用户Thread-2颁奖。2016-05-01 00:57:21 所有用户抽奖完毕，颁发奖项。为用户Thread-1颁奖。2016-05-01 00:57:21 所有用户抽奖完毕，颁发奖项。为用户Thread-4颁奖。2016-05-01 00:57:21 所有用户抽奖完毕，颁发奖项。为用户Thread-3颁奖。2016-05-01 00:57:21 CyclicBarrier中的计数器到0之后，可以重用： public class CyclicBarrierTest2 { public static void main(String[] args) throws InterruptedException { System.out.println(&quot;5个用户开始抽奖&quot; + DateFormatUtils.format(new Date(), &quot;yyyy-MM-dd HH:mm:ss&quot;)); CyclicBarrier cyclicBarrier = new CyclicBarrier(5); for(int i = 0; i &lt; 5; i ++) { final int index = i; new Thread(new Runnable() { @Override public void run() { System.out.println(Thread.currentThread().getName() + &quot; 用户开始抽奖，持续&quot;+(index+1)+&quot;秒&quot; + DateFormatUtils.format(new Date(), &quot;yyyy-MM-dd HH:mm:ss&quot;)); try { Thread.sleep((index + 1) * 1000); cyclicBarrier.await(); } catch (InterruptedException e) { e.printStackTrace(); } catch (BrokenBarrierException e) { e.printStackTrace(); } System.out.println(&quot;所有用户抽奖完毕，颁发奖项。为用户&quot; + Thread.currentThread().getName() + &quot;颁奖。&quot; + DateFormatUtils.format(new Date(), &quot;yyyy-MM-dd HH:mm:ss&quot;)); } }, &quot;Thread-&quot; + i).start(); } Thread.sleep(5000l); System.out.println(&quot;下一轮抽奖开始&quot; + DateFormatUtils.format(new Date(), &quot;yyyy-MM-dd HH:mm:ss&quot;)); for(int i = 0; i &lt; 5; i ++) { final int index = i; new Thread(new Runnable() { @Override public void run() { System.out.println(Thread.currentThread().getName() + &quot; 用户开始抽奖，持续&quot;+(index+1)+&quot;秒&quot; + DateFormatUtils.format(new Date(), &quot;yyyy-MM-dd HH:mm:ss&quot;)); try { Thread.sleep((index + 1) * 1000); cyclicBarrier.await(); } catch (InterruptedException e) { e.printStackTrace(); } catch (BrokenBarrierException e) { e.printStackTrace(); } System.out.println(&quot;所有用户抽奖完毕，颁发奖项。为用户&quot; + Thread.currentThread().getName() + &quot;颁奖。&quot; + DateFormatUtils.format(new Date(), &quot;yyyy-MM-dd HH:mm:ss&quot;)); } }, &quot;Thread-&quot; + i).start(); } } } 输出： 5个用户开始抽奖2016-05-01 00:59:40 Thread-1 用户开始抽奖，持续2秒2016-05-01 00:59:40 Thread-2 用户开始抽奖，持续3秒2016-05-01 00:59:40 Thread-3 用户开始抽奖，持续4秒2016-05-01 00:59:40 Thread-4 用户开始抽奖，持续5秒2016-05-01 00:59:40 Thread-0 用户开始抽奖，持续1秒2016-05-01 00:59:40 所有用户抽奖完毕，颁发奖项。为用户Thread-4颁奖。2016-05-01 00:59:45 所有用户抽奖完毕，颁发奖项。为用户Thread-0颁奖。2016-05-01 00:59:45 所有用户抽奖完毕，颁发奖项。为用户Thread-1颁奖。2016-05-01 00:59:45 下一轮抽奖开始2016-05-01 00:59:45 所有用户抽奖完毕，颁发奖项。为用户Thread-3颁奖。2016-05-01 00:59:45 所有用户抽奖完毕，颁发奖项。为用户Thread-2颁奖。2016-05-01 00:59:45 Thread-1 用户开始抽奖，持续2秒2016-05-01 00:59:45 Thread-0 用户开始抽奖，持续1秒2016-05-01 00:59:45 Thread-2 用户开始抽奖，持续3秒2016-05-01 00:59:45 Thread-3 用户开始抽奖，持续4秒2016-05-01 00:59:45 Thread-4 用户开始抽奖，持续5秒2016-05-01 00:59:45 所有用户抽奖完毕，颁发奖项。为用户Thread-1颁奖。2016-05-01 00:59:50 所有用户抽奖完毕，颁发奖项。为用户Thread-2颁奖。2016-05-01 00:59:50 所有用户抽奖完毕，颁发奖项。为用户Thread-0颁奖。2016-05-01 00:59:50 所有用户抽奖完毕，颁发奖项。为用户Thread-4颁奖。2016-05-01 00:59:50 所有用户抽奖完毕，颁发奖项。为用户Thread-3颁奖。2016-05-01 00:59:50 底层分析CountDownLatchCountDownLatch底层使用的是共享锁，它有个内部类Sync，这个Sync继承AQS，实现了共享锁。 简单画了一下共享锁的实现。 比如有4个线程在等待队列里，并且节点类型都是共享锁。 会唤醒head节点的下一节点中的线程Thread1。head节点就变成了之前head节点的下个节点，然后再做重复操作。 这个过程是一个传播过程，会依次唤醒各个共享节点中的线程。 并发包下的另外一个工具类Semaphore底层也是使用共享锁实现的。但是它跟CountDownLatch唯一的区别就是它不会唤醒所有的共享节点中的线程，而是唤醒它能唤醒的最大线程数(由信号量可用大小决定)。 CyclicBarrierCyclicBarrier底层使用的是ReentrantLock和这个lock的条件对象Condition。 它拥有的属性如下： private static class Generation { boolean broken = false; } private final ReentrantLock lock = new ReentrantLock(); // 可重入锁 private final Condition trip = lock.newCondition(); // 可重入锁的条件对象 private final int parties; // 计数器原始值，永远不会变 private final Runnable barrierCommand; // 计数器到了之后需要执行的Runnable，可为空 private Generation generation = new Generation(); // 一个Generation对象的实例，当计数器为0的时候这个实例将会重新被构造 private int count; // 计数器当前的值 await方法： private int dowait(boolean timed, long nanos) throws InterruptedException, BrokenBarrierException, TimeoutException { final ReentrantLock lock = this.lock; lock.lock(); // 加锁，确保每次只有1个线程调用 try { final Generation g = generation; if (g.broken) // 查看generation是否已经损坏 throw new BrokenBarrierException(); if (Thread.interrupted()) { breakBarrier(); throw new InterruptedException(); } int index = --count; // 计数器减一 if (index == 0) { // 如果计数器为0 boolean ranAction = false; try { final Runnable command = barrierCommand; if (command != null) // 如果Runnable不为空，执行run方法。注意，这里是直接调用run方法，而不是启动1个新的线程 command.run(); ranAction = true; nextGeneration(); // 一个过程结束，重新开始 return 0; } finally { if (!ranAction) breakBarrier(); } } for (;;) { try { if (!timed) trip.await(); // 放到Conditon的等待队列里，同时释放锁，让其他线程执行await方法 else if (nanos &gt; 0L) nanos = trip.awaitNanos(nanos); } catch (InterruptedException ie) { if (g == generation &amp;&amp; ! g.broken) { breakBarrier(); throw ie; } else { // We&apos;re about to finish waiting even if we had not // been interrupted, so this interrupt is deemed to // &quot;belong&quot; to subsequent execution. Thread.currentThread().interrupt(); } } if (g.broken) throw new BrokenBarrierException(); if (g != generation) // 说明执行了nextGeneration方法，计数器到了0 return index; if (timed &amp;&amp; nanos &lt;= 0L) { breakBarrier(); throw new TimeoutException(); } } } finally { lock.unlock(); // 解锁 } } private void nextGeneration() { // 唤醒Conditon等待队列上的所有线程 trip.signalAll(); // 计数器值变成原始值，重新开始 count = parties; // generation被重新构造 generation = new Generation(); } 执行过程解释： 比如Thread1执行了await方法，这个时候await方法加锁，确保其他线程不能再次调用await方法。 然后在await方法中把计数器数字减一。 如果计数器还没到0：将Thread1加入到Condition的条件队列，同时释放锁。这个时候其他线程就可以获得await方法的锁并执行。 如果计数器到了0：调用Conditon的signalAll方法，把Condition等待队列上的所有线程移除，移到AQS的等待队列里，然后返回index，释放锁，之后AQS等待队列上的节点中的线程就可以被唤醒了。","raw":null,"content":null,"categories":[{"name":"java","slug":"java","permalink":"http://fangjian0423.github.io/categories/java/"}],"tags":[{"name":"java","slug":"java","permalink":"http://fangjian0423.github.io/tags/java/"}]},{"title":"Java实现同步的几种方式","slug":"java-synchronize-way","date":"2016-04-18T12:57:57.000Z","updated":"2016-05-10T14:59:25.000Z","comments":true,"path":"2016/04/18/java-synchronize-way/","link":"","permalink":"http://fangjian0423.github.io/2016/04/18/java-synchronize-way/","excerpt":"Java提供了很多同步操作，比如synchronized关键字、wait/notifyAll、ReentrantLock、Condition、一些并发包下的工具类、Semaphore，ThreadLocal、AbstractQueuedSynchronizer等。\n本文简单说明一下这几种方式的使用。\n","text":"Java提供了很多同步操作，比如synchronized关键字、wait/notifyAll、ReentrantLock、Condition、一些并发包下的工具类、Semaphore，ThreadLocal、AbstractQueuedSynchronizer等。 本文简单说明一下这几种方式的使用。 ReentrantLock可重入锁ReentrantLock可重入锁是jdk内置的一个锁对象，可以用来实现同步，基本使用方法如下： public class ReentrantLockTest { private ReentrantLock lock = new ReentrantLock(); public void execute() { lock.lock(); try { System.out.println(Thread.currentThread().getName() + &quot; do something synchronize&quot;); try { Thread.sleep(5000l); } catch (InterruptedException e) { System.err.println(Thread.currentThread().getName() + &quot; interrupted&quot;); Thread.currentThread().interrupt(); } } finally { lock.unlock(); } } public static void main(String[] args) { ReentrantLockTest reentrantLockTest = new ReentrantLockTest(); Thread thread1 = new Thread(new Runnable() { @Override public void run() { reentrantLockTest.execute(); } }); Thread thread2 = new Thread(new Runnable() { @Override public void run() { reentrantLockTest.execute(); } }); thread1.start(); thread2.start(); } } 输出： Thread-0 do something synchronize // 隔了5秒钟 输入下面 Thread-1 do something synchronize 这个例子表示同一时间段只能有1个线程执行execute方法。 可重入锁中可重入表示的意义在于对于同一个线程，可以继续调用加锁的方法，而不会被挂起。可重入锁内部维护一个计数器，对于同一个线程调用lock方法，计数器+1，调用unlock方法，计数器-1。 举个例子再次说明一下可重入的意思，在一个加锁方法execute中调用另外一个加锁方法anotherLock并不会被挂起，可以直接调用(调用execute方法时计数器+1，然后内部又调用了anotherLock方法，计数器+1，变成了2)： public void execute() { lock.lock(); try { System.out.println(Thread.currentThread().getName() + &quot; do something synchronize&quot;); try { anotherLock(); Thread.sleep(5000l); } catch (InterruptedException e) { System.err.println(Thread.currentThread().getName() + &quot; interrupted&quot;); Thread.currentThread().interrupt(); } } finally { lock.unlock(); } } public void anotherLock() { lock.lock(); try { System.out.println(Thread.currentThread().getName() + &quot; invoke anotherLock&quot;); } finally { lock.unlock(); } } 输出： Thread-0 do something synchronize Thread-0 invoke anotherLock // 隔了5秒钟 输入下面 Thread-1 do something synchronize Thread-1 invoke anotherLock synchronized关键字synchronized关键跟ReentrantLock一样，也支持可重入锁。但是它是一个关键字，是一种语法级别的同步方式，称为内置锁： public class SynchronizedKeyWordTest { public synchronized void execute() { System.out.println(Thread.currentThread().getName() + &quot; do something synchronize&quot;); try { anotherLock(); Thread.sleep(5000l); } catch (InterruptedException e) { System.err.println(Thread.currentThread().getName() + &quot; interrupted&quot;); Thread.currentThread().interrupt(); } } public synchronized void anotherLock() { System.out.println(Thread.currentThread().getName() + &quot; invoke anotherLock&quot;); } public static void main(String[] args) { SynchronizedKeyWordTest reentrantLockTest = new SynchronizedKeyWordTest(); Thread thread1 = new Thread(new Runnable() { @Override public void run() { reentrantLockTest.execute(); } }); Thread thread2 = new Thread(new Runnable() { @Override public void run() { reentrantLockTest.execute(); } }); thread1.start(); thread2.start(); } } 输出结果跟ReentrantLock一样，这个例子说明内置锁可以作用在方法上。它还可以作用到变量，静态方法上。 synchronized跟ReentrantLock相比，有几点局限性： 加锁的时候不能设置超时。ReentrantLock有提供tryLock方法，可以设置超时时间，如果超过了这个时间并且没有获取到锁，就会放弃，而synchronized却没有这种功能 ReentrantLock可以使用多个Condition，而synchronized却只能有1个 不能中断一个试图获得锁的线程 ReentrantLock可以选择公平锁和非公平锁 ReentrantLock可以获得正在等待线程的个数，计数器等 Condition条件对象条件对象的意义在于对于一个已经获取锁的线程，如果还需要等待其他条件才能继续执行的情况下，才会使用Condition条件对象。 public class ConditionTest { public static void main(String[] args) { ReentrantLock lock = new ReentrantLock(); Condition condition = lock.newCondition(); Thread thread1 = new Thread(new Runnable() { @Override public void run() { lock.lock(); try { System.out.println(Thread.currentThread().getName() + &quot; run&quot;); System.out.println(Thread.currentThread().getName() + &quot; wait for condition&quot;); try { condition.await(); System.out.println(Thread.currentThread().getName() + &quot; continue&quot;); } catch (InterruptedException e) { System.err.println(Thread.currentThread().getName() + &quot; interrupted&quot;); Thread.currentThread().interrupt(); } } finally { lock.unlock(); } } }); Thread thread2 = new Thread(new Runnable() { @Override public void run() { lock.lock(); try { System.out.println(Thread.currentThread().getName() + &quot; run&quot;); System.out.println(Thread.currentThread().getName() + &quot; sleep 5 secs&quot;); try { Thread.sleep(5000l); } catch (InterruptedException e) { System.err.println(Thread.currentThread().getName() + &quot; interrupted&quot;); Thread.currentThread().interrupt(); } condition.signalAll(); } finally { lock.unlock(); } } }); thread1.start(); thread2.start(); } } 这个例子中thread1执行到condition.await()时，当前线程会被挂起，直到thread2调用了condition.signalAll()方法之后，thread1才会重新被激活执行。 这里需要注意的是thread1调用Condition的await方法之后，thread1线程释放锁，然后马上加入到Condition的等待队列，由于thread1释放了锁，thread2获得锁并执行，thread2执行signalAll方法之后，Condition中的等待队列thread1被取出并加入到AQS中，接下来thread2执行完毕之后释放锁，由于thread1已经在AQS的等待队列中，所以thread1被唤醒，继续执行。 wait/notifyAll 方式wait/notifyAll方式跟ReentrantLock/Condition方式的原理是一样的。 Java中每个对象都拥有一个内置锁，在内置锁中调用wait，notify方法相当于调用锁的Condition条件对象的await和signalAll方法。 使用wait/notifyAll实现上面的那个Condition例子： public class WaitNotifyAllTest { public synchronized void doWait() { System.out.println(Thread.currentThread().getName() + &quot; run&quot;); System.out.println(Thread.currentThread().getName() + &quot; wait for condition&quot;); try { this.wait(); System.out.println(Thread.currentThread().getName() + &quot; continue&quot;); } catch (InterruptedException e) { System.err.println(Thread.currentThread().getName() + &quot; interrupted&quot;); Thread.currentThread().interrupt(); } } public synchronized void doNotify() { try { System.out.println(Thread.currentThread().getName() + &quot; run&quot;); System.out.println(Thread.currentThread().getName() + &quot; sleep 5 secs&quot;); Thread.sleep(5000l); this.notifyAll(); } catch (InterruptedException e) { System.err.println(Thread.currentThread().getName() + &quot; interrupted&quot;); Thread.currentThread().interrupt(); } } public static void main(String[] args) { WaitNotifyAllTest waitNotifyAllTest = new WaitNotifyAllTest(); Thread thread1 = new Thread(new Runnable() { @Override public void run() { waitNotifyAllTest.doWait(); } }); Thread thread2 = new Thread(new Runnable() { @Override public void run() { waitNotifyAllTest.doNotify(); } }); thread1.start(); thread2.start(); } } 这里需要注意的是由于Condition是由锁创建的，所以调用wait/notifyAll方法的时候需要获得当前线程的锁，否则会发生IllegalMonitorStateException异常。 ThreadLocalThreadLocal是一种把变量放到线程本地的方式来实现线程同步的。 比如SimpleDateFormat不是一个线程安全的类，可以使用ThreadLocal实现同步。 public class ThreadLocalTest { private static ThreadLocal&lt;SimpleDateFormat&gt; dateFormatThreadLocal = new ThreadLocal&lt;SimpleDateFormat&gt;() { @Override protected SimpleDateFormat initialValue() { return new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;); } }; public static void main(String[] args) { Thread thread1 = new Thread(new Runnable() { @Override public void run() { Date date = new Date(); System.out.println(dateFormatThreadLocal.get().format(date)); } }); Thread thread2 = new Thread(new Runnable() { @Override public void run() { Date date = new Date(); System.out.println(dateFormatThreadLocal.get().format(date)); } }); thread1.start(); thread2.start(); } } Semaphore信号量Semaphore信号量被用于控制特定资源在同一个时间被访问的个数。类似连接池的概念，保证资源可以被合理的使用。可以使用构造器初始化资源个数： public class SemaphoreTest { private static Semaphore semaphore = new Semaphore(2); public static void main(String[] args) { for(int i = 0; i &lt; 5; i ++) { new Thread(new Runnable() { @Override public void run() { try { semaphore.acquire(); System.out.println(Thread.currentThread().getName() + &quot; &quot; + new Date()); Thread.sleep(5000l); semaphore.release(); } catch (InterruptedException e) { System.err.println(Thread.currentThread().getName() + &quot; interrupted&quot;); } } }).start(); } } } 输出： Thread-1 Mon Apr 18 18:03:46 CST 2016 Thread-0 Mon Apr 18 18:03:46 CST 2016 Thread-3 Mon Apr 18 18:03:51 CST 2016 Thread-2 Mon Apr 18 18:03:51 CST 2016 Thread-4 Mon Apr 18 18:03:56 CST 2016 并发包下的工具类一般情况下，我们不会使用wait/notifyAll或者ReentrantLock这种比较底层的类，而是使用并发包下提供的一些工具类。 CountDownLatchCountDownLatch是一个计数器，它的构造方法中需要设置一个数值，用来设定计数的次数。每次调用countDown()方法之后，这个计数器都会减去1，CountDownLatch会一直阻塞着调用await()方法的线程，直到计数器的值变为0。 public class CountDownLatchTest { public static void main(String[] args) { CountDownLatch countDownLatch = new CountDownLatch(5); for(int i = 0; i &lt; 5; i ++) { new Thread(new Runnable() { @Override public void run() { System.out.println(Thread.currentThread().getName() + &quot; &quot; + new Date() + &quot; run&quot;); try { Thread.sleep(5000l); } catch (InterruptedException e) { e.printStackTrace(); } countDownLatch.countDown(); } }).start(); } try { countDownLatch.await(); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(&quot;all thread over&quot;); } } 输出： Thread-2 Mon Apr 18 18:18:30 CST 2016 run Thread-3 Mon Apr 18 18:18:30 CST 2016 run Thread-4 Mon Apr 18 18:18:30 CST 2016 run Thread-0 Mon Apr 18 18:18:30 CST 2016 run Thread-1 Mon Apr 18 18:18:30 CST 2016 run all thread over CyclicBarrierCyclicBarrier阻塞调用的线程，直到条件满足时，阻塞的线程同时被打开。 调用await()方法的时候，这个线程就会被阻塞，当调用await()的线程数量到达屏障数的时候，主线程就会取消所有被阻塞线程的状态。 在CyclicBarrier的构造方法中，还可以设置一个barrierAction。 在所有的屏障都到达之后，会启动一个线程来运行这里面的代码。 public class CyclicBarrierTest { public static void main(String[] args) { Random random = new Random(); CyclicBarrier cyclicBarrier = new CyclicBarrier(5); for(int i = 0; i &lt; 5; i ++) { new Thread(new Runnable() { @Override public void run() { int secs = random.nextInt(5); System.out.println(Thread.currentThread().getName() + &quot; &quot; + new Date() + &quot; run, sleep &quot; + secs + &quot; secs&quot;); try { Thread.sleep(secs * 1000); cyclicBarrier.await(); } catch (InterruptedException e) { e.printStackTrace(); } catch (BrokenBarrierException e) { e.printStackTrace(); } System.out.println(Thread.currentThread().getName() + &quot; &quot; + new Date() + &quot; runs over&quot;); } }).start(); } } } 相比CountDownLatch，CyclicBarrier是可以被循环使用的，而且遇到线程中断等情况时，还可以利用reset()方法，重置计数器，从这些方面来说，CyclicBarrier会比CountDownLatch更加灵活一些。 AbstractQueuedSynchronizerAQS是很多同步工具类的基础，比如ReentrentLock里的公平锁和非公平锁，Semaphore里的公平锁和非公平锁，CountDownLatch里的锁等他们的底层都是使用AbstractQueuedSynchronizer完成的。 基于AbstractQueuedSynchronizer自定义实现一个独占锁： public class MySynchronizer extends AbstractQueuedSynchronizer { @Override protected boolean tryAcquire(int arg) { if(compareAndSetState(0, 1)) { setExclusiveOwnerThread(Thread.currentThread()); return true; } return false; } @Override protected boolean tryRelease(int arg) { setState(0); setExclusiveOwnerThread(null); return true; } public void lock() { acquire(1); } public void unlock() { release(1); } public static void main(String[] args) { MySynchronizer mySynchronizer = new MySynchronizer(); Thread thread1 = new Thread(new Runnable() { @Override public void run() { mySynchronizer.lock(); try { System.out.println(Thread.currentThread().getName() + &quot; run&quot;); System.out.println(Thread.currentThread().getName() + &quot; will sleep 5 secs&quot;); try { Thread.sleep(5000l); System.out.println(Thread.currentThread().getName() + &quot; continue&quot;); } catch (InterruptedException e) { System.err.println(Thread.currentThread().getName() + &quot; interrupted&quot;); Thread.currentThread().interrupt(); } } finally { mySynchronizer.unlock(); } } }); Thread thread2 = new Thread(new Runnable() { @Override public void run() { mySynchronizer.lock(); try { System.out.println(Thread.currentThread().getName() + &quot; run&quot;); } finally { mySynchronizer.unlock(); } } }); thread1.start(); thread2.start(); } } MySynchronizer并没有实现可重入功能，只是简单的一个独占锁。","raw":null,"content":null,"categories":[{"name":"java","slug":"java","permalink":"http://fangjian0423.github.io/categories/java/"}],"tags":[{"name":"java","slug":"java","permalink":"http://fangjian0423.github.io/tags/java/"}]},{"title":"jdk ConcurrentSkipListMap工作原理分析","slug":"jdk-concurrentskiplistmap","date":"2016-04-12T11:49:11.000Z","updated":"2016-05-10T14:59:25.000Z","comments":true,"path":"2016/04/12/jdk-concurrentskiplistmap/","link":"","permalink":"http://fangjian0423.github.io/2016/04/12/jdk-concurrentskiplistmap/","excerpt":"ConcurrentSkipListMap是一个内部使用跳表，并且支持排序和并发的一个Map。\n跳表的介绍：\n跳表是一种允许在一个有顺序的序列中进行快速查询的数据结构。\n如果在普通的顺序链表中查询一个元素，需要从链表头部开始一个一个节点进行遍历，然后找到节点，如下图所示，要查找234元素的话需要从5元素节点开始一个一个节点进行遍历，这样的效率是非常低的。\n\n跳表可以解决这种查询时间过长的问题：\n\n从上图可以看到，跳表具有以下几种特性：\n\n由很多层组成，level越高的层节点越少，最后一层level用有所有的节点数据\n每一层的节点数据也都是有顺序的\n上面层的节点肯定会在下面层中出现\n每个节点都有两个指针，分别是同一层的下一个节点指针和下一层节点的指针\n\n使用跳表查询元素的时间复杂度是O(log n)，跟红黑树一样。查询效率还是不错的，\n但是跳表的存储容量变大了，本来一共只有7个节点的数据，使用跳表之后变成了14个节点。\n所以跳表是一种使用”空间换时间”的概念用来提高查询效率的链表，开源软件Redis、LevelDB都使用到了跳表。跳表相比B树，红黑树，AVL树时间复杂度一样，但是耗费更多存储空间，但是跳表的优势就是它相比树，实现简单，不需要考虑树的一些rebalance问题。","text":"ConcurrentSkipListMap是一个内部使用跳表，并且支持排序和并发的一个Map。 跳表的介绍： 跳表是一种允许在一个有顺序的序列中进行快速查询的数据结构。 如果在普通的顺序链表中查询一个元素，需要从链表头部开始一个一个节点进行遍历，然后找到节点，如下图所示，要查找234元素的话需要从5元素节点开始一个一个节点进行遍历，这样的效率是非常低的。 跳表可以解决这种查询时间过长的问题： 从上图可以看到，跳表具有以下几种特性： 由很多层组成，level越高的层节点越少，最后一层level用有所有的节点数据 每一层的节点数据也都是有顺序的 上面层的节点肯定会在下面层中出现 每个节点都有两个指针，分别是同一层的下一个节点指针和下一层节点的指针 使用跳表查询元素的时间复杂度是O(log n)，跟红黑树一样。查询效率还是不错的， 但是跳表的存储容量变大了，本来一共只有7个节点的数据，使用跳表之后变成了14个节点。 所以跳表是一种使用”空间换时间”的概念用来提高查询效率的链表，开源软件Redis、LevelDB都使用到了跳表。跳表相比B树，红黑树，AVL树时间复杂度一样，但是耗费更多存储空间，但是跳表的优势就是它相比树，实现简单，不需要考虑树的一些rebalance问题。 下图是一个级别更高的跳表： 跳表的查询比如要在下面这个跳表中查找93元素，过程如下： 从head节点(最上层的第一个节点)开始找，发现5比93小，继续同一层(Level3)的下一个节点150进行比较 发现105比93大，往下一层(Level2)走，然后找Level2的5元素的下一个节点67，发现67比93小，继续同一层(Level2)的下一个节点150进行比较 发现105比93大，往下一层(Level1)走，然后找Level1的67元素的下一个节点93，找到，返回 跳表新增元素跳表中新增元素的话首先会确定Level层，在这个Level以及这个Level以下的层中都加入新的元素，具体的Level层数是通过一个通过一种随机算法获取的，比如之前这个跳表在Level2和Level1中插入666元素： 如果Level大于目前跳表的层数，那么会添加新的一层。 跳表删除元素在各个层中找到对应的元素并删除即可。 ConcurrentSkipListMap分析ConcurrentSkipListMap对跳表中的几个概念做了一层封装，如下： // 每个节点的封装，跟层数没有关系 static final class Node&lt;K,V&gt; { final K key; // 节点的关键字 volatile Object value; // 节点的值 volatile Node&lt;K,V&gt; next; // 节点的next节点引用 ... } // 每一层节点的封装，叫做索引 static class Index&lt;K,V&gt; { final Node&lt;K,V&gt; node; // 对应的节点 final Index&lt;K,V&gt; down; // 下一层索引 volatile Index&lt;K,V&gt; right; // 同一层的下一个索引 ... } // 每一层的头索引 static final class HeadIndex&lt;K,V&gt; extends Index&lt;K,V&gt; { final int level; // Level 级别 HeadIndex(Node&lt;K,V&gt; node, Index&lt;K,V&gt; down, Index&lt;K,V&gt; right, int level) { super(node, down, right); this.level = level; } ... } 简单分析下ConcurrentSkipListMap的get方法： private V doGet(Object key) { if (key == null) throw new NullPointerException(); Comparator&lt;? super K&gt; cmp = comparator; outer: for (;;) { // findPredecessor方法表示找到最接近要查找节点的节点，并且这个节点在最下面那一层，这样就保证会遍历所有节点 for (Node&lt;K,V&gt; b = findPredecessor(key, cmp), n = b.next;;) { Object v; int c; if (n == null) // 已经遍历节点到最后还是没有找到，break，返回null break outer; Node&lt;K,V&gt; f = n.next; if (n != b.next) // 判断比较下一个节点是否发生了变化，如果发生变化break重新开始死循环 break; if ((v = n.value) == null) { // 如果下一个节点已经被删除了 n.helpDelete(b, f); break; } if (b.value == null || v == n) // b is deleted break; if ((c = cpr(cmp, key, n.key)) == 0) { // 比较并且找到了，直接返回 @SuppressWarnings(&quot;unchecked&quot;) V vv = (V)v; return vv; } if (c &lt; 0) // 找过头了，说明没有对应节点了，跳出循环，返回null break outer; b = n; // 继续遍历 n = f; // 继续遍历 } } return null; } private Node&lt;K,V&gt; findPredecessor(Object key, Comparator&lt;? super K&gt; cmp) { if (key == null) throw new NullPointerException(); // don&apos;t postpone errors for (;;) { // 一个死循环内部套着另外一个循环 for (Index&lt;K,V&gt; q = head, r = q.right, d;;) { // head表示最顶层的第一个索引，从这个索引开始找 if (r != null) { // 如果索引的同一层下一个索引不为null Node&lt;K,V&gt; n = r.node; K k = n.key; if (n.value == null) { // 如果是个已删除节点 if (!q.unlink(r)) // 使用cas把已删除节点从跳表上删除掉 break; // 已删除节点从跳表上删除失败，跳出重新循环 r = q.right; // 继续遍历 continue; } if (cpr(cmp, key, k) &gt; 0) { // 使用cas比较要找的关键字和索引内节点的关键字，如果满足比较条件 q = r; // 当前所在索引变成同一层下一个索引 r = r.right; // 当前所在索引的下一个索引变成下下个索引，继续遍历 continue; } } // 直到找出上一层满足不了条件的那个索引 if ((d = q.down) == null) // 找到下一层的索引 return q.node; // 如果下一层没有索引了，返回找到的最接近的节点 q = d; // 下一层开始做相同的操作 r = d.right; // 下一层开始做相同的操作 } } }","raw":null,"content":null,"categories":[{"name":"jdk","slug":"jdk","permalink":"http://fangjian0423.github.io/categories/jdk/"}],"tags":[{"name":"jdk","slug":"jdk","permalink":"http://fangjian0423.github.io/tags/jdk/"},{"name":"map","slug":"map","permalink":"http://fangjian0423.github.io/tags/map/"}]},{"title":"jdk PriorityQueue优先队列工作原理分析","slug":"jdk_priorityqueue","date":"2016-04-10T13:36:08.000Z","updated":"2016-05-10T14:59:25.000Z","comments":true,"path":"2016/04/10/jdk_priorityqueue/","link":"","permalink":"http://fangjian0423.github.io/2016/04/10/jdk_priorityqueue/","excerpt":"优先队列跟普通的队列不一样，普通队列是一种遵循FIFO规则的队列，拿数据的时候按照加入队列的顺序拿取。   而优先队列每次拿数据的时候都会拿出优先级最高的数据。\n优先队列内部维护着一个堆，每次取数据的时候都从堆顶拿数据，这就是优先队列的原理。\njdk的优先队列使用PriorityQueue这个类，使用者可以自己定义优先级规则。","text":"优先队列跟普通的队列不一样，普通队列是一种遵循FIFO规则的队列，拿数据的时候按照加入队列的顺序拿取。 而优先队列每次拿数据的时候都会拿出优先级最高的数据。 优先队列内部维护着一个堆，每次取数据的时候都从堆顶拿数据，这就是优先队列的原理。 jdk的优先队列使用PriorityQueue这个类，使用者可以自己定义优先级规则。 一个PriorityQueue例子定义一个Task类，有2个属性name和level。这个类放到PriorityQueue里，level越大优先级越高： private static class Task { String name; int level; public Task(String name, int level) { this.name = name; this.level = level; } public String getName() { return name; } public void setName(String name) { this.name = name; } public int getLevel() { return level; } public void setLevel(int level) { this.level = level; } @Override public String toString() { return &quot;Task{&quot; + &quot;name=&apos;&quot; + name + &apos;\\&apos;&apos; + &quot;, level=&quot; + level + &apos;}&apos;; } } public static void main(String[] args) { PriorityQueue&lt;Task&gt; queue = new PriorityQueue&lt;Task&gt;(6, new Comparator&lt;Task&gt;() { @Override public int compare(Task t1, Task t2) { return t2.getLevel() - t1.getLevel(); } }); queue.add(new Task(&quot;游戏&quot;, 20)); queue.add(new Task(&quot;吃饭&quot;, 100)); queue.add(new Task(&quot;睡觉&quot;, 90)); queue.add(new Task(&quot;看书&quot;, 70)); queue.add(new Task(&quot;工作&quot;, 80)); queue.add(new Task(&quot;撩妹&quot;, 10)); while(!queue.isEmpty()) { System.out.println(queue.poll()); } } 输出结果： Task{name=&apos;吃饭&apos;, level=100} Task{name=&apos;睡觉&apos;, level=90} Task{name=&apos;工作&apos;, level=80} Task{name=&apos;看书&apos;, level=70} Task{name=&apos;游戏&apos;, level=20} Task{name=&apos;撩妹&apos;, level=10} add过程其实就是在最大堆里添加新的元素，添加之后再进行调整： 出队相当于每次都是堆顶出堆，堆顶出堆之后然后重新调整： PriorityQueue原理分析首先看下PriorityQueue的属性： transient Object[] queue; // 堆 private int size = 0; // 元素个数 private final Comparator&lt;? super E&gt; comparator; // 比较器，如果是null，使用元素自身的比较器 接下来是PriorityQueue的几个方法介绍。 add，添加元素： public boolean add(E e) { return offer(e); // add方法内部调用offer方法 } public boolean offer(E e) { if (e == null) // 元素为空的话，抛出NullPointerException异常 throw new NullPointerException(); modCount++; int i = size; if (i &gt;= queue.length) // 如果当前用堆表示的数组已经满了，调用grow方法扩容 grow(i + 1); // 扩容 size = i + 1; // 元素个数+1 if (i == 0) // 堆还没有元素的情况 queue[0] = e; // 直接给堆顶赋值元素 else // 堆中已有元素的情况 siftUp(i, e); // 重新调整堆，从下往上调整，因为新增元素是加到最后一个叶子节点 return true; } private void siftUp(int k, E x) { if (comparator != null) // 比较器存在的情况下 siftUpUsingComparator(k, x); // 使用比较器调整 else // 比较器不存在的情况下 siftUpComparable(k, x); // 使用元素自身的比较器调整 } private void siftUpUsingComparator(int k, E x) { while (k &gt; 0) { // 一直循环直到父节点还存在 int parent = (k - 1) &gt;&gt;&gt; 1; // 找到父节点索引 Object e = queue[parent]; // 赋值父节点元素 if (comparator.compare(x, (E) e) &gt;= 0) // 新元素与父元素进行比较，如果满足比较器结果，直接跳出，否则进行调整 break; queue[k] = e; // 进行调整，新位置的元素变成了父元素 k = parent; // 新位置索引变成父元素索引，进行递归操作 } queue[k] = x; // 新添加的元素添加到堆中 } siftUp方法调用过程如下： poll，出队方法： public E poll() { if (size == 0) return null; int s = --size; // 元素个数-1 modCount++; E result = (E) queue[0]; // 得到堆顶元素 E x = (E) queue[s]; // 最后一个叶子节点 queue[s] = null; // 最后1个叶子节点置空 if (s != 0) siftDown(0, x); // 从上往下调整，因为删除元素是删除堆顶的元素 return result; } private void siftDown(int k, E x) { if (comparator != null) // 比较器存在的情况下 siftDownUsingComparator(k, x); // 使用比较器调整 else // 比较器不存在的情况下 siftDownComparable(k, x); // 使用元素自身的比较器调整 } private void siftDownUsingComparator(int k, E x) { int half = size &gt;&gt;&gt; 1; // 只需循环节点个数的一般即可 while (k &lt; half) { int child = (k &lt;&lt; 1) + 1; // 得到父节点的左子节点索引 Object c = queue[child]; // 得到左子元素 int right = child + 1; // 得到父节点的右子节点索引 if (right &lt; size &amp;&amp; comparator.compare((E) c, (E) queue[right]) &gt; 0) // 左子节点跟右子节点比较，取更大的值 c = queue[child = right]; if (comparator.compare(x, (E) c) &lt;= 0) // 然后这个更大的值跟最后一个叶子节点比较 break; queue[k] = c; // 新位置使用更大的值 k = child; // 新位置索引变成子元素索引，进行递归操作 } queue[k] = x; // 最后一个叶子节点添加到合适的位置 } siftDown方法调用过程如下： grow扩容方法： private void grow(int minCapacity) { int oldCapacity = queue.length; // 新容量 // 如果老容量小于64 新容量 = 老容量 + 老容量 + 2 // 如果老容量大于等于64 老容量 = 老容量 + 老容量/2 int newCapacity = oldCapacity + ((oldCapacity &lt; 64) ? (oldCapacity + 2) : (oldCapacity &gt;&gt; 1)); // 溢出处理 if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); // 使用新容量 queue = Arrays.copyOf(queue, newCapacity); } remove，删除队列元素操作： public boolean remove(Object o) { int i = indexOf(o); // 找到数据对应的索引 if (i == -1) // 不存在的话返回false return false; else { // 存在的话调用removeAt方法，返回true removeAt(i); return true; } } private E removeAt(int i) { modCount++; int s = --size; // 元素个数-1 if (s == i) // 如果是删除最后一个叶子节点 queue[i] = null; // 直接置空，删除即可，堆还是保持特质，不需要调整 else { // 如果是删除的不是最后一个叶子节点 E moved = (E) queue[s]; // 获得最后1个叶子节点元素 queue[s] = null; // 最后1个叶子节点置空 siftDown(i, moved); // 从上往下调整 if (queue[i] == moved) { // 如果从上往下调整完毕之后发现元素位置没变，从下往上调整 siftUp(i, moved); // 从下往上调整 if (queue[i] != moved) return moved; } } return null; } 下图这个堆如果删除红色节点100的时候，siftDown之后元素位置没变，所以还得siftUp： 总结 jdk内置的优先队列PriorityQueue内部使用一个堆维护数据，每当有数据add进来或者poll出去的时候会对堆做从下往上的调整和从上往下的调整 PriorityQueue不是一个线程安全的类，如果要在多线程环境下使用，可以使用PriorityBlockingQueue这个优先阻塞队列","raw":null,"content":null,"categories":[{"name":"jdk","slug":"jdk","permalink":"http://fangjian0423.github.io/categories/jdk/"}],"tags":[{"name":"jdk","slug":"jdk","permalink":"http://fangjian0423.github.io/tags/jdk/"},{"name":"queue","slug":"queue","permalink":"http://fangjian0423.github.io/tags/queue/"}]},{"title":"堆、二叉堆、堆排序","slug":"heap-heapsort","date":"2016-04-09T09:42:18.000Z","updated":"2016-05-10T14:59:25.000Z","comments":true,"path":"2016/04/09/heap-heapsort/","link":"","permalink":"http://fangjian0423.github.io/2016/04/09/heap-heapsort/","excerpt":"堆的概念：\nn个元素序列 { k1, k2, k3, k4, k5, k6 …. kn } 当且仅当满足以下关系时才会被称为堆：\nki &lt;= k2i,ki &lt;= k2i+1 或者 ki &gt;= k2i,ki &gt;= k2i+1 (i = 1,2,3,4 .. n/2)\n如果数组的下表是从0开始，那么需要满足 \nki &lt;= k2i+1,ki &lt;= k2i+2 或者 ki &gt;= k2i+1,ki &gt;= k2i+2 (i = 0,1,2,3 .. n/2)\n比如 { 1,3,5,10,15,9 } 这个序列就满足 [1 &lt;= 3; 1 &lt;= 5],  [3 &lt;= 10; 3 &lt;= 15], [5 &lt;= 9] 这3个条件，这个序列就是一个堆。\n所以堆其实是一个序列(数组)，如果这个序列满足上述条件，那么就把这个序列看成堆。\n堆的实现通常是通过构造二叉堆，因为二叉堆应用很普遍，当不加限定时，堆通常指的就是二叉堆。","text":"堆的概念： n个元素序列 { k1, k2, k3, k4, k5, k6 …. kn } 当且仅当满足以下关系时才会被称为堆： ki &lt;= k2i,ki &lt;= k2i+1 或者 ki &gt;= k2i,ki &gt;= k2i+1 (i = 1,2,3,4 .. n/2) 如果数组的下表是从0开始，那么需要满足 ki &lt;= k2i+1,ki &lt;= k2i+2 或者 ki &gt;= k2i+1,ki &gt;= k2i+2 (i = 0,1,2,3 .. n/2) 比如 { 1,3,5,10,15,9 } 这个序列就满足 [1 &lt;= 3; 1 &lt;= 5], [3 &lt;= 10; 3 &lt;= 15], [5 &lt;= 9] 这3个条件，这个序列就是一个堆。 所以堆其实是一个序列(数组)，如果这个序列满足上述条件，那么就把这个序列看成堆。 堆的实现通常是通过构造二叉堆，因为二叉堆应用很普遍，当不加限定时，堆通常指的就是二叉堆。 二叉堆的概念： 二叉堆是一种特殊的堆，是一棵完全二叉树或者是近似完全二叉树，同时二叉堆还满足堆的特性：父节点的键值总是保持固定的序关系于任何一个子节点的键值，且每个节点的左子树和右子树都是一个二叉堆。 当父节点的键值总是大于或等于任何一个子节点的键值时为最大堆。 当父节点的键值总是小于或等于任何一个子节点的键值时为最小堆。 上图中的最小堆对应的序列是： [1,3,5,9,10,15] 满足最小堆的特性(父节点的键值小于或等于任何一个子节点的键值，并且也满足堆的性质 [1 &lt;= 3; 1 &lt;= 5], [3 &lt;= 9; 3 &lt;= 10], [5 &lt;= 15]) 上图中的最大堆对应的序列是： [15,10,9,7,5,3] 满足最大堆的特性(父节点的键值大于或等于任何一个子节点的键值，并且也满足堆的性质 [15 &gt;= 10; 15 &gt;= 9], [10 &gt;= 7; 10 &gt;= 5], [9 &gt;= 3]) 堆的操作堆排序堆排序指的是对堆这种数据结构进行排序的一种算法。其基本思想如下，以最大堆为例： 将数组序列构建成最大堆[ A1, A2, A3 .. An]，这个堆是一个刚初始化无序区，同时有序区为空 堆顶元素A1与最后一个元素An进行交换，得到新的有序区[An]，无序区变成[A1 … An-1] 交换之后可能导致[A1 … An-1]这个无序区不是一个最大堆，[A1 … An-1]无序区重新调整成最大堆。重复步骤2，A1与An-1进行交换，得到新的有序区[An,An-1]，无序区变成[A1 … An-2].. 不断重复，直到有序区的个数为n-1才结束排序过程 构造堆的过程如下(以最大堆为例)： 从最后一个非叶子节点开始调整，遍历节点和2个子节点，选择键值最大的节点的键值代替父节点的键值，如果进行了调整，调整之后的两个子节点可能不符合堆特性，递归调整。一直直到调整完根节点。 以序列[3,5,15,9,10,1]为例进行的堆排序： 首先第1步先把数组转换成完全二叉树： 接下来是第2、3步构造有序区和无序区： 构造完之后有序区的元素依次是：1，3，5，9，10，15 简单地使用java写一下堆排序： public class HeapSort { public static void maxHeapify(int[] arr, int size, int index) { int leftSonIndex = 2 * index + 1; int rightSonIndex = 2 * index + 2; int temp = index; if(index &lt;= size / 2) { if(leftSonIndex &lt; size &amp;&amp; arr[temp] &lt; arr[leftSonIndex]) { temp = leftSonIndex; } if(rightSonIndex &lt; size &amp;&amp; arr[temp] &lt; arr[rightSonIndex]) { temp = rightSonIndex; } // 左右子节点的值存在比父节点的值更大 if(temp != index) { swap(arr, index, temp); // 交换值 maxHeapify(arr, size, temp); // 递归调整 } } } public static void heapSort(int[] arr, int size) { // 构造成最大堆 buildMaxHeap(arr, arr.length); for(int i = size - 1; i &gt; 0; i --) { // 先交换堆顶元素和无序区最后一个元素 swap(arr, 0, i); // 重新调整无序区 buildMaxHeap(arr, i - 1); } } public static void buildMaxHeap(int[] arr, int size) { for(int i = size / 2; i &gt;= 0; i --) { // 最后一个非叶子节点开始调整 maxHeapify(arr, size, i); } } public static void swap(int[] arr, int i, int j) { int temp = arr[i]; arr[i] = arr[j]; arr[j] = temp; } public static void main(String[] args) { int[] arr = { 3, 5, 15, 9, 10, 1}; System.out.println(&quot;before build: &quot; + Arrays.toString(arr)); // before build: [3, 5, 15, 9, 10, 1] buildMaxHeap(arr, arr.length); System.out.println(&quot;after build: &quot; + Arrays.toString(arr)); // after build: [15, 10, 3, 9, 5, 1] heapSort(arr, arr.length); System.out.println(&quot;after sort: &quot; + Arrays.toString(arr)); // after sort: [1, 3, 5, 9, 10, 15] } } 添加在最大堆[ 15,10,9,7,5,3 ]上添加一个新的元素 11 ，执行的步骤如下： 删除在最大堆[ 15,10,9,7,5,3 ]上删除元素 10 ，执行的步骤如下：","raw":null,"content":null,"categories":[{"name":"algorithm","slug":"algorithm","permalink":"http://fangjian0423.github.io/categories/algorithm/"}],"tags":[{"name":"algorithm","slug":"algorithm","permalink":"http://fangjian0423.github.io/tags/algorithm/"}]},{"title":"jdk TreeSet工作原理分析","slug":"jdk_treeset","date":"2016-04-07T16:20:23.000Z","updated":"2016-05-10T14:59:25.000Z","comments":true,"path":"2016/04/08/jdk_treeset/","link":"","permalink":"http://fangjian0423.github.io/2016/04/08/jdk_treeset/","excerpt":"TreeSet跟HashSet，LinkedHashSet一样，都是Set接口的实现类。\nHashSet内部使用的HashMap，LinkedHashSet继承HashSet，内部使用的是LinkedHashMap。\nTreeSet实现的是NavigableSet接口，而不是HashSet和LinkedHashSet实现的Set接口。\nNavigableSet接口继承自SortedSet接口，SortedSet接口继承自Set接口。\nNavigableSet接口比Set更方便，可以使用firstKey[最小关键字]，lastKey[最大关键字]，pollFirstEntry[最小键值对]，pollLastEntry[最大键值对]，higherEntry[比参数关键字要大的键值对]，lowerEntry[比参数关键字要小的键值对]等等方便方法，可以使用这些方法方便地获取期望位置上的键值对。","text":"TreeSet跟HashSet，LinkedHashSet一样，都是Set接口的实现类。 HashSet内部使用的HashMap，LinkedHashSet继承HashSet，内部使用的是LinkedHashMap。 TreeSet实现的是NavigableSet接口，而不是HashSet和LinkedHashSet实现的Set接口。 NavigableSet接口继承自SortedSet接口，SortedSet接口继承自Set接口。 NavigableSet接口比Set更方便，可以使用firstKey[最小关键字]，lastKey[最大关键字]，pollFirstEntry[最小键值对]，pollLastEntry[最大键值对]，higherEntry[比参数关键字要大的键值对]，lowerEntry[比参数关键字要小的键值对]等等方便方法，可以使用这些方法方便地获取期望位置上的键值对。 TreeSet原理分析TreeSet跟HashSet一样，内部都使用Map，HashSet内部使用的是HashMap，但是TreeSet使用的是NavigableMap。 TreeSet的几个构造方法会构造NavigableMap，如果使用没有参数的构造函数，NavigableMap是TreeMap： TreeSet(NavigableMap&lt;E,Object&gt; m) { this.m = m; } public TreeSet() { this(new TreeMap&lt;E,Object&gt;()); } public TreeSet(Comparator&lt;? super E&gt; comparator) { this(new TreeMap&lt;&gt;(comparator)); } add方法调用Map的put方法： public boolean add(E e) { return m.put(e, PRESENT)==null; } remove方法调用Map的remove方法： public boolean remove(Object o) { return m.remove(o)==PRESENT; } 原理基本跟HashSet一样。 // 最小的关键字 public E first() { return m.firstKey(); } // 最大的关键字 public E last() { return m.lastKey(); } // 比参数小的关键字 public E lower(E e) { return m.lowerKey(e); } 一个TreeSet例子使用没有参数的TreeMap构造函数，内部的Map使用TreeMap红黑树： TreeSet&lt;String&gt; set = new TreeSet&lt;String&gt;(); set.add(&quot;1:语文&quot;); set.add(&quot;2:数学&quot;); set.add(&quot;3:英语&quot;); set.add(&quot;4:政治&quot;); set.add(&quot;5:物理&quot;); set.add(&quot;6:化学&quot;); set.add(&quot;7:生物&quot;); set.add(&quot;8:体育&quot;); 内部红黑树结构如下： 还可以调用TreeSet的其他方法： set.first(); // 1:语文 set.last(); // 8:体育 set.higher(&quot;5:物理&quot;); // 6:化学 set.lower(&quot;5:物理&quot;); // 4:政治","raw":null,"content":null,"categories":[{"name":"jdk","slug":"jdk","permalink":"http://fangjian0423.github.io/categories/jdk/"}],"tags":[{"name":"jdk","slug":"jdk","permalink":"http://fangjian0423.github.io/tags/jdk/"},{"name":"set","slug":"set","permalink":"http://fangjian0423.github.io/tags/set/"}]},{"title":"jdk TreeMap工作原理分析","slug":"jdk_treemap","date":"2016-04-06T16:55:31.000Z","updated":"2016-05-10T14:59:25.000Z","comments":true,"path":"2016/04/07/jdk_treemap/","link":"","permalink":"http://fangjian0423.github.io/2016/04/07/jdk_treemap/","excerpt":"TreeMap是jdk中基于红黑树的一种map实现。HashMap底层是使用链表法解决冲突的哈希表，LinkedHashMap继承自HashMap，内部同样也是使用链表法解决冲突的哈希表，但是额外添加了一个双向链表用于处理元素的插入顺序或访问访问。\n既然TreeMap底层使用的是红黑树，首先先来简单了解一下红黑树的定义。\n红黑树是一棵平衡二叉查找树，同时还需要满足以下5个规则：\n\n每个节点只能是红色或者黑点\n根节点是黑点\n叶子节点(Nil节点，空节点)是黑色节点\n如果一个节点是红色节点，那么它的两个子节点必须是黑色节点(一条路径上不能出现相邻的两个红色节点)\n从任一节点到其每个叶子节点的所有路径都包含相同数目的黑色节点\n\n红黑树的这些特性决定了它的查询、插入、删除操作的时间复杂度均为O(log n)。","text":"TreeMap是jdk中基于红黑树的一种map实现。HashMap底层是使用链表法解决冲突的哈希表，LinkedHashMap继承自HashMap，内部同样也是使用链表法解决冲突的哈希表，但是额外添加了一个双向链表用于处理元素的插入顺序或访问访问。 既然TreeMap底层使用的是红黑树，首先先来简单了解一下红黑树的定义。 红黑树是一棵平衡二叉查找树，同时还需要满足以下5个规则： 每个节点只能是红色或者黑点 根节点是黑点 叶子节点(Nil节点，空节点)是黑色节点 如果一个节点是红色节点，那么它的两个子节点必须是黑色节点(一条路径上不能出现相邻的两个红色节点) 从任一节点到其每个叶子节点的所有路径都包含相同数目的黑色节点 红黑树的这些特性决定了它的查询、插入、删除操作的时间复杂度均为O(log n)。 一个TreeMap例子一段TreeMap代码： TreeMap&lt;Integer, String&gt; treeMap = new TreeMap&lt;Integer, String&gt;(); treeMap.put(1, &quot;语文&quot;); treeMap.put(2, &quot;数学&quot;); treeMap.put(3, &quot;英语&quot;); treeMap.put(4, &quot;政治&quot;); treeMap.put(5, &quot;物理&quot;); treeMap.put(6, &quot;化学&quot;); treeMap.put(7, &quot;生物&quot;); treeMap.put(8, &quot;体育&quot;); 执行过程： 从上面这个例子看到，红黑树添加新节点的时候可能会对节点进行旋转，以保证树的局部平衡。 TreeMap原理分析TreeMap内部类Entry表示红黑树中的节点： static final class Entry&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; { K key; // 关键字 V value; // 值 Entry&lt;K,V&gt; left; // 左节点 Entry&lt;K,V&gt; right; // 右节点 Entry&lt;K,V&gt; parent; // 父节点 boolean color = BLACK; // 颜色，默认为黑色 Entry(K key, V value, Entry&lt;K,V&gt; parent) { this.key = key; this.value = value; this.parent = parent; } ... } TreeMap的属性： private transient Entry&lt;K,V&gt; root; // 根节点 private transient int size = 0; // 节点个数 put操作红黑树新节点的添加一定是红色节点，添加完新的节点之后会进行旋转操作以保持红黑树的特性。 为什么新添加的节点一定是红色节点，如果添加的是黑色节点，那么就会导致根到叶子的路径上有一条路上，多一个额外的黑节点，这个是很难调整的；但是如果插入的是红色节点，只需要解决其父节点也为红色节点的这个冲突即可。 以N为新插入节点，P为其父节点，U为其父节点的兄弟节点，R为P和U的父亲节点进行分析。如果N的父节点为黑色节点，那直接添加新节点即可，没有产生冲突。如果出现P节点是红色节点，那便产生冲突，可以分为以下几种冲突： (1) P为红色节点，且U也为红色节点，P不论是R的左节点还是右节点 将P和U接口变成黑色节点，R节点变成红色节点。修改之后如果R节点的父节点也是红色节点，那么在R节点上执行相同操作，形成了一个递归过程。如果R节点是根节点的话，那么直接把R节点修改成黑色节点。 (2) P为红色节点，U为黑色节点或缺少，且N是P的右节点、P是R的左节点 或者 P为红色节点，U为黑色节点或缺少，且N是P的左节点、P是R的右节点 这两种情况分别对P进行左旋和右旋操作。操作结果就变成了冲突3。 (总结起来就是左右变左左，右左变右右) (3) P为红色节点，U为黑色节点或缺少，且N是P的左节点、P是R的左节点 或者 P为红色节点，U为黑色节点或缺少，且N是P的右节点、P是R的右节点 这两种情况分别对祖父R进行右旋和左旋操作。完美解决冲突。(总结起来就是左左祖右，右右祖左) 这3个新增节点的冲突处理方法了解之后，我们回过头来看本文一开始的例子中添加最后一个[8:体育]节点是如何处理冲突的： 接下来我们看TreeMap是如何实现新增节点并处理冲突的。 TreeMap对应的put方法： public V put(K key, V value) { Entry&lt;K,V&gt; t = root; if (t == null) { // 如果根节点是空的，说明是第一次插入数据 compare(key, key); root = new Entry&lt;&gt;(key, value, null); // 构造根节点，并赋值给属性root，默认颜色是黑色 size = 1; // 节点数 = 1 modCount++; return null; } int cmp; Entry&lt;K,V&gt; parent; // split comparator and comparable paths Comparator&lt;? super K&gt; cpr = comparator; if (cpr != null) { // 比较器存在 do { // 遍历寻找节点，关键字比节点小找左节点，比节点大的找右节点，直到找到那个叶子节点，会保存需要新构造节点的父节点到parent变量里 parent = t; cmp = cpr.compare(key, t.key); if (cmp &lt; 0) t = t.left; else if (cmp &gt; 0) t = t.right; else return t.setValue(value); // 关键字存在的话，直接用值覆盖原节点的关键字的值，并返回 } while (t != null); } else { // 比较器不存在 if (key == null) throw new NullPointerException(); @SuppressWarnings(&quot;unchecked&quot;) Comparable&lt;? super K&gt; k = (Comparable&lt;? super K&gt;) key; // 比较器不存在直接将关键字转换成比较器，如果关键字不是一个Comparable接口实现类，将会报错 do { // 遍历寻找节点，关键字比节点小找左节点，比节点大的找右节点，直到找到那个叶子节点，会保存需要新构造节点的父节点到parent变量里 parent = t; cmp = k.compareTo(t.key); if (cmp &lt; 0) t = t.left; else if (cmp &gt; 0) t = t.right; else return t.setValue(value); // 关键字存在的话，直接用值覆盖原节点的关键字的值，并返回 } while (t != null); } Entry&lt;K,V&gt; e = new Entry&lt;&gt;(key, value, parent); // 构造新的关键字节点 if (cmp &lt; 0) // 需要在左节点构造 parent.left = e; else // 需要在右节点构造 parent.right = e; fixAfterInsertion(e); // 插入节点之后，处理冲突以保持树符合红黑树的特性 size++; modCount++; return null; } fixAfterInsertion方法处理红黑树冲突实现如下： private void fixAfterInsertion(Entry&lt;K,V&gt; x) { x.color = RED; // 新增的节点一定是红色节点 while (x != null &amp;&amp; x != root &amp;&amp; x.parent.color == RED) { // P节点是红色节点并且N节点不是根节点的话一直循环 if (parentOf(x) == leftOf(parentOf(parentOf(x)))) { // P节点是R节点的左节点 Entry&lt;K,V&gt; y = rightOf(parentOf(parentOf(x))); // y就是U节点 if (colorOf(y) == RED) { // 如果U节点是红色节点，说明P和U这两个节点都是红色节点，满足冲突(1) setColor(parentOf(x), BLACK); // 冲突(1)解决方案 把P设置为黑色 setColor(y, BLACK); // 冲突(1)解决方案 把U设置为黑色 setColor(parentOf(parentOf(x)), RED); // 冲突(1)解决方案 把R设置为红色 x = parentOf(parentOf(x)); // 递归处理R节点 } else { // 如果U节点是黑色节点，满足冲突(2)或(3) if (x == rightOf(parentOf(x))) { // 如果N节点是P节点的右节点，满足冲突(2)的第一种情况 x = parentOf(x); rotateLeft(x); // P节点进行左旋操作 } // P节点左旋操作之后，满足了冲突(3)的第一种情况或者N一开始就是P节点的左节点，这本来就是冲突(3)的第一种情况 setColor(parentOf(x), BLACK); // P节点和R节点交换颜色，P节点变成黑色 setColor(parentOf(parentOf(x)), RED); // P节点和R节点交换颜色，R节点变成红色 rotateRight(parentOf(parentOf(x))); // R节点右旋操作 } } else { // P节点是R节点的右节点 Entry&lt;K,V&gt; y = leftOf(parentOf(parentOf(x))); // y就是U节点 if (colorOf(y) == RED) { // 如果U节点是红色节点，说明P和U这两个节点都是红色节点，满足冲突(1) setColor(parentOf(x), BLACK); // 冲突(1)解决方案 把P设置为黑色 setColor(y, BLACK); // 冲突(1)解决方案 把U设置为黑色 setColor(parentOf(parentOf(x)), RED); // 冲突(1)解决方案 把R设置为红色 x = parentOf(parentOf(x)); // 递归处理R节点 } else { // 如果U节点是黑色节点，满足冲突(2)或(3) if (x == leftOf(parentOf(x))) { // 如果N节点是P节点的左节点，满足冲突(2)的第二种情况 x = parentOf(x); rotateRight(x); // P节点右旋 } // P节点右旋操作之后，满足了冲突(3)的第二种情况或者N一开始就是P节点的右节点，这本来就是冲突(3)的第二种情况 setColor(parentOf(x), BLACK); // P节点和R节点交换颜色，P节点变成黑色 setColor(parentOf(parentOf(x)), RED); // P节点和R节点交换颜色，R节点变成红色 rotateLeft(parentOf(parentOf(x))); // R节点左旋操作 } } } root.color = BLACK; // 根节点是黑色节点 } fixAfterInsertion方法的代码跟之前分析的冲突解决方案一模一样。 get操作红黑树的get操作相比add操作简单不少，只需要比较关键字即可，要查找的关键字比节点关键字要小的话找左节点，否则找右节点，一直递归操作，直到找到或找不到。代码如下： final Entry&lt;K,V&gt; getEntry(Object key) { if (comparator != null) return getEntryUsingComparator(key); if (key == null) throw new NullPointerException(); @SuppressWarnings(&quot;unchecked&quot;) Comparable&lt;? super K&gt; k = (Comparable&lt;? super K&gt;) key; Entry&lt;K,V&gt; p = root; while (p != null) { int cmp = k.compareTo(p.key); // 得到比较值 if (cmp &lt; 0) // 小的话找左节点 p = p.left; else if (cmp &gt; 0) // 大的话找右节点 p = p.right; else return p; } return null; } remove操作红黑树的删除节点跟添加节点一样，比较复杂，删除节点也会让树不符合红黑树的特性，也需要解决这些冲突。 删除操作分为2个步骤： 将红黑树当作一颗二叉查找树，将节点删除 通过”旋转和重新着色”等一系列来修正该树，使之重新成为一棵红黑树 步骤1的删除操作可分为几种情况： 删除节点没有儿子：直接删除该节点 删除节点有1个儿子：删除该节点，并用该节点的儿子节点顶替它的位置 删除节点有2个儿子：可以转成成删除节点只有1个儿子的情况，跟二叉查找树一样，找出节点的右子树的最小元素(或者左子树的最大元素，这种节点称为后继节点)，并把它的值转移到删除节点，然后删除这个后继节点。这个后继节点最多只有1个子节点(如果有2个子节点，说明还能找出右子树更小的值)，所以这样删除2个儿子的节点就演变成了删除没有儿子的节点和删除只有1个儿子的节点的情况 删除节点之后要考虑的问题就是红黑树失衡的调整问题。 步骤2遇到的调整问题只有2种情况： 删除节点没有儿子节点 删除节点只有1个儿子节点 删除节点没有儿子节点的话，直接把节点删除即可。如果节点是黑色节点，需要进行平衡性调整，否则，不用调整平衡性。这里的平衡性调整跟删除只有1个儿子节点一样，删除只有1个儿子的调整会先把节点删除，然后儿子节点顶上来，顶上来之后再进行平衡性调整。而删除没有儿子节点的节点的话，先进行调整，调整之后再把这个节点删除。他们的调整策略是一样的，只不过没有儿子节点的情况下先进行调整，然后再删除节点，而有儿子节点的情况下，先把节点删除，删除之后儿子节点顶上来，然后再做平衡性调整。 删除节点只有1个儿子节点还分几种情况： 如果被删除的节点是红色节点，那说明它的父节点是黑色节点，儿子节点也是黑色节点，那么删除这个节点就不会影响红黑树的属性，直接使用它的黑色子节点代替它即可 如果被删除的节点是黑色节点，而它的儿子节点是红色节点。删除这个黑色节点之后，它的红色儿子节点顶替之后，会破坏性质5，只需要把儿子节点重绘为黑色节点即可，这样原先通过黑色删除节点的所有路径被现在的重绘后的儿子节点所代替 如果被删除的节点是黑色节点，而它的儿子节点也是黑色节点。这是一种复杂的情况，因为路径路过被删除节点的黑色节点路径少了1个，导致违反了性质5，所以需要对红黑树进行平衡调整。可分为以下几种情况进行调整： 以N为删除节点的儿子节点(删除之后，处于新的位置上)，它的兄弟节点为S，它们的父节点为P，Sl和Sr为S节点的左右子节点为例，进行讲解，其中N是父节点P的左子节点，如果N是父节点P的右子节点，做对称处理。 3.1：N是新的根节点。这种情况下不用做任何处理，因为原先的节点也是一个根节点，相当于所有的路径都需要经过这个根节点，删除之后没有什么影响，而且新根也是黑色节点，符合所有特性，不需要进行调整 3.2: S节点是红色节点，那么P节点，Sl，Sr节点是黑色节点。在这种情况下，对P节点进行左选操作并且交换P和S的颜色。完成这2个操作之后，所有路径上的黑色节点没有变化，但是N节点有了一个黑色兄弟节点Sl和一个红色的父亲节点P，左子树删除节点后还有存在着少1个黑色节点路径的问题。接下来按照N节点新的位置(兄弟节点S是个黑色节点，父节点P是个红色节点)进行3.4、3.5或3.6情况处理 3.3：N的父亲节点P、兄弟节点S，还有S的两个子节点Sl，Sr均为黑色节点。在这种情况下，重绘S为红色。重绘之后路过S节点这边的路径跟N节点一样也少了一个黑色节点，但是出现了另外一个问题：不经过P节点的路径还是少了一个黑色节点。 接下来，要调整以P作为N递归调整树 3.4：S和S的儿子节点Sl、Sr为黑色节点，但是N的父亲节点P为红色节点。在这种情况下，交换N的兄弟S与父亲P的颜色，颜色交换之后左子树多了1个黑色节点路径，刚好填补了左子树删除节点的少一个黑色节点路径的问题，而右子树的黑色路径没有改变，解决平衡问题 3.5：S是黑色节点，S的左儿子节点Sl是红色，S的右儿子节点Sr是黑色节点。在这种情况下，在S上做右旋操作交换S和它新父亲的颜色。操作之后，左子树的黑色节点路径和右子树的黑色节点路径没有改变。但是现在N节点有了一个黑色的兄弟节点，黑色的兄弟节点有个红色的右儿子节点，满足了3.6的情况，按照3.6处理 3.6：S是黑色节点，S的右儿子节点Sr为红色节点，S的左儿子Sl是黑色节点，P是红色或黑色节点。在这种情况下，N的父亲P做左旋操作，交换N父亲P和S的颜色，S的右子节点Sr变成黑色。这样操作以后，左子树的黑色路径+1，补了删除节点的黑色路径，右子树黑色路径不变，解决平衡问题 了解了删除节点之后的平衡性调整之后，我们回过头来看本文一开始的例子进行节点删除的操作过程： TreeMap删除方法如下： private void deleteEntry(Entry&lt;K,V&gt; p) { modCount++; size--; // 节点个数 -1 if (p.left != null &amp;&amp; p.right != null) { // 如果要删除的节点有2个子节点，去找后继节点 Entry&lt;K,V&gt; s = successor(p); // 找出后继节点 p.key = s.key; // 后继节点的关键字赋值给删除节点 p.value = s.value; // 后继节点的值赋值给删除节点 p = s; // 改为删除后继节点 } Entry&lt;K,V&gt; replacement = (p.left != null ? p.left : p.right); // 找出替代节点，左子树存在的话使用左子树，否则使用右子树。这个替代节点就是被删除节点的左子节点或右子节点 if (replacement != null) { // 替代节点如果存在的话 replacement.parent = p.parent; // 删除要删除的节点 // 有子节点的删除节点先删除节点，然后再做平衡性调整 if (p.parent == null) // 如果被删除节点的父节点为空，说明被删除节点是根节点 root = replacement; // 用替代节点替代根节点 else if (p == p.parent.left) p.parent.left = replacement; // 用替代节点替代原先被删除的节点 else p.parent.right = replacement; // 用替代节点替代原先被删除的节点 p.left = p.right = p.parent = null; if (p.color == BLACK) // 被删除节点如果是黑色节点，需要进行平衡性调整 fixAfterDeletion(replacement); } else if (p.parent == null) { // 如果被删除节点的父节点为空，说明被删除节点是根节点 root = null; // 根节点的删除直接把根节点置空即可 } else { // 如果要删除的节点没有子节点 if (p.color == BLACK) // 如果要删除的节点是个黑色节点，需要进行平衡性调整 fixAfterDeletion(p); // 调整平衡性，没有子节点的删除节点先进行平衡性调整 if (p.parent != null) { // 没有子节点的删除节点平衡性调整完毕之后再进行节点删除 if (p == p.parent.left) p.parent.left = null; else if (p == p.parent.right) p.parent.right = null; p.parent = null; } } } // 删除节点后的平衡性调整，对应之前分析的节点昵称，N、S、P、Sl、Sr private void fixAfterDeletion(Entry&lt;K,V&gt; x) { while (x != root &amp;&amp; colorOf(x) == BLACK) { // N节点是黑色节点并且不是根节点就一直循环 if (x == leftOf(parentOf(x))) { // 如果N是P的左子节点 Entry&lt;K,V&gt; sib = rightOf(parentOf(x)); // sib就是N节点的兄弟节点S if (colorOf(sib) == RED) { // 如果S节点是红色节点，满足删除冲突3.2，对P节点进行左旋操作并交换P和S的颜色 // 交换P和S的颜色，S原先为红色，P原先为黑色(2个红色节点不能相连) setColor(sib, BLACK); // S节点从红色变成黑色 setColor(parentOf(x), RED); // P节点从黑色变成红色 rotateLeft(parentOf(x)); // 删除冲突3.2中P节点进行左旋 sib = rightOf(parentOf(x)); // 左旋之后N节点有了一个黑色的兄弟节点和红色的父亲节点，S节点重新赋值成N节点现在的兄弟节点。接下来按照删除冲突3.4、3.5、3.6处理 } // 执行到这里S节点一定是黑色节点，如果是红色节点，会按照冲突3.2交换成黑色节点 // 如果S节点的左右子节点Sl、Sr均为黑色节点并且S节点也为黑色节点 if (colorOf(leftOf(sib)) == BLACK &amp;&amp; colorOf(rightOf(sib)) == BLACK) { // 按照删除冲突3.3和3.4进行处理 // 如果是冲突3.3，说明P节点也是黑色节点 // 如果是冲突3.4，说明P节点是红色节点，P节点和S节点需要交换颜色 // 3.3和3.4冲突的处理结果S节点都为红色节点，但是3.4冲突处理完毕之后直接结束，而3.3冲突处理完毕之后继续调整 setColor(sib, RED); // S节点变成红色节点，如果是3.4冲突需要交换颜色，N节点的颜色交换在跳出循环进行 x = parentOf(x); // N节点重新赋值成N节点的父节点P之后继续递归处理 } else { // S节点的2个子节点Sl，Sr中存在红色节点 if (colorOf(rightOf(sib)) == BLACK) { // 如果S节点的右子节点Sr为黑色节点，Sl为红色节点[Sl如果为黑色节点的话就在上一个if逻辑里处理了]，满足删除冲突3.5 // 删除冲突3.5，对S节点做右旋操作，交换S和Sl的颜色，S变成红色节点，Sl变成黑色节点 setColor(leftOf(sib), BLACK); // Sl节点变成黑色节点 setColor(sib, RED); // S节点变成红色节点 rotateRight(sib); // S节点进行右旋操作 sib = rightOf(parentOf(x)); // S节点赋值现在N节点的兄弟节点 } // 删除冲突3.5处理之后变成了删除冲突3.6或者一开始就是删除冲突3.6 // 删除冲突3.6，P节点做左旋操作，P节点和S接口交换颜色，Sr节点变成黑色 setColor(sib, colorOf(parentOf(x))); // S节点颜色变成P节点颜色，红色 setColor(parentOf(x), BLACK); // P节点变成S节点颜色，也就是黑色 setColor(rightOf(sib), BLACK); // Sr节点变成黑色 rotateLeft(parentOf(x)); // P节点做左旋操作 x = root; // 准备跳出循环 } } else { // 如果N是P的右子节点，处理过程跟N是P的左子节点一样，左右对换即可 Entry&lt;K,V&gt; sib = leftOf(parentOf(x)); if (colorOf(sib) == RED) { setColor(sib, BLACK); setColor(parentOf(x), RED); rotateRight(parentOf(x)); sib = leftOf(parentOf(x)); } if (colorOf(rightOf(sib)) == BLACK &amp;&amp; colorOf(leftOf(sib)) == BLACK) { setColor(sib, RED); x = parentOf(x); } else { if (colorOf(leftOf(sib)) == BLACK) { setColor(rightOf(sib), BLACK); setColor(sib, RED); rotateLeft(sib); sib = leftOf(parentOf(x)); } setColor(sib, colorOf(parentOf(x))); setColor(parentOf(x), BLACK); setColor(leftOf(sib), BLACK); rotateRight(parentOf(x)); x = root; } } } setColor(x, BLACK); // 删除冲突3.4循环调出来之后N节点颜色设置为黑色 或者 删除节点只有1个红色子节点的时候，将顶上来的红色节点设置为黑色 } 参考资料http://dongxicheng.org/structure/red-black-tree/ http://blog.csdn.net/chenssy/article/details/26668941 http://zh.wikipedia.org/wiki/%E7%BA%A2%E9%BB%91%E6%A0%91 http://www.cnblogs.com/fanzhidongyzby/p/3187912.html","raw":null,"content":null,"categories":[{"name":"jdk","slug":"jdk","permalink":"http://fangjian0423.github.io/categories/jdk/"}],"tags":[{"name":"jdk","slug":"jdk","permalink":"http://fangjian0423.github.io/tags/jdk/"},{"name":"map","slug":"map","permalink":"http://fangjian0423.github.io/tags/map/"}]},{"title":"jdk ArrayDeque工作原理分析","slug":"jdk_arraydeque","date":"2016-04-02T16:43:59.000Z","updated":"2017-01-05T06:30:04.000Z","comments":true,"path":"2016/04/03/jdk_arraydeque/","link":"","permalink":"http://fangjian0423.github.io/2016/04/03/jdk_arraydeque/","excerpt":"ArrayDeque双向队列是jdk中列表的一种实现，支持元素在头和尾这两端进行插入和删除操作。\nDeque接口(双向队列)的两个主要实现类是ArrayDeque和LinkedList。\n其中ArrayDeque底层使用循环数组实现双向队列，而LinkedList是使用链表实现，之前在jdk LinkedList工作原理分析这篇文章中，已经分析过了LinkedList的实现原理，本文分析ArrayDeque的实现原理。","text":"ArrayDeque双向队列是jdk中列表的一种实现，支持元素在头和尾这两端进行插入和删除操作。 Deque接口(双向队列)的两个主要实现类是ArrayDeque和LinkedList。 其中ArrayDeque底层使用循环数组实现双向队列，而LinkedList是使用链表实现，之前在jdk LinkedList工作原理分析这篇文章中，已经分析过了LinkedList的实现原理，本文分析ArrayDeque的实现原理。 一个ArrayDeque例子一段ArrayDeque代码： ArrayDeque&lt;String&gt; arrayDeque = new ArrayDeque&lt;String&gt;(4); arrayDeque.add(&quot;1&quot;); arrayDeque.add(&quot;2&quot;); arrayDeque.add(&quot;3&quot;); arrayDeque.addFirst(&quot;0.5&quot;); arrayDeque.add(&quot;4&quot;); ArrayDeque内部使用的循环数组的容量，当首次进行初始化的时候，最小容量为8，如果超过8，扩大成2的幂。 // 调用带有容量参数的构造函数后，数组初始化过程 private void allocateElements(int numElements) { int initialCapacity = MIN_INITIAL_CAPACITY; // 最小容量为8 if (numElements &gt;= initialCapacity) { // 如果要分配的容量大于等于8，扩大成2的幂；否则使用最小容量8 initialCapacity = numElements; initialCapacity |= (initialCapacity &gt;&gt;&gt; 1); initialCapacity |= (initialCapacity &gt;&gt;&gt; 2); initialCapacity |= (initialCapacity &gt;&gt;&gt; 4); initialCapacity |= (initialCapacity &gt;&gt;&gt; 8); initialCapacity |= (initialCapacity &gt;&gt;&gt; 16); initialCapacity++; if (initialCapacity &lt; 0) initialCapacity &gt;&gt;&gt;= 1; } elements = new Object[initialCapacity]; // 构造数组 } 上面例子构造容量为4的数组，但是由于最小容量为8，所以构造的数组的容量是8。 执行过程如下： ArrayDeque原理分析ArrayDeque使用的是循环数组，内部有3个属性，分别是： Object[] elements; // 数组 int head; // 头索引 int tail; // 尾索引 add操作上面例子使用的add方法，其实内部使用了addLast方法，addLast也就添加数据到双向队列尾端： public void addLast(E e) { if (e == null) throw new NullPointerException(); elements[tail] = e; // 根据尾索引，添加到尾端 if ( (tail = (tail + 1) &amp; (elements.length - 1)) == head) // 尾索引+1，如果尾索引和头索引重复了，说明数组满了，进行扩容 doubleCapacity(); } addFirst方法跟addLast方法相反，添加数据到双向队列头端： public void addFirst(E e) { if (e == null) throw new NullPointerException(); elements[head = (head - 1) &amp; (elements.length - 1)] = e; // 根据头索引，添加到头端，头索引-1 if (head == tail) // 如果头索引和尾索引重复了，说明数组满了，进行扩容 doubleCapacity(); } remove操作remove操作分别removeFirst和removeLast，removeLast代码如下： public E removeLast() { E x = pollLast(); // 调用pollLast方法 if (x == null) throw new NoSuchElementException(); return x; } public E pollLast() { int t = (tail - 1) &amp; (elements.length - 1); // 尾索引 -1 @SuppressWarnings(&quot;unchecked&quot;) E result = (E) elements[t]; // 根据尾索引，得到尾元素 if (result == null) return null; elements[t] = null; // 尾元素置空 tail = t; return result; } removeFirst方法原理一样，remove头元素。 头索引 +1 扩容ArrayDeque的扩容会把数组容量扩大2倍，同时还会重置头索引和尾索引，头索引置为0，尾索引置为原容量的值。 比如容量为8，扩容为16，头索引变成0，尾索引变成8。 扩容代码如下： private void doubleCapacity() { assert head == tail; int p = head; int n = elements.length; int r = n - p; int newCapacity = n &lt;&lt; 1; if (newCapacity &lt; 0) throw new IllegalStateException(&quot;Sorry, deque too big&quot;); Object[] a = new Object[newCapacity]; System.arraycopy(elements, p, a, 0, r); System.arraycopy(elements, 0, a, r, p); elements = a; head = 0; // 头索引重置 tail = n; // 尾索引重置 } 其他Deque接口同时还附带了Stack的功能。 ArrayDeque&lt;String&gt; stack = new ArrayDeque&lt;String&gt;(4); stack.push(&quot;1&quot;); stack.push(&quot;2&quot;); stack.push(&quot;3&quot;); String pop = stack.pop(); // 3 push方法内部调用addFirst方法，pop方法内部调用removeFirst方法。 注意点 ArrayDeque是一个使用循环数组实现的双向队列，LinkedList也是一个双向队列，不过它的底层实现是使用链表 ArrayDeque的扩容会把数组容量扩大2倍，同时还会重置头索引和尾索引 Deque双向队列接口同时也实现了Stack接口，可以把Deque当成Stack使用，它的速度比java.util.Stack要快，因为Stack底层操作数据会加锁，而Deque不会加锁 ArrayDeque不是一个线程安全的类","raw":null,"content":null,"categories":[{"name":"jdk","slug":"jdk","permalink":"http://fangjian0423.github.io/categories/jdk/"}],"tags":[{"name":"jdk","slug":"jdk","permalink":"http://fangjian0423.github.io/tags/jdk/"},{"name":"collection","slug":"collection","permalink":"http://fangjian0423.github.io/tags/collection/"}]},{"title":"jdk HashSet, LinkedHashSet工作原理分析","slug":"jdk_hashset_linkedhashset","date":"2016-03-29T16:46:33.000Z","updated":"2016-05-10T14:59:25.000Z","comments":true,"path":"2016/03/30/jdk_hashset_linkedhashset/","link":"","permalink":"http://fangjian0423.github.io/2016/03/30/jdk_hashset_linkedhashset/","excerpt":"Set是一个没有包括重复数据的集合，跟List一样，他们都继承自Collection。\nJava中的Set接口最主要的实现类就是HashSet和LinkedHashSet。","text":"Set是一个没有包括重复数据的集合，跟List一样，他们都继承自Collection。 Java中的Set接口最主要的实现类就是HashSet和LinkedHashSet。 HashSet原理分析首先看下HashSet的属性。 HashSet内部有个HashMap属性和一个对象属性： private transient HashMap&lt;E,Object&gt; map; // HashSet内部使用HashMap进行处理，由于Set只需要键值对中的键，而不需要值，所有的值都用这个对象 private static final Object PRESENT = new Object(); HashSet的构造函数中也提供了HashMap的capacity，loadFactor这些参数。 add方法调用HashMap的put操作完成Set的add操作。 public boolean add(E e) { return map.put(e, PRESENT)==null; // HashMap put成功返回true，否则false } HashMap相关的put操作在之前的博客中已经介绍过了，这里就不分析了。 boolean remove(Object o)调用HashMap的remove操作完成。 public boolean remove(Object o) { return map.remove(o)==PRESENT; // 对应的节点移除成功返回true，否则false } 一个HashSet例子Set&lt;String&gt; set = new HashSet&lt;String&gt;(5); set.add(&quot;java&quot;); set.add(&quot;golang&quot;); set.add(&quot;python&quot;); set.add(&quot;ruby&quot;); set.add(&quot;scala&quot;); set.add(&quot;c&quot;); for(String str : set) { System.out.println(str); } 这个例子中set中的HashMap内部结构如下图所示： HashSet总结 HashSet内部使用HashMap，HashSet集合内部所有的操作基本上都是基于HashMap完成的 HashSet中的元素是无序的，这是因为它内部使用HashMap进行存储，而HashMap添加键值对的时候是根据hash函数得到数组的下标的 LinkedHashSet原理分析LinkedHashSet继承自HashSet，它的构造函数会调用父类HashSet的构造函数： public LinkedHashSet(int initialCapacity, float loadFactor) { super(initialCapacity, loadFactor, true); } HashSet(int initialCapacity, float loadFactor, boolean dummy) { // map使用LinkedHashMap构造，LinkedHashMap是HashMap的子类，accessOrder为false，即使用插入顺序 map = new LinkedHashMap&lt;&gt;(initialCapacity, loadFactor); } 一个LinkedHashSet例子Set&lt;String&gt; set = new LinkedHashSet&lt;String&gt;(5); set.add(&quot;java&quot;); set.add(&quot;golang&quot;); set.add(&quot;python&quot;); set.add(&quot;ruby&quot;); set.add(&quot;scala&quot;); for(String str : set) { System.out.println(str); } 这个例子中set中的LinkedHashMap内部结构如下图所示： LinkedHashSet总结 LinkedHashSet继自HashSet，但是内部的map是使用LinkedHashMap构造的，并且accessOrder为false，使用查询顺序。所以LinkedHashSet遍历的顺序就是插入顺序。","raw":null,"content":null,"categories":[{"name":"jdk","slug":"jdk","permalink":"http://fangjian0423.github.io/categories/jdk/"}],"tags":[{"name":"jdk","slug":"jdk","permalink":"http://fangjian0423.github.io/tags/jdk/"},{"name":"set","slug":"set","permalink":"http://fangjian0423.github.io/tags/set/"}]},{"title":"jdk LinkedHashMap工作原理分析","slug":"jdk_linkedhashmap","date":"2016-03-29T11:23:23.000Z","updated":"2016-05-10T14:59:25.000Z","comments":true,"path":"2016/03/29/jdk_linkedhashmap/","link":"","permalink":"http://fangjian0423.github.io/2016/03/29/jdk_linkedhashmap/","excerpt":"LinkedHashMap是一种会记录插入顺序的Map，内部维护着一个accessOrder属性，用于表示map数据的迭代顺序是基于访问顺序还是插入顺序。","text":"LinkedHashMap是一种会记录插入顺序的Map，内部维护着一个accessOrder属性，用于表示map数据的迭代顺序是基于访问顺序还是插入顺序。 LinkedHashMap原理分析首先是LinkedHashMap的定义： public class LinkedHashMap&lt;K,V&gt; extends HashMap&lt;K,V&gt; implements Map&lt;K,V&gt; LinkedHashMap继承HashMap，实现Map接口，所以它的结构跟HashMap是一样的，使用链表法解决哈希冲突的哈希表，基本操作跟HashMap也是一样的，就是多了一点额外的步骤用于处理链表。 LinkedHashMap有个内部类Entry，这个Entry就是链表中的节点，继承自HashMap.Node，多出了2个属性before和after，所以LinkedHashMap内部链表的节点是双向的，代码如下： static class Entry&lt;K,V&gt; extends HashMap.Node&lt;K,V&gt; { Entry&lt;K,V&gt; before, after; Entry(int hash, K key, V value, Node&lt;K,V&gt; next) { super(hash, key, value, next); } } 另外LinkedHashMap还有两个重要的属性head，tail，这2个属性用于存储插入的节点，形成一个双向链表： // 首节点 transient LinkedHashMap.Entry&lt;K,V&gt; head; // 尾节点 transient LinkedHashMap.Entry&lt;K,V&gt; tail; 跟HashMap一样，下面这个例子对应的LinkedHashMap结构图示如下所示，accessOrder为false，使用插入顺序： Map&lt;String, Integer&gt; map = new LinkedHashMap&lt;String, Integer&gt;(5); map.put(&quot;java&quot;, 1); map.put(&quot;golang&quot;, 2); map.put(&quot;python&quot;, 3); map.put(&quot;ruby&quot;, 4); map.put(&quot;scala&quot;, 5); put操作LinkedHashMap没有覆盖HashMap的put方法，所以put操作跟HashMap是一样的。但是它覆盖了newNode方法，也就是说构造新节点的时候，LinkedHashMap跟HashMap是不一样的： Node&lt;K,V&gt; newNode(int hash, K key, V value, Node&lt;K,V&gt; e) { // 使用Entry双向链表构造节点，而不是HashMap的Node单向链表 LinkedHashMap.Entry&lt;K,V&gt; p = new LinkedHashMap.Entry&lt;K,V&gt;(hash, key, value, e); linkNodeLast(p); // 更新双向链表，这一操作在HashMap里面是没有的 return p; } 另外，LinkedHashMap重写了afterNodeInsertion这个钩子方法，在put一个关键字不存在的节点之后会调用这个方法： void afterNodeInsertion(boolean evict) { // possibly remove eldest LinkedHashMap.Entry&lt;K,V&gt; first; // removeEldestEntry方法LinkedHashMap永远返回false，一些使用缓存策略的Map会覆盖这个方法，比如jackson的LRUMap，会移除最老的节点，也就是首节点 if (evict &amp;&amp; (first = head) != null &amp;&amp; removeEldestEntry(first)) { K key = first.key; removeNode(hash(key), key, null, false, true); } } put操作如果关键字已经存在，会调用afterNodeAccess这个钩子方法： void afterNodeAccess(Node&lt;K,V&gt; e) { // move node to last LinkedHashMap.Entry&lt;K,V&gt; last; if (accessOrder &amp;&amp; (last = tail) != e) { // 如果使用访问顺序并且访问的不是尾节点 LinkedHashMap.Entry&lt;K,V&gt; p = (LinkedHashMap.Entry&lt;K,V&gt;)e, b = p.before, a = p.after; p.after = null; if (b == null) head = a; else b.after = a; if (a != null) a.before = b; else last = b; if (last == null) head = p; else { p.before = last; last.after = p; } tail = p; ++modCount; } } get操作LinkedHashMap复写了get方法： public V get(Object key) { Node&lt;K,V&gt; e; if ((e = getNode(hash(key), key)) == null) return null; if (accessOrder) // 使用访问顺序的话，调用afterNodeAccess方法 afterNodeAccess(e); return e.value; } remove操作LinkedHashMap的remove方法没有复写HashMap的remove方法，但是同样实现了afterNodeRemoval这个钩子方法： // 更新双向链表 void afterNodeRemoval(Node&lt;K,V&gt; e) { // unlink LinkedHashMap.Entry&lt;K,V&gt; p = (LinkedHashMap.Entry&lt;K,V&gt;)e, b = p.before, a = p.after; p.before = p.after = null; if (b == null) head = a; else b.after = a; if (a == null) tail = b; else a.before = b; } accessOrder属性分析LinkedHashMap默认情况下，accessOrder属性为false，也就是使用插入顺序，这个插入顺序是根据LinkedHashMap内部的一个双向链表实现的。如果accessOrder为true，也就是使用访问顺序，那么afterNodeAccess这个钩子方法内部的逻辑会被执行，将会修改双向链表的结构，再来看一下这个方法的具体逻辑： void afterNodeAccess(Node&lt;K,V&gt; e) { // move node to last LinkedHashMap.Entry&lt;K,V&gt; last; if (accessOrder &amp;&amp; (last = tail) != e) { // 使用访问顺序，把节点移动到双向链表的最后面，如果已经在最后面了，不需要进行移动 LinkedHashMap.Entry&lt;K,V&gt; p = (LinkedHashMap.Entry&lt;K,V&gt;)e, b = p.before, a = p.after; p.after = null; if (b == null) head = a; // 特殊情况，处理头节点 else b.after = a; // 节点处理 if (a != null) a.before = b; // 节点处理 else last = b; // 特殊情况，处理尾节点 if (last == null) head = p; else { p.before = last; // 尾节点处理 last.after = p; } tail = p; ++modCount; } } afterNodeAccess在使用get方法或者put方法遇到关键字已经存在的情况下，会被触发，一个例子如下： Map&lt;String, Integer&gt; map = new LinkedHashMap&lt;String, Integer&gt;(5, 0.75f, true); map.put(&quot;java&quot;, 1); map.put(&quot;golang&quot;, 2); map.put(&quot;python&quot;, 3); map.put(&quot;ruby&quot;, 4); map.put(&quot;scala&quot;, 5); System.out.println(map.get(&quot;ruby&quot;)); 上面这段代码，LinkedHashMap的accessOrder属性为true，使用访问顺序，最后调用了get方法，触发afterNodeAccess方法，修改双向链表，效果如下： 注意点LinkedHashMap使用访问顺序并且进行遍历的时候，如果使用如下代码，会发生ConcurrentModificationException异常： for(String str : map.keySet()) { System.out.println(map.get(str)); } 不应该这么使用，而是应该直接读取value： for(Integer it : map.values()) { System.out.println(it); } 具体可以参考stackoverflow上的这篇帖子。 总结 LinkedHashMap也是一种使用拉链式哈希表的数据结构，除了哈希表，它内部还维护着一个双向链表，用于处理访问顺序和插入顺序的问题 LinkedHashMap继承自HashMap，大多数的方法都是跟HashMap一样的，不过覆盖了一些方法","raw":null,"content":null,"categories":[{"name":"jdk","slug":"jdk","permalink":"http://fangjian0423.github.io/categories/jdk/"}],"tags":[{"name":"jdk","slug":"jdk","permalink":"http://fangjian0423.github.io/tags/jdk/"},{"name":"map","slug":"map","permalink":"http://fangjian0423.github.io/tags/map/"}]},{"title":"jdk HashMap工作原理分析","slug":"jdk_hashmap","date":"2016-03-28T17:49:58.000Z","updated":"2016-05-10T14:59:25.000Z","comments":true,"path":"2016/03/29/jdk_hashmap/","link":"","permalink":"http://fangjian0423.github.io/2016/03/29/jdk_hashmap/","excerpt":"Map是一个映射键和值的对象。类似于Python中的字典。\nHashMap为什么会出现呢?\n因为数组这种数据结构，虽然遍历简单，但是插入和删除操作复杂，需要移动数组内部的元素；链表这种数据结构，插入和删除操作简单，但是查找复杂，只能一个一个地遍历。\n有没有一种新的数据结构，插入数据简单，同时查找也简单？ 这个时候就出现了哈希表这种数据结构。 这是一种折中的方式，插入没链表快，查询没数组快。\nwiki上就是这么定义哈希表的：\n散列表（Hash table，也叫哈希表），是根据关键字（Key value）而直接访问在内存存储位置的数据结构。也就是说，它通过计算一个关于键值的函数，将所需查询的数据映射到表中一个位置来访问记录，这加快了查找速度。这个映射函数称做散列函数，存放记录的数组称做散列表。","text":"Map是一个映射键和值的对象。类似于Python中的字典。 HashMap为什么会出现呢? 因为数组这种数据结构，虽然遍历简单，但是插入和删除操作复杂，需要移动数组内部的元素；链表这种数据结构，插入和删除操作简单，但是查找复杂，只能一个一个地遍历。 有没有一种新的数据结构，插入数据简单，同时查找也简单？ 这个时候就出现了哈希表这种数据结构。 这是一种折中的方式，插入没链表快，查询没数组快。 wiki上就是这么定义哈希表的： 散列表（Hash table，也叫哈希表），是根据关键字（Key value）而直接访问在内存存储位置的数据结构。也就是说，它通过计算一个关于键值的函数，将所需查询的数据映射到表中一个位置来访问记录，这加快了查找速度。这个映射函数称做散列函数，存放记录的数组称做散列表。 有几个概念要解释一下： 如果有1个关键字为k，它是通过一种函数f(k)得到散列表的地址，然后把值放到这个地址上。这个函数f就称为散列函数，也叫哈希函数。 对于不同的关键字，得到了同一地址，即k1 != k2，但是f(k1) = f(k2)。这种现象称为冲突， 若对于关键字集合中的任一个关键字，经散列函数映象到地址集合中任何一个地址的概率是相等的，则称此类散列函数为均匀散列函数 散列函数有好几种实现，分别有直接定址法、随机数法、除留余数法等，在wiki散列表上都有介绍。 散列表的冲突解决方法，也有好几种，有开放定址法、单独链表法、再散列等。 Java中的HashMap采用的冲突解决方法是使用单独链表法，如下图所示： HashMap原理分析HashMap是jdk中Map接口的实现类之一，是一个散列表的实现。 HashMap中的key和value都可以为null，且它的方法都没有synchronized。 其他方法的实现大部分跟HashTable一致。HashTable的相关源码不在这里介绍，基本上跟HashTable一致。 HashMap有个内部静态类Node，这个Node就是为了解决冲突而设计的链表中的节点的概念。它有4个属性，hash表示哈希地址，key表示关键字，value表示值, next表示这个节点的下一个节点，是一个单项链表： static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; { final int hash; final K key; V value; Node&lt;K,V&gt; next; Node(int hash, K key, V value, Node&lt;K,V&gt; next) { this.hash = hash; this.key = key; this.value = value; this.next = next; } ... } 在分析HashMap源码之前，先看一个HashMap使用例子Map&lt;String, Integer&gt; map = new HashMap&lt;String, Integer&gt;(5); map.put(&quot;java&quot;, 1); map.put(&quot;golang&quot;, 2); map.put(&quot;python&quot;, 3); map.put(&quot;ruby&quot;, 4); map.put(&quot;scala&quot;, 5); 上面这段代码执行之后会生成下面这张哈希表。 至于为什么会生成这样的哈希表，会在后面分析源码中讲解。 HashMap的属性HashMap的几个重要的属性: transient Node&lt;K,V&gt;[] table; // 哈希表数组 transient int size; // 键值对个数 int threshold; // 阀值。 值 = 容量 * 加载因子。默认值为12(16(默认容量) * 0.75(默认加载因子))。当哈希表中的键值对个数超过该值时，会进行扩容 final float loadFactor; // 加载因子，默认是0.75 有2个重要的特性影响着HashMap的性能，分别是capacity(容量)和load factor(加载因子)。 其中capacity表示哈希表bucket的数量，HashMap的默认值是16。load factor加载因子表示当一个map填满了达到这个比例之后的bucket时候，和ArrayList一样，将会创建原来HashMap大小的两倍的bucket数组，来重新调整map的大小，并将原来的对象放入新的bucket数组中。这个过程也叫做重哈希。默认的load factor为0.75 。 HashMap的操作分析一下HashMapput键值对的过程，是如何找到bucket的，遇到哈希冲突的时候是如何使用链表法的。 put操作public V put(K key, V value) { // 第一个参数就是关键字key的哈希值 return putVal(hash(key), key, value, false, true); } final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) { Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; // 哈希表是空的话，重新构建，进行扩容 if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); // 没有hash冲突的话，直接在对应位置上构造一个新的节点即可 else { // 如果哈希表当前位置上已经有节点的话，说明有hash冲突 Node&lt;K,V&gt; e; K k; // 关键字跟哈希表上的首个节点济宁比较 if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; // 如果使用的是红黑树，用红黑树的方式进行处理 else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else { // 跟链表进行比较 for (int binCount = 0; ; ++binCount) { if ((e = p.next) == null) { // 一直遍历链表，直到找到最后一个 p.next = newNode(hash, key, value, null); // 构造链表上的新节点 if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; } if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; } } if (e != null) { // 如果找到了节点，说明关键字相同，进行覆盖操作，直接返回旧的关键字的值 V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; } } ++modCount; if (++size &gt; threshold) // 如果目前键值对个数已经超过阀值，重新构建 resize(); afterNodeInsertion(evict); // 节点插入以后的钩子方法 return null; } get操作get操作关键点就是怎么在哈希表上取数据，理解了put操作之后，get方法很容易理解了： public V get(Object key) { Node&lt;K,V&gt; e; return (e = getNode(hash(key), key)) == null ? null : e.value; } getNode方法就说明了如何取数据： final Node&lt;K,V&gt; getNode(int hash, Object key) { Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k; if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (first = tab[(n - 1) &amp; hash]) != null) { // 如果哈希表容量为0或者关键字没有命中，直接返回null if (first.hash == hash &amp;&amp; // 关键字命中的话比较第一个节点 ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) return first; if ((e = first.next) != null) { if (first instanceof TreeNode) // 以红黑树的方式查找 return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); do { // 遍历链表查找 if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; } while ((e = e.next) != null); } } return null; } hash过程和resize过程分析hash过程在HashMap里就是一个hash方法： static final int hash(Object key) { int h; // 使用hashCode的值和hashCode的值无符号右移16位做异或操作 return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16); } 这段代码是什么意思呢？ 我们以文中的那个demo为例，说明”java”这个关键字是如何找到对应bucket的过程。 从上图可以看到，hash方法得到的hash值是根据关键字的hashCode的高16位和低16位进行异或操作得到的一个值。 这个值再与哈希表容量-1值进行与操作得到最终的bucket索引值。 (n - 1) &amp; hash hashCode的高16位与低16位进行异或操作主要是设计者想了一个顾全大局的方法(综合考虑了速度、作用、质量)来做的。 如果链表的数量大了，HashMap会把哈希表转换成红黑树来进行处理，本文不讨论这部分内容。 现在回过头来看例子，为什么初始化了一个容量为5的HashMap，但是哈希表的容量为8，而且阀值为6？ 因为HashMap的构造函数初始化threshold的时候调用了tableSizeFor方法，这个方法会把容量改成2的幂的整数，主要是为了哈希表散列更均匀。 // 定位bucket索引的最后操作。如果n为奇数，n-1就是偶数，偶数的话转成二进制最后一位是0，相反如果是奇数，最后一位是1，这样产生的索引值将更均匀 (n - 1) &amp; hash tableSizeFor方法如下： this.threshold = tableSizeFor(initialCapacity); // 保证thresold为2的幂 static final int tableSizeFor(int cap) { int n = cap - 1; n |= n &gt;&gt;&gt; 1; n |= n &gt;&gt;&gt; 2; n |= n &gt;&gt;&gt; 4; n |= n &gt;&gt;&gt; 8; n |= n &gt;&gt;&gt; 16; return (n &lt; 0) ? 1 : (n &gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1; } 阀值为6是因为之后进行resize操作的时候更新了阀值 阀值 = 容量 * 加载因子 = 8 * 0.75 = 6 HashMap的扩容会把原先哈希表的容量扩大两倍。扩大之后，会对节点重新进行处理。 哈希表上的节点的状态有3种，分别是单节点，无节点，链表，扩容对于这3种状态的处理方式如下： 以8节点为原先容量，扩容为16容量讲解。 单节点：由于容量扩大两倍，相当于左移1位。扩容前与00000111[7，n - 1 = 8 - 1]进行与操作。扩容后与00001111[15, n - 1 = 16 - 1]进行与操作。所以最终的结果要是还是在原位置，要么在原位置 +8(+old capacity) 位置 无节点：不处理 链表：遍历各个节点，每个节点的处理方式跟单节点一样，结果分成2种，还在原位置和原位置 +8 位置 单节点处理示意图如下，这么设计的原因就是不需要再次计算hash值，只需要移动位置(+old capacity)即可： 下图是一个HashMap扩容之后的效果图（省去了索引为7橙色链表的虚线，太多线条了）： 哈希表扩容是使用resize方法完成： final Node&lt;K,V&gt;[] resize() { Node&lt;K,V&gt;[] oldTab = table; int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; int newCap, newThr = 0; if (oldCap &gt; 0) { // 如果老容量大于0，说明哈希表中已经有数据了，然后进行扩容 if (oldCap &gt;= MAXIMUM_CAPACITY) { // 超过最大容量的话，不扩容 threshold = Integer.MAX_VALUE; return oldTab; } else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; // 容量加倍 oldCap &gt;= DEFAULT_INITIAL_CAPACITY) // 如果老的容量超过默认容量的话 newThr = oldThr &lt;&lt; 1; // 阀值加倍 } else if (oldThr &gt; 0) // 根据thresold初始化数组 newCap = oldThr; else { // 使用默认配置 newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); } if (newThr == 0) { float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); } threshold = newThr; @SuppressWarnings({&quot;rawtypes&quot;,&quot;unchecked&quot;}) Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; table = newTab; if (oldTab != null) { for (int j = 0; j &lt; oldCap; ++j) { // 扩容之后进行rehash操作 Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) { oldTab[j] = null; if (e.next == null) newTab[e.hash &amp; (newCap - 1)] = e; // 单节点扩容 else if (e instanceof TreeNode) ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); // 红黑树方式处理 else { // 链表扩容 Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; do { next = e.next; if ((e.hash &amp; oldCap) == 0) { if (loTail == null) loHead = e; else loTail.next = e; loTail = e; } else { if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; } } while ((e = next) != null); if (loTail != null) { loTail.next = null; newTab[j] = loHead; } if (hiTail != null) { hiTail.next = null; newTab[j + oldCap] = hiHead; } } } } } return newTab; } HashMap注意的地方 HashMap底层是个哈希表，使用拉链法解决冲突 HashMap内部存储的数据是无序的，这是因为HashMap内部的数组的下表是根据hash值算出来的 HashMap允许key为null HashMap不是一个线程安全的类","raw":null,"content":null,"categories":[{"name":"jdk","slug":"jdk","permalink":"http://fangjian0423.github.io/categories/jdk/"}],"tags":[{"name":"jdk","slug":"jdk","permalink":"http://fangjian0423.github.io/tags/jdk/"},{"name":"map","slug":"map","permalink":"http://fangjian0423.github.io/tags/map/"}]},{"title":"jdk LinkedList工作原理分析","slug":"jdk_linkedlist","date":"2016-03-27T09:35:27.000Z","updated":"2016-05-10T14:59:25.000Z","comments":true,"path":"2016/03/27/jdk_linkedlist/","link":"","permalink":"http://fangjian0423.github.io/2016/03/27/jdk_linkedlist/","excerpt":"List接口的实现类之一ArrayList的内部实现是一个数组，而另外一个实现LinkedList内部实现是使用双向链表。\nLinkedList在内部定义了一个叫做Node类型的内部类，这个Node就是一个节点，链表中的节点，这个节点有3个属性，分别是元素item(当前节点要表示的值), 前节点prev(当前节点之前位置上的一个节点)，后节点next(当前节点后面位置的一个节点)。 \nLinkedList关于数据的插入，删除操作都会处理这些节点的前后关系。而不像ArrayList那样只需要移动元素的位置即可。","text":"List接口的实现类之一ArrayList的内部实现是一个数组，而另外一个实现LinkedList内部实现是使用双向链表。 LinkedList在内部定义了一个叫做Node类型的内部类，这个Node就是一个节点，链表中的节点，这个节点有3个属性，分别是元素item(当前节点要表示的值), 前节点prev(当前节点之前位置上的一个节点)，后节点next(当前节点后面位置的一个节点)。 LinkedList关于数据的插入，删除操作都会处理这些节点的前后关系。而不像ArrayList那样只需要移动元素的位置即可。 源码分析在分析LinkedList之前，我们先看下它里面的内部类Node，也就是节点的定义： private static class Node&lt;E&gt; { E item; // 节点所表示的值 Node&lt;E&gt; next; // 后节点 Node&lt;E&gt; prev; // 前节点 Node(Node&lt;E&gt; prev, E element, Node&lt;E&gt; next) { this.item = element; this.next = next; this.prev = prev; } } LinkedList的3个属性： transient int size = 0; // 集合链表内节点数量 transient Node&lt;E&gt; first; // 集合链表的首节点 transient Node&lt;E&gt; last; // 集合链表的尾节点 add(E e) 添加元素到链表的最后一个位置 public boolean add(E e) { linkLast(e); return true; } void linkLast(E e) { final Node&lt;E&gt; l = last; final Node&lt;E&gt; newNode = new Node&lt;&gt;(l, e, null); // 由于是添加元素的链表尾部，所以也就是这个新的节点是最后1个节点，它的前节点肯定是目前链表的尾节点，它的后节点为null last = newNode; // 尾节点变成新的节点 if (l == null) // 如果一开始尾节点还没设置，那么说明这个新的节点是第一个节点，那么首节点也就是这个第一个节点 first = newNode; else // 否则，说明新节点不是第一个节点，处理节点前后关系 l.next = newNode; size++; // 节点数量+1 modCount++; } 上图第一个图就表示一个已经有1，2这2个节点的LinkedList调用add方法，第二个图表示添加一个值为3的元素后的情况。原先的尾节点2的后节点变成的新节点3，新节点3的前节点是原先的尾节点2，新节点3的后节点为null。同时链表的尾节点变成了3. add(int index, E element) 添加元素到列表中的指定位置 public void add(int index, E element) { checkPositionIndex(index); // 检查索引的合法性，不能超过链表个数，不能小于0 if (index == size) // 如果是在链表尾部插入节点，那么直接调用linkLast方法，上面已经分析过 linkLast(element); else // 不在链表尾部插入节点的话，调用linkBefore方法，参数为要插入的元素值和节点对象 linkBefore(element, node(index)); } 先看一下node方法是如何根据索引找到对应的节点的： Node&lt;E&gt; node(int index) { // 用了一个小算法，如果索引比链表数量的一半还要小，从前往后找，这样只需要花O(n/2)的时间获取节点 if (index &lt; (size &gt;&gt; 1)) { Node&lt;E&gt; x = first; for (int i = 0; i &lt; index; i++) x = x.next; return x; } else { // 否则从后往前找 Node&lt;E&gt; x = last; for (int i = size - 1; i &gt; index; i--) x = x.prev; return x; } } void linkBefore(E e, Node&lt;E&gt; succ) { // succ节点表示要新插入节点应该在的位置 final Node&lt;E&gt; pred = succ.prev; final Node&lt;E&gt; newNode = new Node&lt;&gt;(pred, e, succ); // 1：新节点的前节点就是succ节点的前节点，新节点的后节点是succ节点 succ.prev = newNode; // 2：succ的前节点就是新节点 if (pred == null) // prev=null表示succ节点就是head首节点，这样的话只需要重新set一下首节点即可，首节点的后节点在步骤1以及设置过了 first = newNode; else // succ不是首节点的话执行步骤3 pred.next = newNode; // 3：succ节点的前节点的后节点就是新节点 size++; // 节点数量+1 modCount++; } 上面代码中注释的1，2，3点就在下图中表示： LinkedList还提供了2种特殊的add方法，分别是addFirst和addLast方法，处理添加首节点和尾节点，原理都是差不多的，处理链表之间的关联关系即可。 remove(int index) 移除指定位置上的节点 public E remove(int index) { checkElementIndex(index); // 检查索引的合法性，不能超过链表个数，不能小于0 return unlink(node(index)); } E unlink(Node&lt;E&gt; x) { final E element = x.item; final Node&lt;E&gt; next = x.next; final Node&lt;E&gt; prev = x.prev; if (prev == null) { // 特殊情况，删除的是头节点 first = next; } else { prev.next = next; // 1 x.prev = null; // 1 } if (next == null) { // 特殊情况，删除的是尾节点 last = prev; } else { next.prev = prev; // 2 x.next = null; // 2 } x.item = null; // 3 size--; // 链表数量减一 modCount++; return element; } 上面代码中注释的1，2，3点就在下图中表示： get(int index) 得到索引位置上的元素 public E get(int index) { checkElementIndex(index); // 检查索引的合法性，不能超过链表个数，不能小于0 return node(index).item; // 直接找到节点，返回节点的元素值即可 } LinkedList和ArrayList的比较 LinkedList和ArrayList的设计理念完全不一样，ArrayList基于数组，而LinkedList基于节点，也就是链表。所以LinkedList内部没有容量这个概念，因为是链表，链表是无界的 两者的使用场景不同，ArrayList适用于读多写少的场合。LinkedList适用于写多读少的场合。 刚好相反。 那是因为LinkedList要找节点的话必须要遍历一个一个节点，直到找到为止。而ArrayList完全不需要，因为ArrayList内部维护着一个数组，直接根据索引拿到需要的元素即可。 两个都是List接口的实现类，都是一种集合","raw":null,"content":null,"categories":[{"name":"jdk","slug":"jdk","permalink":"http://fangjian0423.github.io/categories/jdk/"}],"tags":[{"name":"jdk","slug":"jdk","permalink":"http://fangjian0423.github.io/tags/jdk/"},{"name":"collection","slug":"collection","permalink":"http://fangjian0423.github.io/tags/collection/"}]},{"title":"jdk ArrayList工作原理分析","slug":"jdk_arraylist","date":"2016-03-27T05:33:17.000Z","updated":"2016-05-10T14:59:25.000Z","comments":true,"path":"2016/03/27/jdk_arraylist/","link":"","permalink":"http://fangjian0423.github.io/2016/03/27/jdk_arraylist/","excerpt":"list是一种有序的集合(an ordered collection), 通常也会被称为序列(sequence)，使用list可以精确地控制每个元素的插入，可以通过索引值找到对应list中的各个项，也可以在list中查询元素。\n以前的几段话摘自jdk文档的说明。\n其实list就相当于一个动态的数组，也就是链表，普通的数组长度大小都是固定的，而list是一个动态的数组，当list的长度满了，再次插入数据到list当中的时候，list会自动地扩展它的长度。","text":"list是一种有序的集合(an ordered collection), 通常也会被称为序列(sequence)，使用list可以精确地控制每个元素的插入，可以通过索引值找到对应list中的各个项，也可以在list中查询元素。 以前的几段话摘自jdk文档的说明。 其实list就相当于一个动态的数组，也就是链表，普通的数组长度大小都是固定的，而list是一个动态的数组，当list的长度满了，再次插入数据到list当中的时候，list会自动地扩展它的长度。 ArrayList源码分析首先我们先分析一个List接口的实现类之一，也是最常用的ArrayList的源码。 ArrayList底层使用一个数组完成数据的添加，查询，删除，修改。这个数组就是下面提到的elementData。 这里分析的代码是基于jdk1.7的。 ArrayList类的属性如下： private static final int DEFAULT_CAPACITY = 10; // 集合的默认容量 private static final Object[] EMPTY_ELEMENTDATA = {}; // 一个空集合数组，容量为0 private transient Object[] elementData; // 存储集合数据的数组，默认值为null private int size; // ArrayList集合中数组的当前有效长度，比如数组的容量是5，size是1 表示容量为5的数组目前已经有1条记录了，其余4条记录还是为空 接下来看一下ArrayList的构造函数： ArrayList有3个构造函数，分别是 public ArrayList(int initialCapacity) { // 带有集合容量参数的构造函数 // 调用父类AbstractList的方法构造函数 super(); if (initialCapacity &lt; 0) // 如果集合的容量小于0，这明显是个错误数值，直接抛出异常 throw new IllegalArgumentException(&quot;Illegal Capacity: &quot;+ initialCapacity); this.elementData = new Object[initialCapacity]; // 初始化elementData属性，确定容量 } public ArrayList() { // 没有参数的构造函数 super(); // 调用父类AbstractList的方法构造函数 this.elementData = EMPTY_ELEMENTDATA; // 让elementData和ArrayList的EMPTY_ELEMENTDATA这个空数组使用同一个引用 } public ArrayList(Collection&lt;? extends E&gt; c) { // 参数是一个集合的构造函数 elementData = c.toArray(); // elementData直接使用参数集合内部的数组 size = elementData.length; // 初始化数组当前有效长度 // c.toArray方法可能不会返回一个Object[]结果，需要做一层判断。这个一个Java的bug，可以在http://bugs.java.com/bugdatabase/view_bug.do?bug_id=6260652查看 if (elementData.getClass() != Object[].class) elementData = Arrays.copyOf(elementData, size, Object[].class); } 接下来挑几个重要的方法讲解一下： add(E e) 方法这个方法的作用就是把 元素添加到集合的最后面 源码： public boolean add(E e) { ensureCapacityInternal(size + 1); // 调用ensureCapacityInternal，参数是集合当前的长度。确保集合容量够大，不够的话需要扩容 elementData[size++] = e; // 数组容量够的话，直接添加元素到数组最后一个位置即可，同时修改集合当前有效长度 return true; } private void ensureCapacityInternal(int minCapacity) { if (elementData == EMPTY_ELEMENTDATA) { // 如果数组是个空数组，说明调用的是无参的构造函数 // 如果调用的是无参构造函数，说明数组容量为0，那就需要使用默认容量 minCapacity = Math.max(DEFAULT_CAPACITY, minCapacity); } ensureExplicitCapacity(minCapacity); } private void ensureExplicitCapacity(int minCapacity) { modCount++; // 如果集合需要的最小长度比数组容量要大，那么就需要扩容，已经放不下了 if (minCapacity - elementData.length &gt; 0) grow(minCapacity); } private void grow(int minCapacity) { // 扩容的实现 int oldCapacity = elementData.length; int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); // 长度扩大1.5倍 if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); // 将数组拷贝到新长度的数组中 elementData = Arrays.copyOf(elementData, newCapacity); } 以下面这段代码讲解一下扩容的机制： // 初始化一个容量为5的数组 ArrayList&lt;Integer&gt; list = new ArrayList&lt;Integer&gt;(5); list.add(1); list.add(2); list.add(3); list.add(4); list.add(5); // 当添加第6个元素的时候，数组进行了扩容，扩容1.5倍(5+5/2=7) list.add(6); 上图2个白色的空间就是扩容出来的，添加第6个元素之后，最后一个元素没被设置。 add(int index, E element) 方法这个方法的作用是 在指定位置插入数据，该方法的缺点就是如果集合数据量很大，移动元素位置将会话费不少时间： public void add(int index, E element) { rangeCheckForAdd(index); // 检查索引位置的正确的，不能小于0也不能大于数组有效长度 ensureCapacityInternal(size + 1); // 扩容检测 System.arraycopy(elementData, index, elementData, index + 1, size - index); // 移动数组位置，数据量很大的话，性能变差 elementData[index] = element; // 指定的位置插入数据 size++; // 数组有效长度+1 } 上图就表示要在容量为5的数组中的第4个位置插入6这个元素，会进行3个步骤： 容量为5，再次加入元素，需要扩容，扩容出2个白色的空间 扩容之后，5和4这2个元素都移到后面那个位置上 移动完毕之后空出了第4个位置，插入元素6 remove(int index)remove方法就是 移除对应坐标值上的数据 public E remove(int index) { rangeCheck(index); // 检查索引值是否合法 modCount++; E oldValue = elementData(index); // 得到对应索引位置上的元素 int numMoved = size - index - 1; // 需要移动的数量 if (numMoved &gt; 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); // 从后往前移，留出最后一个元素 elementData[--size] = null; // 清楚对应位置上的对象，让gc回收 return oldValue; } 比如要移除5个元素中的第3个元素，首先要把4和5这2个位置的元素分别set到3和4这2个位置上，set完之后最后一个位置也就是第5个位置set为null。 remove(Object o) 找出数组中的元素，然后移除 // 跟remove索引元素一样，这个方法是根据equals比较 public boolean remove(Object o) { if (o == null) { // ArrayList允许元素为null，所以对null值的删除在这个分支里进行 for (int index = 0; index &lt; size; index++) if (elementData[index] == null) { fastRemove(index); return true; } } else { // 效率比较低，需要从第1个元素开始遍历直到找到equals相等的元素后才进行删除，删除同样需要移动元素 for (int index = 0; index &lt; size; index++) if (o.equals(elementData[index])) { fastRemove(index); return true; } } return false; } clear清除list中的所有数据 public void clear() { modCount++; // 遍历集合数据，全部set为null for (int i = 0; i &lt; size; i++) elementData[i] = null; size = 0; // 数组有效长度变成0 } set(int index, E element)用element值替换下标值为index的值 public E set(int index, E element) { rangeCheck(index); // 检查索引值是否合法 E oldValue = elementData(index); elementData[index] = element; // 直接替换 return oldValue; } get(int index)得到下标值为index的元素 public E get(int index) { rangeCheck(index); // 检查索引值是否合法 return elementData(index); // 直接返回下标值 } addAll在列表的结尾添加一个Collection集合 public boolean addAll(Collection&lt;? extends E&gt; c) { Object[] a = c.toArray(); int numNew = a.length; ensureCapacityInternal(size + numNew); // 扩容检测 System.arraycopy(a, 0, elementData, size, numNew); // 直接在数组后面添加新的数组中的所有元素 size += numNew; // 更新有效长度 return numNew != 0; } toArray根据elementData数组拷贝一份新的数组 public Object[] toArray() { return Arrays.copyOf(elementData, size); } ArrayList的注意点 当数据量很大的时候，ArrayList内部操作元素的时候会移动位置，很耗性能 ArrayList虽然可以自动扩展长度，但是数据量一大，扩展的也多，会造成很多空间的浪费 ArrayList有一个内部私有类，SubList。ArrayList提供一个subList方法用于构造这个SubList。这里需要注意的是SubList和ArrayList使用的数据引用是同一个对象，在SubList中操作数据和在ArrayList中操作数据都会影响双方。 ArrayList允许加入null元素","raw":null,"content":null,"categories":[{"name":"jdk","slug":"jdk","permalink":"http://fangjian0423.github.io/categories/jdk/"}],"tags":[{"name":"jdk","slug":"jdk","permalink":"http://fangjian0423.github.io/tags/jdk/"},{"name":"collection","slug":"collection","permalink":"http://fangjian0423.github.io/tags/collection/"}]},{"title":"Java线程池ThreadPoolExecutor源码分析","slug":"java-threadpool-analysis","date":"2016-03-22T12:56:11.000Z","updated":"2016-05-10T14:59:25.000Z","comments":true,"path":"2016/03/22/java-threadpool-analysis/","link":"","permalink":"http://fangjian0423.github.io/2016/03/22/java-threadpool-analysis/","excerpt":"ThreadPoolExecutor是jdk内置线程池的一个实现，基本上大部分情况都会使用这个线程池完成各项操作。\n","text":"ThreadPoolExecutor是jdk内置线程池的一个实现，基本上大部分情况都会使用这个线程池完成各项操作。 本文分析ThreadPoolExecutor的实现原理。 ThreadPoolExecutor的状态和属性ThreadPoolExecutor的属性在之前的一篇java内置的线程池笔记文章中解释过了，本文不再解释。 ThreadPoolExecutor线程池有5个状态，分别是： RUNNING：可以接受新的任务，也可以处理阻塞队列里的任务 SHUTDOWN：不接受新的任务，但是可以处理阻塞队列里的任务 STOP：不接受新的任务，不处理阻塞队列里的任务，中断正在处理的任务 TIDYING：过渡状态，也就是说所有的任务都执行完了，当前线程池已经没有有效的线程，这个时候线程池的状态将会TIDYING，并且将要调用terminated方法 TERMINATED：终止状态。terminated方法调用完成以后的状态 状态之间可以进行转换： RUNNING -&gt; SHUTDOWN：手动调用shutdown方法，或者ThreadPoolExecutor要被GC回收的时候调用finalize方法，finalize方法内部也会调用shutdown方法 (RUNNING or SHUTDOWN) -&gt; STOP：调用shutdownNow方法 SHUTDOWN -&gt; TIDYING：当队列和线程池都为空的时候 STOP -&gt; TIDYING：当线程池为空的时候 TIDYING -&gt; TERMINATED：terminated方法调用完成之后 ThreadPoolExecutor内部还保存着线程池的有效线程个数。 状态和线程数在ThreadPoolExecutor内部使用一个整型变量保存，没错，一个变量表示两种含义。 为什么一个整型变量既可以保存状态，又可以保存数量？ 分析一下： 首先，我们知道java中1个整型占4个字节，也就是32位，所以1个整型有32位。 所以整型1用二进制表示就是：00000000000000000000000000000001 整型-1用二进制表示就是：11111111111111111111111111111111(这个是补码，不懂的同学可以看下原码，反码，补码的知识) 在ThreadPoolExecutor，整型中32位的前3位用来表示线程池状态，后3位表示线程池中有效的线程数。 // 前3位表示状态，所有线程数占29位 private static final int COUNT_BITS = Integer.SIZE - 3; 线程池容量大小为 1 &lt;&lt; 29 - 1 = 00011111111111111111111111111111(二进制)，代码如下 private static final int CAPACITY = (1 &lt;&lt; COUNT_BITS) - 1; RUNNING状态 -1 &lt;&lt; 29 = 11111111111111111111111111111111 &lt;&lt; 29 = 11100000000000000000000000000000(前3位为111)： private static final int RUNNING = -1 &lt;&lt; COUNT_BITS; SHUTDOWN状态 0 &lt;&lt; 29 = 00000000000000000000000000000000 &lt;&lt; 29 = 00000000000000000000000000000000(前3位为000) private static final int SHUTDOWN = 0 &lt;&lt; COUNT_BITS; STOP状态 1 &lt;&lt; 29 = 00000000000000000000000000000001 &lt;&lt; 29 = 00100000000000000000000000000000(前3位为001)： private static final int STOP = 1 &lt;&lt; COUNT_BITS; TIDYING状态 2 &lt;&lt; 29 = 00000000000000000000000000000010 &lt;&lt; 29 = 01000000000000000000000000000000(前3位为010)： private static final int TIDYING = 2 &lt;&lt; COUNT_BITS; TERMINATED状态 3 &lt;&lt; 29 = 00000000000000000000000000000011 &lt;&lt; 29 = 01100000000000000000000000000000(前3位为011)： private static final int TERMINATED = 3 &lt;&lt; COUNT_BITS; 清楚状态位之后，下面是获得状态和线程数的内部方法： // 得到线程数，也就是后29位的数字。 直接跟CAPACITY做一个与操作即可，CAPACITY就是的值就 1 &lt;&lt; 29 - 1 = 00011111111111111111111111111111。 与操作的话前面3位肯定为0，相当于直接取后29位的值 private static int workerCountOf(int c) { return c &amp; CAPACITY; } // 得到状态，CAPACITY的非操作得到的二进制位11100000000000000000000000000000，然后做在一个与操作，相当于直接取前3位的的值 private static int runStateOf(int c) { return c &amp; ~CAPACITY; } // 或操作。相当于更新数量和状态两个操作 private static int ctlOf(int rs, int wc) { return rs | wc; } 线程池初始化状态线程数变量： // 初始化状态和数量，状态为RUNNING，线程数为0 private final AtomicInteger ctl = new AtomicInteger(ctlOf(RUNNING, 0)); ThreadPoolExecutor执行任务使用ThreadPoolExecutor执行任务的时候，可以使用execute或submit方法，submit方法如下： public Future&lt;?&gt; submit(Runnable task) { if (task == null) throw new NullPointerException(); RunnableFuture&lt;Void&gt; ftask = newTaskFor(task, null); execute(ftask); return ftask; } 很明显地看到，submit方法内部使用了execute方法，而且submit方法是有返回值的。在调用execute方法之前，使用FutureTask包装一个Runnable，这个FutureTask就是返回值。 由于submit方法内部调用execute方法，所以execute方法就是执行任务的方法，来看一下execute方法，execute方法内部分3个步骤进行处理。 如果当前正在执行的Worker数量比corePoolSize(基本大小)要小。直接创建一个新的Worker执行任务，会调用addWorker方法 如果当前正在执行的Worker数量大于等于corePoolSize(基本大小)。将任务放到阻塞队列里，如果阻塞队列没满并且状态是RUNNING的话，直接丢到阻塞队列，否则执行第3步。丢到阻塞队列之后，还需要再做一次验证(丢到阻塞队列之后可能另外一个线程关闭了线程池或者刚刚加入到队列的线程死了)。如果这个时候线程池不在RUNNING状态，把刚刚丢入队列的任务remove掉，调用reject方法，否则查看Worker数量，如果Worker数量为0，起一个新的Worker去阻塞队列里拿任务执行 丢到阻塞失败的话，会调用addWorker方法尝试起一个新的Worker去阻塞队列拿任务并执行任务，如果这个新的Worker创建失败，调用reject方法 上面说的Worker可以暂时理解为一个执行任务的线程。 execute方法源码如下，上面提到的3个步骤对应源码中的3个注释： public void execute(Runnable command) { if (command == null) throw new NullPointerException(); int c = ctl.get(); if (workerCountOf(c) &lt; corePoolSize) { // 第一个步骤，满足线程池中的线程大小比基本大小要小 if (addWorker(command, true)) // addWorker方法第二个参数true表示使用基本大小 return; c = ctl.get(); } if (isRunning(c) &amp;&amp; workQueue.offer(command)) { // 第二个步骤，线程池的线程大小比基本大小要大，并且线程池还在RUNNING状态，阻塞队列也没满的情况，加到阻塞队列里 int recheck = ctl.get(); if (! isRunning(recheck) &amp;&amp; remove(command)) // 虽然满足了第二个步骤，但是这个时候可能突然线程池关闭了，所以再做一层判断 reject(command); else if (workerCountOf(recheck) == 0) addWorker(null, false); } else if (!addWorker(command, false)) // 第三个步骤，直接使用线程池最大大小。addWorker方法第二个参数false表示使用最大大小 reject(command); } addWorker关系着如何起一个线程，再看addWorker方法之前，先看一下ThreadPoolExecutor的一个内部类Worker, Worker是一个AQS的实现类(为何设计成一个AQS在闲置Worker里会说明)，同时也是一个实现Runnable的类，使用独占锁，它的构造函数只接受一个Runnable参数，内部保存着这个Runnable属性，还有一个thread线程属性用于包装这个Runnable(这个thread属性使用ThreadFactory构造，在构造函数内完成thread线程的构造)，另外还有一个completedTasks计数器表示这个Worker完成的任务数。Worker类复写了run方法，使用ThreadPoolExecutor的runWorker方法(在addWorker方法里调用)，直接启动Worker的话，会调用ThreadPoolExecutor的runWork方法。需要特别注意的是这个Worker是实现了Runnable接口的，thread线程属性使用ThreadFactory构造Thread的时候，构造的Thread中使用的Runnable其实就是Worker。下面的Worker的源码： private final class Worker extends AbstractQueuedSynchronizer implements Runnable { /** * This class will never be serialized, but we provide a * serialVersionUID to suppress a javac warning. */ private static final long serialVersionUID = 6138294804551838833L; /** Thread this worker is running in. Null if factory fails. */ final Thread thread; /** Initial task to run. Possibly null. */ Runnable firstTask; /** Per-thread task counter */ volatile long completedTasks; /** * Creates with given first task and thread from ThreadFactory. * @param firstTask the first task (null if none) */ Worker(Runnable firstTask) { // 使用ThreadFactory构造Thread，这个构造的Thread内部的Runnable就是本身，也就是Worker。所以得到Worker的thread并start的时候，会执行Worker的run方法，也就是执行ThreadPoolExecutor的runWorker方法 setState(-1); 把状态位设置成-1，这样任何线程都不能得到Worker的锁，除非调用了unlock方法。这个unlock方法会在runWorker方法中一开始就调用，这是为了确保Worker构造出来之后，没有任何线程能够得到它的锁，除非调用了runWorker之后，其他线程才能获得Worker的锁 this.firstTask = firstTask; this.thread = getThreadFactory().newThread(this); } /** Delegates main run loop to outer runWorker */ public void run() { runWorker(this); } // Lock methods // // The value 0 represents the unlocked state. // The value 1 represents the locked state. protected boolean isHeldExclusively() { return getState() != 0; } protected boolean tryAcquire(int unused) { if (compareAndSetState(0, 1)) { setExclusiveOwnerThread(Thread.currentThread()); return true; } return false; } protected boolean tryRelease(int unused) { setExclusiveOwnerThread(null); setState(0); return true; } public void lock() { acquire(1); } public boolean tryLock() { return tryAcquire(1); } public void unlock() { release(1); } public boolean isLocked() { return isHeldExclusively(); } void interruptIfStarted() { Thread t; if (getState() &gt;= 0 &amp;&amp; (t = thread) != null &amp;&amp; !t.isInterrupted()) { try { t.interrupt(); } catch (SecurityException ignore) { } } } } 接下来看一下addWorker源码： // 两个参数，firstTask表示需要跑的任务。boolean类型的core参数为true的话表示使用线程池的基本大小，为false使用线程池最大大小 // 返回值是boolean类型，true表示新任务被接收了，并且执行了。否则是false private boolean addWorker(Runnable firstTask, boolean core) { retry: for (;;) { int c = ctl.get(); int rs = runStateOf(c); // 线程池当前状态 // 这个判断转换成 rs &gt;= SHUTDOWN &amp;&amp; (rs != SHUTDOWN || firstTask != null || workQueue.isEmpty)。 // 概括为3个条件： // 1. 线程池不在RUNNING状态并且状态是STOP、TIDYING或TERMINATED中的任意一种状态 // 2. 线程池不在RUNNING状态，线程池接受了新的任务 // 3. 线程池不在RUNNING状态，阻塞队列为空。 满足这3个条件中的任意一个的话，拒绝执行任务 if (rs &gt;= SHUTDOWN &amp;&amp; ! (rs == SHUTDOWN &amp;&amp; firstTask == null &amp;&amp; ! workQueue.isEmpty())) return false; for (;;) { int wc = workerCountOf(c); // 线程池线程个数 if (wc &gt;= CAPACITY || wc &gt;= (core ? corePoolSize : maximumPoolSize)) // 如果线程池线程数量超过线程池最大容量或者线程数量超过了基本大小(core参数为true，core参数为false的话判断超过最大大小) return false; // 超过直接返回false if (compareAndIncrementWorkerCount(c)) // 没有超过各种大小的话，cas操作线程池线程数量+1，cas成功的话跳出循环 break retry; c = ctl.get(); // 重新检查状态 if (runStateOf(c) != rs) // 如果状态改变了，重新循环操作 continue retry; // else CAS failed due to workerCount change; retry inner loop } } // 走到这一步说明cas操作成功了，线程池线程数量+1 boolean workerStarted = false; // 任务是否成功启动标识 boolean workerAdded = false; // 任务是否添加成功标识 Worker w = null; try { final ReentrantLock mainLock = this.mainLock; // 得到线程池的可重入锁 w = new Worker(firstTask); // 基于任务firstTask构造worker final Thread t = w.thread; // 使用Worker的属性thread，这个thread是使用ThreadFactory构造出来的 if (t != null) { // ThreadFactory构造出的Thread有可能是null，做个判断 mainLock.lock(); // 锁住，防止并发 try { // 在锁住之后再重新检测一下状态 int c = ctl.get(); int rs = runStateOf(c); if (rs &lt; SHUTDOWN || (rs == SHUTDOWN &amp;&amp; firstTask == null)) { // 如果线程池在RUNNING状态或者线程池在SHUTDOWN状态并且任务是个null if (t.isAlive()) // 判断线程是否还活着，也就是说线程已经启动并且还没死掉 throw new IllegalThreadStateException(); // 如果存在已经启动并且还没死的线程，抛出异常 workers.add(w); // worker添加到线程池的workers属性中，是个HashSet int s = workers.size(); // 得到目前线程池中的线程个数 if (s &gt; largestPoolSize) // 如果线程池中的线程个数超过了线程池中的最大线程数时，更新一下这个最大线程数 largestPoolSize = s; workerAdded = true; // 标识一下任务已经添加成功 } } finally { mainLock.unlock(); // 解锁 } if (workerAdded) { // 如果任务添加成功，运行任务，改变一下任务成功启动标识 t.start(); // 启动线程，这里的t是Worker中的thread属性，所以相当于就是调用了Worker的run方法 workerStarted = true; } } } finally { if (! workerStarted) // 如果任务启动失败，调用addWorkerFailed方法 addWorkerFailed(w); } return workerStarted; } Worker中的线程start的时候，调用Worker本身run方法，这个run方法之前分析过，调用外部类ThreadPoolExecutor的runWorker方法，直接看runWorker方法： final void runWorker(Worker w) { Thread wt = Thread.currentThread(); // 得到当前线程 Runnable task = w.firstTask; // 得到Worker中的任务task，也就是用户传入的task w.firstTask = null; // 将Worker中的任务置空 w.unlock(); // allow interrupts。 boolean completedAbruptly = true; try { // 如果worker中的任务不为空，继续知否，否则使用getTask获得任务。一直死循环，除非得到的任务为空才退出 while (task != null || (task = getTask()) != null) { w.lock(); // 如果拿到了任务，给自己上锁，表示当前Worker已经要开始执行任务了，已经不是闲置Worker(闲置Worker的解释请看下面的线程池关闭) // 在执行任务之前先做一些处理。 1. 如果线程池已经处于STOP状态并且当前线程没有被中断，中断线程 2. 如果线程池还处于RUNNING或SHUTDOWN状态，并且当前线程已经被中断了，重新检查一下线程池状态，如果处于STOP状态并且没有被中断，那么中断线程 if ((runStateAtLeast(ctl.get(), STOP) || (Thread.interrupted() &amp;&amp; runStateAtLeast(ctl.get(), STOP))) &amp;&amp; !wt.isInterrupted()) wt.interrupt(); try { beforeExecute(wt, task); // 任务执行前需要做什么，ThreadPoolExecutor是个空实现 Throwable thrown = null; try { task.run(); // 真正的开始执行任务，调用的是run方法，而不是start方法。这里run的时候可能会被中断，比如线程池调用了shutdownNow方法 } catch (RuntimeException x) { // 任务执行发生的异常全部抛出，不在runWorker中处理 thrown = x; throw x; } catch (Error x) { thrown = x; throw x; } catch (Throwable x) { thrown = x; throw new Error(x); } finally { afterExecute(task, thrown); // 任务执行结束需要做什么，ThreadPoolExecutor是个空实现 } } finally { task = null; w.completedTasks++; // 记录执行任务的个数 w.unlock(); // 执行完任务之后，解锁，Worker变成闲置Worker } } completedAbruptly = false; } finally { processWorkerExit(w, completedAbruptly); // 回收Worker方法 } } 我们看一下getTask方法是如何获得任务的： // 如果发生了以下四件事中的任意一件，那么Worker需要被回收： // 1. Worker个数比线程池最大大小要大 // 2. 线程池处于STOP状态 // 3. 线程池处于SHUTDOWN状态并且阻塞队列为空 // 4. 使用超时时间从阻塞队列里拿数据，并且超时之后没有拿到数据(allowCoreThreadTimeOut || workerCount &gt; corePoolSize) private Runnable getTask() { boolean timedOut = false; // 如果使用超时时间并且也没有拿到任务的标识 retry: for (;;) { int c = ctl.get(); int rs = runStateOf(c); // 如果线程池是SHUTDOWN状态并且阻塞队列为空的话，worker数量减一，直接返回null(SHUTDOWN状态还会处理阻塞队列任务，但是阻塞队列为空的话就结束了)，如果线程池是STOP状态的话，worker数量建议，直接返回null(STOP状态不处理阻塞队列任务)[方法一开始注释的2，3两点，返回null，开始Worker回收] if (rs &gt;= SHUTDOWN &amp;&amp; (rs &gt;= STOP || workQueue.isEmpty())) { decrementWorkerCount(); return null; } boolean timed; // 标记从队列中取任务时是否设置超时时间，如果为true说明这个worker可能需要回收，为false的话这个worker会一直存在，并且阻塞当前线程等待阻塞队列中有数据 for (;;) { int wc = workerCountOf(c); // 得到当前线程池Worker个数 // allowCoreThreadTimeOut属性默认为false，表示线程池中的核心线程在闲置状态下还保留在池中；如果是true表示核心线程使用keepAliveTime这个参数来作为超时时间 // 如果worker数量比基本大小要大的话，timed就为true，需要进行回收worker timed = allowCoreThreadTimeOut || wc &gt; corePoolSize; if (wc &lt;= maximumPoolSize &amp;&amp; ! (timedOut &amp;&amp; timed)) // 方法一开始注释的1，4两点，会进行下一步worker数量减一 break; if (compareAndDecrementWorkerCount(c)) // worker数量减一，返回null，之后会进行Worker回收工作 return null; c = ctl.get(); // 重新检查线程池状态 if (runStateOf(c) != rs) // 线程池状态改变的话重新开始外部循环，否则继续内部循环 continue retry; // else CAS failed due to workerCount change; retry inner loop } try { // 如果需要设置超时时间，使用poll方法，否则使用take方法一直阻塞等待阻塞队列新进数据 Runnable r = timed ? workQueue.poll(keepAliveTime, TimeUnit.NANOSECONDS) : workQueue.take(); if (r != null) return r; timedOut = true; } catch (InterruptedException retry) { timedOut = false; // 闲置Worker被中断 } } } 如果getTask返回的是null，那说明阻塞队列已经没有任务并且当前调用getTask的Worker需要被回收，那么会调用processWorkerExit方法进行回收： private void processWorkerExit(Worker w, boolean completedAbruptly) { if (completedAbruptly) // 如果Worker没有正常结束流程调用processWorkerExit方法，worker数量减一。如果是正常结束的话，在getTask方法里worker数量已经减一了 decrementWorkerCount(); final ReentrantLock mainLock = this.mainLock; mainLock.lock(); // 加锁，防止并发问题 try { completedTaskCount += w.completedTasks; // 记录总的完成任务数 workers.remove(w); // 线程池的worker集合删除掉需要回收的Worker } finally { mainLock.unlock(); // 解锁 } tryTerminate(); // 尝试结束线程池 int c = ctl.get(); if (runStateLessThan(c, STOP)) { // 如果线程池还处于RUNNING或者SHUTDOWN状态 if (!completedAbruptly) { // Worker是正常结束流程的话 int min = allowCoreThreadTimeOut ? 0 : corePoolSize; if (min == 0 &amp;&amp; ! workQueue.isEmpty()) min = 1; if (workerCountOf(c) &gt;= min) return; // 不需要新开一个Worker } // 新开一个Worker代替原先的Worker // 新开一个Worker需要满足以下3个条件中的任意一个： // 1. 用户执行的任务发生了异常 // 2. Worker数量比线程池基本大小要小 // 3. 阻塞队列不空但是没有任何Worker在工作 addWorker(null, false); } } 在回收Worker的时候线程池会尝试结束自己的运行，tryTerminate方法： final void tryTerminate() { for (;;) { int c = ctl.get(); // 满足3个条件中的任意一个，不终止线程池 // 1. 线程池还在运行，不能终止 // 2. 线程池处于TIDYING或TERMINATED状态，说明已经在关闭了，不允许继续处理 // 3. 线程池处于SHUTDOWN状态并且阻塞队列不为空，这时候还需要处理阻塞队列的任务，不能终止线程池 if (isRunning(c) || runStateAtLeast(c, TIDYING) || (runStateOf(c) == SHUTDOWN &amp;&amp; ! workQueue.isEmpty())) return; // 走到这一步说明线程池已经不在运行，阻塞队列已经没有任务，但是还要回收正在工作的Worker if (workerCountOf(c) != 0) { // 由于线程池不运行了，调用了线程池的关闭方法，在解释线程池的关闭原理的时候会说道这个方法 interruptIdleWorkers(ONLY_ONE); // 中断闲置Worker，直到回收全部的Worker。这里没有那么暴力，只中断一个，中断之后退出方法，中断了Worker之后，Worker会回收，然后还是会调用tryTerminate方法，如果还有闲置线程，那么继续中断 return; } // 走到这里说明worker已经全部回收了，并且线程池已经不在运行，阻塞队列已经没有任务。可以准备结束线程池了 final ReentrantLock mainLock = this.mainLock; mainLock.lock(); // 加锁，防止并发 try { if (ctl.compareAndSet(c, ctlOf(TIDYING, 0))) { // cas操作，将线程池状态改成TIDYING try { terminated(); // 调用terminated方法 } finally { ctl.set(ctlOf(TERMINATED, 0)); // terminated方法调用完毕之后，状态变为TERMINATED termination.signalAll(); } return; } } finally { mainLock.unlock(); // 解锁 } // else retry on failed CAS } } 解释了这么多，对线程池的启动并且执行任务做一个总结： 首先，构造线程池的时候，需要一些参数。一些重要的参数解释在 java内置的线程池笔记 文章中的结尾已经说明了一下重要参数的意义。 线程池构造完毕之后，如果用户调用了execute或者submit方法的时候，最后都会使用execute方法执行。 execute方法内部分3种情况处理任务： 如果当前正在执行的Worker数量比corePoolSize(基本大小)要小。直接创建一个新的Worker执行任务，会调用addWorker方法 如果当前正在执行的Worker数量大于等于corePoolSize(基本大小)。将任务放到阻塞队列里，如果阻塞队列没满并且状态是RUNNING的话，直接丢到阻塞队列，否则执行第3步 丢到阻塞失败的话，会调用addWorker方法尝试起一个新的Worker去阻塞队列拿任务并执行任务，如果这个新的Worker创建失败，调用reject方法 线程池中的这个基本大小指的是Worker的数量。一个Worker是一个Runnable的实现类，会被当做一个线程进行启动。Worker内部带有一个Runnable属性firstTask，这个firstTask可以为null，为null的话Worker会去阻塞队列拿任务执行，否则会先执行这个任务，执行完毕之后再去阻塞队列继续拿任务执行。 所以说如果Worker数量超过了基本大小，那么任务都会在阻塞队列里，当Worker执行完了它的第一个任务之后，就会去阻塞队列里拿其他任务继续执行。 Worker在执行的时候会根据一些参数进行调节，比如Worker数量超过了线程池基本大小或者超时时间到了等因素，这个时候Worker会被线程池回收，线程池会尽量保持内部的Worker数量不超过基本大小。 另外Worker执行任务的时候调用的是Runnable的run方法，而不是start方法，调用了start方法就相当于另外再起一个线程了。 Worker在回收的时候会尝试终止线程池。尝试关闭线程池的时候，会检查是否还有Worker在工作，检查线程池的状态，没问题的话会将状态过度到TIDYING状态，之后调用terminated方法，terminated方法调用完成之后将线程池状态更新到TERMINATED。 ThreadPoolExecutor的关闭线程池的启动过程分析好了之后，接下来看线程池的关闭操作： shutdown方法，关闭线程池，关闭之后阻塞队列里的任务不受影响，会继续被Worker处理，但是新的任务不会被接受： public void shutdown() { final ReentrantLock mainLock = this.mainLock; mainLock.lock(); // 关闭的时候需要加锁，防止并发 try { checkShutdownAccess(); // 检查关闭线程池的权限 advanceRunState(SHUTDOWN); // 把线程池状态更新到SHUTDOWN interruptIdleWorkers(); // 中断闲置的Worker onShutdown(); // 钩子方法，默认不处理。ScheduledThreadPoolExecutor会做一些处理 } finally { mainLock.unlock(); // 解锁 } tryTerminate(); // 尝试结束线程池，上面已经分析过了 } interruptIdleWorkers方法，注意，这个方法打断的是闲置Worker，打断闲置Worker之后，getTask方法会返回null，然后Worker会被回收。那什么是闲置Worker呢？ 闲置Worker是这样解释的：Worker运行的时候会去阻塞队列拿数据(getTask方法)，拿的时候如果没有设置超时时间，那么会一直阻塞等待阻塞队列进数据，这样的Worker就被称为闲置Worker。由于Worker也是一个AQS，在runWorker方法里会有一对lock和unlock操作，这对lock操作是为了确保Worker不是一个闲置Worker。 所以Worker被设计成一个AQS是为了根据Worker的锁来判断是否是闲置线程，是否可以被强制中断。 下面我们看下interruptIdleWorkers方法： // 调用他的一个重载方法，传入了参数false，表示要中断所有的正在运行的闲置Worker，如果为true表示只打断一个闲置Worker private void interruptIdleWorkers() { interruptIdleWorkers(false); } private void interruptIdleWorkers(boolean onlyOne) { final ReentrantLock mainLock = this.mainLock; mainLock.lock(); // 中断闲置Worker需要加锁，防止并发 try { for (Worker w : workers) { Thread t = w.thread; // 拿到worker中的线程 if (!t.isInterrupted() &amp;&amp; w.tryLock()) { // Worker中的线程没有被打断并且Worker可以获取锁，这里Worker能获取锁说明Worker是个闲置Worker，在阻塞队列里拿数据一直被阻塞，没有数据进来。如果没有获取到Worker锁，说明Worker还在执行任务，不进行中断(shutdown方法不会中断正在执行的任务) try { t.interrupt(); // 中断Worker线程 } catch (SecurityException ignore) { } finally { w.unlock(); // 释放Worker锁 } } if (onlyOne) // 如果只打断1个Worker的话，直接break退出，否则，遍历所有的Worker break; } } finally { mainLock.unlock(); // 解锁 } } shutdown方法将线程池状态改成SHUTDOWN，线程池还能继续处理阻塞队列里的任务，并且会回收一些闲置的Worker。但是shutdownNow方法不一样，它会把线程池状态改成STOP状态，这样不会处理阻塞队列里的任务，也不会处理新的任务： // shutdownNow方法会有返回值的，返回的是一个任务列表，而shutdown方法没有返回值 public List&lt;Runnable&gt; shutdownNow() { List&lt;Runnable&gt; tasks; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); // shutdownNow操作也需要加锁，防止并发 try { checkShutdownAccess(); // 检查关闭线程池的权限 advanceRunState(STOP); // 把线程池状态更新到STOP interruptWorkers(); // 中断Worker的运行 tasks = drainQueue(); } finally { mainLock.unlock(); // 解锁 } tryTerminate(); // 尝试结束线程池，上面已经分析过了 return tasks; } shutdownNow的中断和shutdown方法不一样，调用的是interruptWorkers方法： private void interruptWorkers() { final ReentrantLock mainLock = this.mainLock; mainLock.lock(); // 中断Worker需要加锁，防止并发 try { for (Worker w : workers) w.interruptIfStarted(); // 中断Worker的执行 } finally { mainLock.unlock(); // 解锁 } } Worker的interruptIfStarted方法中断Worker的执行： void interruptIfStarted() { Thread t; // Worker无论是否被持有锁，只要还没被中断，那就中断Worker if (getState() &gt;= 0 &amp;&amp; (t = thread) != null &amp;&amp; !t.isInterrupted()) { try { t.interrupt(); // 强行中断Worker的执行 } catch (SecurityException ignore) { } } } 线程池关闭总结： 线程池的关闭主要是两个方法，shutdown和shutdownNow方法。 shutdown方法会更新状态到SHUTDOWN，不会影响阻塞队列里任务的执行，但是不会执行新进来的任务。同时也会回收闲置的Worker，闲置Worker的定义上面已经说过了。 shutdownNow方法会更新状态到STOP，会影响阻塞队列的任务执行，也不会执行新进来的任务。同时会回收所有的Worker。","raw":null,"content":null,"categories":[{"name":"java","slug":"java","permalink":"http://fangjian0423.github.io/categories/java/"}],"tags":[{"name":"java","slug":"java","permalink":"http://fangjian0423.github.io/tags/java/"},{"name":"thread","slug":"thread","permalink":"http://fangjian0423.github.io/tags/thread/"}]},{"title":"Java可重入锁ReentrantLock分析","slug":"java-ReentrantLock-analysis","date":"2016-03-19T06:31:58.000Z","updated":"2016-05-10T14:59:25.000Z","comments":true,"path":"2016/03/19/java-ReentrantLock-analysis/","link":"","permalink":"http://fangjian0423.github.io/2016/03/19/java-ReentrantLock-analysis/","excerpt":"Java中的可重入锁ReentrantLock很常见，可以用它来代替内置锁synchronized，ReentrantLock是语法级别的锁，所以比内置锁更加灵活。","text":"Java中的可重入锁ReentrantLock很常见，可以用它来代替内置锁synchronized，ReentrantLock是语法级别的锁，所以比内置锁更加灵活。 下面这段代码是ReentrantLock的一个例子： class Context { private ReentrantLock lock = new ReentrantLock(); public void method() { lock.lock(); System.out.println(&quot;do atomic operation&quot;); try { Thread.sleep(3000l); } catch (InterruptedException e) { e.printStackTrace(); } finally { lock.unlock(); } } } class MyThread implements Runnable { private Context context; public MyThread(Context context) { this.context = context; } @Override public void run() { context.method(); } } main方法： public static void main(String[] args) { Context context = new Context(); ExecutorService executorService = Executors.newFixedThreadPool(5); for(int i = 0; i &lt; 5; i ++ ) { executorService.submit(new MyThread(context)); } } 输出结果，每隔3秒输出： do atomic operation 如果没有使用可重入锁的话，那么一次性输出5条 do atomic operation。 ReentrantLock中有3个内部类，分别是Sync、FairSync和NonfairSync。 Sync是一个继承AQS的抽象类，使用独占锁，复写了tryRelease方法。tryAcquire方法由它的两个FairSync(公平锁)和NonfairSync(非公平锁)实现。 AQS相关的内容可以参考文章末尾的参考资料，这篇文章写得非常棒。 ReentrantLock的lock方法使用sync的lock方法，Sync的lock方法是个抽象方法，由公平锁和非公平锁去实现。unlock方法直接使用AQS的release方法。所以说公平锁和非公平锁的释放锁过程是一样的，不一样的是获取锁过程。 先来看一下unlock方法，unlock方法调用的AQS的release方法，也就是调用了tryRelease方法，tryRelease方法调完之后恢复第一个挂起的线程： protected final boolean tryRelease(int releases) { int c = getState() - releases; // 释放 if (Thread.currentThread() != getExclusiveOwnerThread()) // 如果当前线程不是独占线程，直接抛出异常 throw new IllegalMonitorStateException(); boolean free = false; if (c == 0) { // 由于是可重入锁，需要判断是否全部释放了 free = true; setExclusiveOwnerThread(null); // 全部释放的话直接把独占线程设置为null } setState(c); return free; } // 恢复线程 public final boolean release(int arg) { if (tryRelease(arg)) { Node h = head; // 恢复第一个挂起的线程 if (h != null &amp;&amp; h.waitStatus != 0) unparkSuccessor(h); return true; } return false; } ReentrantLock的lock方法就是获取锁的方法，AQS中线程对锁的的竞争结果只有两种，要么获取到了锁；要么没有获取到锁，没有获取的锁线程被挂起等待被唤醒。 公平锁FairSync的lock方法： final void lock() { // acquire方法内部调用tryAcquire方法 // 公平锁的获取锁方法，对于没有获取到的线程，会按照队列的方式挂起线程 acquire(1); } protected final boolean tryAcquire(int acquires) { final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) { // 公平锁这里多了一个!hasQueuedPredecessors()判断，表示是否有线程在队列里等待的时间比当前线程要长，如果有等待时间更长的线程，那么放弃获取锁 if (!hasQueuedPredecessors() &amp;&amp; compareAndSetState(0, acquires)) { setExclusiveOwnerThread(current); return true; } } else if (current == getExclusiveOwnerThread()) { int nextc = c + acquires; if (nextc &lt; 0) throw new Error(&quot;Maximum lock count exceeded&quot;); setState(nextc); return true; } return false; } 非公平锁NonfairSync的lock方法： final void lock() { // 非公平锁的获取锁 // 跟公平锁的区别就在这里。直接对状态位state进行cas操作，成功就获取锁，这是一种抢占式的方式。不成功跟公平锁一样进入队列挂起线程 if (compareAndSetState(0, 1)) setExclusiveOwnerThread(Thread.currentThread()); else acquire(1); } // 调用Sync的nonfairTryAcquire方法 protected final boolean tryAcquire(int acquires) { return nonfairTryAcquire(acquires); } final boolean nonfairTryAcquire(int acquires) { final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) { if (compareAndSetState(0, acquires)) { setExclusiveOwnerThread(current); return true; } } else if (current == getExclusiveOwnerThread()) { int nextc = c + acquires; if (nextc &lt; 0) // overflow throw new Error(&quot;Maximum lock count exceeded&quot;); setState(nextc); return true; } return false; } 如上述源码和注释所说，公平锁和非公平锁的最主要区别就是获取锁的方式不一样。 公平锁获取锁的时候，首先先读取状态位state，然后再做判断，之后使用cas设置状态位。能获取锁的线程就获取锁，不能获取锁的线程被挂起进入队列。之后再来的线程的等待时间没有已经在队列里的线程等待时间长，所以会一直进入等待队列。 公平锁类似于排队买火车票一样，后面来的人没有前面来的人等待时间长，会一直在队尾被加入到队列里。 非公平锁获取锁的时候，立马就使用cas判断设置状态位，是一种抢占式的方式。同时非公平锁也没有等待时间长的线程会优先获取锁这个概念。非公平锁类似吃饭排队，但是总会有那么几个人试图插队。 公平锁和非公平锁的还有另外一个差别，前面已经分析过了。就是公平锁获取锁多了一个判断条件，当前线程的等待时间没有队列里的线程等待时间长的话，不能获取锁；而非公平锁没有这个条件。 ReentrantLock的默认构造函数使用的是NonfairSync，如果想使用FairSync，使用带有boolean参数的构造函数，传入true表示FairSync，否则是NonfairSync。 ReentrantLock内部还提供了一些有用的方法： hasQueuedThreads： 查询是否有线程在等待队列里hasQueuedThread(Thread thread)：查询线程是否在等待队列里isHeldByCurrentThread：当前线程是否持有锁getQueueLength：队列中的挂起线程个数 等等还有其他的一些有用方法。 总结： ReentrantLock可重入锁内部有3个类，Sync、FairSync和NonfairSync。 Sync是一个继承AQS的抽象类，并发的控制就是通过Sync实现的(当然是使用AQS实现的，AQS是Java并发包的一个同步基础类)，它复写了tryRelease方法，它有2个子类FairSync和NonfairSync，也就是公平锁和非公平锁。 由于Sync复写了tryRelease方法，它的2个子类公平锁和非公平锁没有再次复写这个方法，所以公平锁和非公平锁的释放锁操作是一样的，释放锁也就是唤醒等待队列中的第一个被挂起的线程。 虽然公平锁和非公平锁的释放锁方式一样，但是它们的获取锁方式不一样，公平锁获取锁的时候，如果1个线程获取到了锁，其他线程都会被挂起并且进入等待队列，后面来的线程的等待时间没有队列里的线程等待时间长的话，那么就放弃获取锁，进入等待队列。非公平锁获取锁的方式是一种抢占式的方式，不考虑等待时间的问题，无论哪个线程获取到了锁，其他线程就进入等待队列。 参考资料： http://ifeve.com/jdk1-8-abstractqueuedsynchronizer/","raw":null,"content":null,"categories":[{"name":"java","slug":"java","permalink":"http://fangjian0423.github.io/categories/java/"}],"tags":[{"name":"java","slug":"java","permalink":"http://fangjian0423.github.io/tags/java/"},{"name":"thread","slug":"thread","permalink":"http://fangjian0423.github.io/tags/thread/"}]},{"title":"Java AtomicInteger原理分析","slug":"java-AtomicInteger-analysis","date":"2016-03-16T12:32:35.000Z","updated":"2016-05-10T14:59:25.000Z","comments":true,"path":"2016/03/16/java-AtomicInteger-analysis/","link":"","permalink":"http://fangjian0423.github.io/2016/03/16/java-AtomicInteger-analysis/","excerpt":"Java中的AtomicInteger大家应该很熟悉，它是为了解决多线程访问Integer变量导致结果不正确所设计的一个基于多线程并且支持原子操作的Integer类。\n","text":"Java中的AtomicInteger大家应该很熟悉，它是为了解决多线程访问Integer变量导致结果不正确所设计的一个基于多线程并且支持原子操作的Integer类。 它的使用也非常简单： AtomicInteger ai = new AtomicInteger(0); ai.addAndGet(5); // 5 ai.getAndAdd(1); // 5 ai.get(); // 6 AtomicInteger内部有一个变量UnSafe： private static final Unsafe unsafe = Unsafe.getUnsafe(); Unsafe类是一个可以执行不安全、容易犯错的操作的一个特殊类。虽然Unsafe类中所有方法都是public的，但是这个类只能在一些被信任的代码中使用。Unsafe的源码可以在这里看 -&gt; UnSafe源码。 Unsafe类可以执行以下几种操作： 分配内存，释放内存：在方法allocateMemory，reallocateMemory，freeMemory中，有点类似c中的malloc，free方法 可以定位对象的属性在内存中的位置，可以修改对象的属性值。使用objectFieldOffset方法 挂起和恢复线程，被封装在LockSupport类中供使用 CAS操作(CompareAndSwap，比较并交换，是一个原子操作) AtomicInteger中用的就是Unsafe的CAS操作。 Unsafe中的int类型的CAS操作方法： public final native boolean compareAndSwapInt(Object o, long offset, int expected, int x); 参数o就是要进行cas操作的对象，offset参数是内存位置，expected参数就是期望的值，x参数是需要更新到的值。 也就是说，如果我把1这个数字属性更新到2的话，需要这样调用： compareAndSwapInt(this, valueOffset, 1, 2) valueOffset字段表示内存位置，可以在AtomicInteger对象中使用unsafe得到： static { try { valueOffset = unsafe.objectFieldOffset (AtomicInteger.class.getDeclaredField(&quot;value&quot;)); } catch (Exception ex) { throw new Error(ex); } } AtomicInteger内部使用变量value表示当前的整型值，这个整型变量还是volatile的，表示内存可见性，一个线程修改value之后保证对其他线程的可见性： private volatile int value; AtomicInteger内部还封装了一下CAS，定义了一个compareAndSet方法，只需要2个参数： public final boolean compareAndSet(int expect, int update) { return unsafe.compareAndSwapInt(this, valueOffset, expect, update); } addAndGet方法，addAndGet方法内部使用一个死循环，先得到当前的值value，然后再把当前的值加一，加完之后使用cas原子操作让当前值加一处理正确。当然cas原子操作不一定是成功的，所以做了一个死循环，当cas操作成功的时候返回数据。这里由于使用了cas原子操作，所以不会出现多线程处理错误的问题。比如线程A得到current为1，线程B也得到current为1；线程A的next值为2，进行cas操作并且成功的时候，将value修改成了2；这个时候线程B也得到next值为2，当进行cas操作的时候由于expected值已经是2，而不是1了；所以cas操作会失败，下一次循环的时候得到的current就变成了2；也就不会出现多线程处理问题了： public final int addAndGet(int delta) { for (;;) { int current = get(); int next = current + delta; if (compareAndSet(current, next)) return next; } } incrementAndGet方法，跟addAndGet方法类似，只不过next值变成了current+1： public final int incrementAndGet() { for (;;) { int current = get(); int next = current + 1; if (compareAndSet(current, next)) return next; } } getAndAdd方法，跟addAndGet方法一样，返回值变成了current： public final int getAndAdd(int delta) { for (;;) { int current = get(); int next = current + delta; if (compareAndSet(current, next)) return current; } } 缺点： 虽然AtomicInteger中的cas操作可以实现非阻塞的原子操作，但是会产生ABA问题， 参考资料： http://blog.csdn.net/ghsau/article/details/38471987 http://blog.csdn.net/aesop_wubo/article/details/7537278 http://ifeve.com/sun-misc-unsafe/ http://ifeve.com/java-atomic/","raw":null,"content":null,"categories":[{"name":"java","slug":"java","permalink":"http://fangjian0423.github.io/categories/java/"}],"tags":[{"name":"java","slug":"java","permalink":"http://fangjian0423.github.io/tags/java/"}]},{"title":"Java根类Object的方法说明","slug":"java-Object-method","date":"2016-03-12T09:44:59.000Z","updated":"2016-05-10T14:59:25.000Z","comments":true,"path":"2016/03/12/java-Object-method/","link":"","permalink":"http://fangjian0423.github.io/2016/03/12/java-Object-method/","excerpt":"","text":"Java中的Object类是所有类的父类，它提供了以下11个方法： public final native Class&lt;?&gt; getClass() public native int hashCode() public boolean equals(Object obj) protected native Object clone() throws CloneNotSupportedException public String toString() public final native void notify() public final native void notifyAll() public final native void wait(long timeout) throws InterruptedException public final void wait(long timeout, int nanos) throws InterruptedException public final void wait() throws InterruptedException protected void finalize() throws Throwable { } 下面我们一个个方法进行分析，看这些方法到底有什么作用： getClass方法getClass方法是一个final方法，不允许子类重写，并且也是一个native方法。 返回当前运行时对象的Class对象，注意这里是运行时，比如以下代码中n是一个Number类型的实例，但是java中数值默认是Integer类型，所以getClass方法返回的是java.lang.Integer： &quot;str&quot;.getClass() // class java.lang.String &quot;str&quot;.getClass == String.class // true Number n = 0; Class&lt;? extends Number&gt; c = n.getClass(); // class java.lang.Integer hashCode方法hashCode方法也是一个native方法。 该方法返回对象的哈希码，主要使用在哈希表中，比如JDK中的HashMap。 哈希码的通用约定如下： 在java程序执行过程中，在一个对象没有被改变的前提下，无论这个对象被调用多少次，hashCode方法都会返回相同的整数值。对象的哈希码没有必要在不同的程序中保持相同的值。 如果2个对象使用equals方法进行比较并且相同的话，那么这2个对象的hashCode方法的值也必须相等。 如果根据equals方法，得到两个对象不相等，那么这2个对象的hashCode值不需要必须不相同。但是，不相等的对象的hashCode值不同的话可以提高哈希表的性能。 通常情况下，不同的对象产生的哈希码是不同的。默认情况下，对象的哈希码是通过将该对象的内部地址转换成一个整数来实现的。 String的hashCode方法实现如下， 计算方法是 s[0]31^(n-1) + s[1]31^(n-2) + … + s[n-1]，其中s[0]表示字符串的第一个字符，n表示字符串长度： public int hashCode() { int h = hash; if (h == 0 &amp;&amp; value.length &gt; 0) { char val[] = value; for (int i = 0; i &lt; value.length; i++) { h = 31 * h + val[i]; } hash = h; } return h; } 比如”fo”的hashCode = 102 31^1 + 111 = 3273， “foo”的hashCode = 102 31^2 + 111 * 31^1 + 111 = 101574 (‘f’的ascii码为102, ‘o’的ascii码为111) hashCode在哈希表HashMap中的应用： // Student类，只重写了hashCode方法 public static class Student { private String name; private int age; public Student(String name, int age) { this.name = name; this.age = age; } @Override public int hashCode() { return name.hashCode(); } } Map&lt;Student, String&gt; map = new HashMap&lt;Student, String&gt;(); Student stu1 = new Student(&quot;fo&quot;, 11); Student stu2 = new Student(&quot;fo&quot;, 22); map.put(stu1, &quot;fo&quot;); map.put(stu2, &quot;fo&quot;); 上面这段代码中，map中有2个元素stu1和stu2。但是这2个元素是在哈希表中的同一个数组项中的位置，也就是在同一串链表中。 但是为什么stu1和stu2的hashCode相同，但是两条元素都插到map里了，这是因为map判断重复数据的条件是 两个对象的哈希码相同并且(两个对象是同一个对象或者两个对象相等[equals为true])。 所以再给Student重写equals方法，并且只比较name的话，这样map就只有1个元素了。 @Override public boolean equals(Object o) { if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; Student student = (Student) o; return this.name.equals(student.name); } 这个例子直接说明了hashCode中通用约定的第三点： 第三点：如果根据equals方法，得到两个对象不相等，那么这2个对象的hashCode值不需要必须不相同。但是，不相等的对象的hashCode值不同的话可以提高哈希表的性能。 –&gt; 上面例子一开始没有重写equals方法，导致两个对象不相等，但是这两个对象的hashCode值一样，所以导致这两个对象在同一串链表中，影响性能。 当然，还有第三种情况，那就是equals方法相等，但是hashCode的值不相等。 这种情况也就是违反了通用约定的第二点： 第二点：如果2个对象使用equals方法进行比较并且相同的话，那么这2个对象的hashCode方法的值也必须相等。 违反这一点产生的后果就是如果一个stu1实例是Student(“fo”, 11)，stu2实例是Student(“fo”, 11)，那么这2个实例是相等的，但是他们的hashCode不一样，这样是导致哈希表中都会存入stu1实例和stu2实例，但是实际情况下，stu1和stu2是重复数据，只允许存在一条数据在哈希表中。所以这一点是非常重点的，再强调一下：如果2个对象的equals方法相等，那么他们的hashCode值也必须相等，反之，如果2个对象hashCode值相等，但是equals不相等，这样会影响性能，所以还是建议2个方法都一起重写。 equals方法比较两个对象是否相等。Object类的默认实现，即比较2个对象的内存地址是否相等： public boolean equals(Object obj) { return (this == obj); } equals方法在非空对象引用上的特性： reflexive，自反性。任何非空引用值x，对于x.equals(x)必须返回true symmetric，对称性。任何非空引用值x和y，如果x.equals(y)为true，那么y.equals(x)也必须为true transitive，传递性。任何非空引用值x、y和z，如果x.equals(y)为true并且y.equals(z)为true，那么x.equals(z)也必定为true consistent，一致性。任何非空引用值x和y，多次调用 x.equals(y) 始终返回 true 或始终返回 false，前提是对象上 equals 比较中所用的信息没有被修改 对于任何非空引用值 x，x.equals(null) 都应返回 false Object类的equals方法对于任何非空引用值x和y，当x和y引用同一个对象时，此方法才返回true。这个也就是我们常说的地址相等。 注意点：如果重写了equals方法，通常有必要重写hashCode方法，这点已经在hashCode方法中说明了。 clone方法创建并返回当前对象的一份拷贝。一般情况下，对于任何对象 x，表达式 x.clone() != x 为true，x.clone().getClass() == x.getClass() 也为true。 Object类的clone方法是一个protected的native方法。 由于Object本身没有实现Cloneable接口，所以不重写clone方法并且进行调用的话会发生CloneNotSupportedException异常。 toString方法Object对象的默认实现，即输出类的名字@实例的哈希码的16进制： public String toString() { return getClass().getName() + &quot;@&quot; + Integer.toHexString(hashCode()); } toString方法的结果应该是一个简明但易于读懂的字符串。建议Object所有的子类都重写这个方法。 notify方法notify方法是一个native方法，并且也是final的，不允许子类重写。 唤醒一个在此对象监视器上等待的线程(监视器相当于就是锁的概念)。如果所有的线程都在此对象上等待，那么只会选择一个线程。选择是任意性的，并在对实现做出决定时发生。一个线程在对象监视器上等待可以调用wait方法。 直到当前线程放弃对象上的锁之后，被唤醒的线程才可以继续处理。被唤醒的线程将以常规方式与在该对象上主动同步的其他所有线程进行竞争。例如，唤醒的线程在作为锁定此对象的下一个线程方面没有可靠的特权或劣势。 notify方法只能被作为此对象监视器的所有者的线程来调用。一个线程要想成为对象监视器的所有者，可以使用以下3种方法： 执行对象的同步实例方法 使用synchronized内置锁 对于Class类型的对象，执行同步静态方法 一次只能有一个线程拥有对象的监视器。 如果当前线程不是此对象监视器的所有者的话会抛出IllegalMonitorStateException异常 注意点： 因为notify只能在拥有对象监视器的所有者线程中调用，否则会抛出IllegalMonitorStateException异常 notifyAll方法跟notify一样，唯一的区别就是会唤醒在此对象监视器上等待的所有线程，而不是一个线程。 同样，如果当前线程不是对象监视器的所有者，那么调用notifyAll同样会发生IllegalMonitorStateException异常。 以下这段代码直接调用notify或者notifyAll方法会发生IllegalMonitorStateException异常，这是因为调用这两个方法需要当前线程是对象监视器的所有者： Factory factory = new Factory(); factory.notify(); factory.notifyAll(); wait(long timeout) throws InterruptedException方法wait(long timeout)方法同样是一个native方法，并且也是final的，不允许子类重写。 wait方法会让当前线程等待直到另外一个线程调用对象的notify或notifyAll方法，或者超过参数设置的timeout超时时间。 跟notify和notifyAll方法一样，当前线程必须是此对象的监视器所有者，否则还是会发生IllegalMonitorStateException异常。 wait方法会让当前线程(我们先叫做线程T)将其自身放置在对象的等待集中，并且放弃该对象上的所有同步要求。出于线程调度目的，线程T是不可用并处于休眠状态，直到发生以下四件事中的任意一件： 其他某个线程调用此对象的notify方法，并且线程T碰巧被任选为被唤醒的线程 其他某个线程调用此对象的notifyAll方法 其他某个线程调用Thread.interrupt方法中断线程T 时间到了参数设置的超时时间。如果timeout参数为0，则不会超时，会一直进行等待 所以可以理解wait方法相当于放弃了当前线程对对象监视器的所有者(也就是说释放了对象的锁) 之后，线程T会被等待集中被移除，并且重新进行线程调度。然后，该线程以常规方式与其他线程竞争，以获得在该对象上同步的权利；一旦获得对该对象的控制权，该对象上的所有其同步声明都将被恢复到以前的状态，这就是调用wait方法时的情况。然后，线程T从wait方法的调用中返回。所以，从wait方法返回时，该对象和线程T的同步状态与调用wait方法时的情况完全相同。 在没有被通知、中断或超时的情况下，线程还可以唤醒一个所谓的虚假唤醒 (spurious wakeup)。虽然这种情况在实践中很少发生，但是应用程序必须通过以下方式防止其发生，即对应该导致该线程被提醒的条件进行测试，如果不满足该条件，则继续等待。换句话说，等待应总是发生在循环中，如下面的示例： synchronized (obj) { while (&lt;condition does not hold&gt;) obj.wait(timeout); ... // Perform action appropriate to condition } 如果当前线程在等待之前或在等待时被任何线程中断，则会抛出InterruptedException异常。在按上述形式恢复此对象的锁定状态时才会抛出此异常。 wait(long timeout, int nanos) throws InterruptedException方法跟wait(long timeout)方法类似，多了一个nanos参数，这个参数表示额外时间（以毫微秒为单位，范围是 0-999999）。 所以超时的时间还需要加上nanos毫秒。 需要注意的是 wait(0, 0)和wait(0)效果是一样的，即一直等待。 wait() throws InterruptedException方法跟之前的2个wait方法一样，只不过该方法一直等待，没有超时时间这个概念。 以下这段代码直接调用wait方法会发生IllegalMonitorStateException异常，这是因为调用wait方法需要当前线程是对象监视器的所有者： Factory factory = new Factory(); factory.wait(); 一般情况下，wait方法和notify方法会一起使用的，wait方法阻塞当前线程，notify方法唤醒当前线程，一个使用wait和notify方法的生产者消费者例子代码如下： public class WaitNotifyTest { public static void main(String[] args) { Factory factory = new Factory(); new Thread(new Producer(factory, 5)).start(); new Thread(new Producer(factory, 5)).start(); new Thread(new Producer(factory, 20)).start(); new Thread(new Producer(factory, 30)).start(); new Thread(new Consumer(factory, 10)).start(); new Thread(new Consumer(factory, 20)).start(); new Thread(new Consumer(factory, 5)).start(); new Thread(new Consumer(factory, 5)).start(); new Thread(new Consumer(factory, 20)).start(); } } class Factory { public static final Integer MAX_NUM = 50; private int currentNum = 0; public void consume(int num) throws InterruptedException { synchronized (this) { while(currentNum - num &lt; 0) { this.wait(); } currentNum -= num; System.out.println(&quot;consume &quot; + num + &quot;, left: &quot; + currentNum); this.notifyAll(); } } public void produce(int num) throws InterruptedException { synchronized (this) { while(currentNum + num &gt; MAX_NUM) { this.wait(); } currentNum += num; System.out.println(&quot;produce &quot; + num + &quot;, left: &quot; + currentNum); this.notifyAll(); } } } class Producer implements Runnable { private Factory factory; private int num; public Producer(Factory factory, int num) { this.factory = factory; this.num = num; } @Override public void run() { try { factory.produce(num); } catch (InterruptedException e) { e.printStackTrace(); } } } class Consumer implements Runnable { private Factory factory; private int num; public Consumer(Factory factory, int num) { this.factory = factory; this.num = num; } @Override public void run() { try { factory.consume(num); } catch (InterruptedException e) { e.printStackTrace(); } } } 注意的是Factory类的produce和consume方法都将Factory实例锁住了，锁住之后线程就成为了对象监视器的所有者，然后才能调用wait和notify方法。 输出： produce 5, left: 5 produce 20, left: 25 produce 5, left: 30 consume 10, left: 20 produce 30, left: 50 consume 20, left: 30 consume 5, left: 25 consume 5, left: 20 consume 20, left: 0 finalize方法finalize方法是一个protected方法，Object类的默认实现是不进行任何操作。 该方法的作用是实例被垃圾回收器回收的时候触发的操作，就好比 “死前的最后一波挣扎”。 直接写个弱引用例子： Car car = new Car(9999, &quot;black&quot;); WeakReference&lt;Car&gt; carWeakReference = new WeakReference&lt;Car&gt;(car); int i = 0; while(true) { if(carWeakReference.get() != null) { i++; System.out.println(&quot;Object is alive for &quot;+i+&quot; loops - &quot;+carWeakReference); } else { System.out.println(&quot;Object has been collected.&quot;); break; } } class Car { private double price; private String colour; public Car(double price, String colour){ this.price = price; this.colour = colour; } // get set method @Override protected void finalize() throws Throwable { System.out.println(&quot;i will be destroyed&quot;); } } 输出： .... Object is alive for 26417 loops - java.lang.ref.WeakReference@7c2f1622 Object is alive for 26418 loops - java.lang.ref.WeakReference@7c2f1622 Object is alive for 26419 loops - java.lang.ref.WeakReference@7c2f1622 Object is alive for 26420 loops - java.lang.ref.WeakReference@7c2f1622 Object is alive for 26421 loops - java.lang.ref.WeakReference@7c2f1622 Object is alive for 26422 loops - java.lang.ref.WeakReference@7c2f1622 Object has been collected. i will be destroyed","raw":null,"content":null,"categories":[{"name":"java","slug":"java","permalink":"http://fangjian0423.github.io/categories/java/"}],"tags":[{"name":"java","slug":"java","permalink":"http://fangjian0423.github.io/tags/java/"},{"name":"jdk","slug":"jdk","permalink":"http://fangjian0423.github.io/tags/jdk/"}]},{"title":"Avro介绍","slug":"avro-intro","date":"2016-02-20T17:23:22.000Z","updated":"2016-05-10T14:59:25.000Z","comments":true,"path":"2016/02/21/avro-intro/","link":"","permalink":"http://fangjian0423.github.io/2016/02/21/avro-intro/","excerpt":"","text":"Avro介绍Apache Avro是一个数据序列化系统。 Avro所提供的属性： 1.丰富的数据结构2.使用快速的压缩二进制数据格式3.提供容器文件用于持久化数据4.远程过程调用RPC5.简单的动态语言结合功能，Avro 和动态语言结合后，读写数据文件和使用 RPC 协议都不需要生成代码，而代码生成作为一种可选的优化只值得在静态类型语言中实现。 Avro的SchemaAvro的Schema用JSON表示。Schema定义了简单数据类型和复杂数据类型。 基本类型其中简单数据类型有以下8种： 类型 含义 null 没有值 boolean 布尔值 int 32位有符号整数 long 64位有符号整数 float 单精度（32位）的IEEE 754浮点数 double 双精度（64位）的IEEE 754浮点数 bytes 8位无符号字节序列 string 字符串 基本类型没有属性，基本类型的名字也就是类型的名字，比如： {&quot;type&quot;: &quot;string&quot;} 复杂类型Avro提供了6种复杂类型。分别是Record，Enum，Array，Map，Union和Fixed。 RecordRecord类型使用的类型名字是 “record”，还支持其它属性的设置： name：record类型的名字(必填) namespace：命名空间(可选) doc：这个类型的文档说明(可选) aliases：record类型的别名，是个字符串数组(可选) fields：record类型中的字段，是个对象数组(必填)。每个字段需要以下属性： name：字段名字(必填) doc：字段说明文档(可选) type：一个schema的json对象或者一个类型名字(必填) default：默认值(可选) order：排序(可选)，只有3个值ascending(默认)，descending或ignore aliases：别名，字符串数组(可选) 一个Record类型例子，定义一个元素类型是Long的链表： { &quot;type&quot;: &quot;record&quot;, &quot;name&quot;: &quot;LongList&quot;, &quot;aliases&quot;: [&quot;LinkedLongs&quot;], // old name for this &quot;fields&quot; : [ {&quot;name&quot;: &quot;value&quot;, &quot;type&quot;: &quot;long&quot;}, // each element has a long {&quot;name&quot;: &quot;next&quot;, &quot;type&quot;: [&quot;null&quot;, &quot;LongList&quot;]} // optional next element ] } Enum枚举类型的类型名字是”enum”，还支持其它属性的设置： name：枚举类型的名字(必填)namespace：命名空间(可选)aliases：字符串数组，别名(可选)doc：说明文档(可选)symbols：字符串数组，所有的枚举值(必填)，不允许重复数据。 一个枚举类型的例子： { &quot;type&quot;: &quot;enum&quot;, &quot;name&quot;: &quot;Suit&quot;, &quot;symbols&quot; : [&quot;SPADES&quot;, &quot;HEARTS&quot;, &quot;DIAMONDS&quot;, &quot;CLUBS&quot;] } Array数组类型的类型名字是”array”并且只支持一个属性： items：数组元素的schema 一个数组例子： {&quot;type&quot;: &quot;array&quot;, &quot;items&quot;: &quot;string&quot;} MapMap类型的类型名字是”map”并且只支持一个属性： values：map值的schema Map的key必须是字符串。 一个Map例子： {&quot;type&quot;: &quot;map&quot;, &quot;values&quot;: &quot;long&quot;} Union组合类型，表示各种类型的组合，使用数组进行组合。比如[“null”, “string”]表示类型可以为null或者string。 组合类型的默认值是看组合类型的第一个元素，因此如果一个组合类型包括null类型，那么null类型一般都会放在第一个位置，这样子的话这个组合类型的默认值就是null。 组合类型中不允许同一种类型的元素的个数不会超过1个，除了record，fixed和enum。比如组合类中有2个array类型或者2个map类型，这是不允许的。 组合类型不允许嵌套组合类型。 Fixed混合类型的类型名字是fixed，支持以下属性： name：名字(必填)namespace：命名空间(可选)aliases：字符串数组，别名(可选)size：一个整数，表示每个值的字节数(必填) 比如16个字节数的fixed类型例子如下： {&quot;type&quot;: &quot;fixed&quot;, &quot;size&quot;: 16, &quot;name&quot;: &quot;md5&quot;} 1个Avro例子首先定义一个User的schema： { &quot;namespace&quot;: &quot;example.avro&quot;, &quot;type&quot;: &quot;record&quot;, &quot;name&quot;: &quot;User&quot;, &quot;fields&quot;: [ {&quot;name&quot;: &quot;name&quot;, &quot;type&quot;: &quot;string&quot;}, {&quot;name&quot;: &quot;favorite_number&quot;, &quot;type&quot;: &quot;int&quot;}, {&quot;name&quot;: &quot;favorite_color&quot;, &quot;type&quot;: &quot;string&quot;} ] } User有3个属性，分别是name，favorite_number和favorite_color。 json文件内容： {&quot;name&quot;:&quot;format&quot;,&quot;favorite_number&quot;:1,&quot;favorite_color&quot;:&quot;red&quot;} {&quot;name&quot;:&quot;format2&quot;,&quot;favorite_number&quot;:2,&quot;favorite_color&quot;:&quot;black&quot;} {&quot;name&quot;:&quot;format3&quot;,&quot;favorite_number&quot;:666,&quot;favorite_color&quot;:&quot;blue&quot;} 使用avro工具将json文件转换成avro文件： java -jar avro-tools-1.8.0.jar fromjson --schema-file user.avsc user.json &gt; user.avro 可以设置压缩格式： java -jar avro-tools-1.8.0.jar fromjson --codec snappy --schema-file user.avsc user.json &gt; user2.avro 将avro文件反转换成json文件： java -jar avro-tools-1.8.0.jar tojson user.avro java -jar avro-tools-1.8.0.jar --pretty tojson user.avro 得到avro文件的meta： java -jar avro-tools-1.8.0.jar getmeta user.avro 输出： avro.codec null avro.schema {&quot;type&quot;:&quot;record&quot;,&quot;name&quot;:&quot;User&quot;,&quot;namespace&quot;:&quot;example.avro&quot;,&quot;fields&quot;:[{&quot;name&quot;:&quot;name&quot;,&quot;type&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;favorite_number&quot;,&quot;type&quot;:&quot;int&quot;},{&quot;name&quot;:&quot;favorite_color&quot;,&quot;type&quot;:&quot;string&quot;}]} 得到avro文件的schema： java -jar avro-tools-1.8.0.jar getschema user.avro 将文本文件转换成avro文件： java -jar avro-tools-1.8.0.jar fromtext user.txt usertxt.avro Avro使用生成的代码进行序列化和反序列化以上面一个例子的schema为例讲解。 Avro可以根据schema自动生成对应的类： java -jar /path/to/avro-tools-1.8.0.jar compile schema user.avsc . user.avsc的namespace为example.avro，name为User。最终在当前目录生成的example/avro目录下有个User.java文件。 ├── example │ └── avro │ └── User.java 使用Avro生成的代码创建User： User user1 = new User(); user1.setName(&quot;Format&quot;); user1.setFavoriteColor(&quot;red&quot;); user1.setFavoriteNumber(666); User user2 = new User(&quot;Format2&quot;, 66, &quot;blue&quot;); User user3 = User.newBuilder() .setName(&quot;Format3&quot;) .setFavoriteNumber(6) .setFavoriteColor(&quot;black&quot;).build(); 可以使用有参的构造函数和无参的构造函数，也可以使用Builder构造User。 序列化： DatumWrite接口用来把java对象转换成内存中的序列化格式，SpecificDatumWriter用来生成类并且指定生成的类型。 最后使用DataFileWriter来进行具体的序列化，create方法指定文件和schema信息，append方法用来写数据，最后写完后close文件。 DatumWriter&lt;User&gt; userDatumWriter = new SpecificDatumWriter&lt;User&gt;(User.class); DataFileWriter&lt;User&gt; dataFileWriter = new DataFileWriter&lt;User&gt;(userDatumWriter); dataFileWriter.create(user1.getSchema(), new File(&quot;users.avro&quot;)); dataFileWriter.append(user1); dataFileWriter.append(user2); dataFileWriter.append(user3); dataFileWriter.close(); 反序列化： 反序列化跟序列化很像，相应的Writer换成Reader。这里只创建一个User对象是为了性能优化，每次都重用这个User对象，如果文件量很大，对象分配和垃圾收集处理的代价很昂贵。如果不考虑性能，可以使用 for (User user : dataFileReader) 循环遍历对象 File file = new File(&quot;users.avro&quot;); DatumReader&lt;User&gt; userDatumReader = new SpecificDatumReader&lt;User&gt;(User.class); DataFileReader&lt;User&gt; dataFileReader = new DataFileReader&lt;User&gt;(file, userDatumReader); User user = null; while(dataFileReader.hasNext()) { user = dataFileReader.next(user); System.out.println(user); } 打印出： {&quot;name&quot;: &quot;Format&quot;, &quot;favorite_number&quot;: 666, &quot;favorite_color&quot;: &quot;red&quot;} {&quot;name&quot;: &quot;Format2&quot;, &quot;favorite_number&quot;: 66, &quot;favorite_color&quot;: &quot;blue&quot;} {&quot;name&quot;: &quot;Format3&quot;, &quot;favorite_number&quot;: 6, &quot;favorite_color&quot;: &quot;black&quot;} Avro不使用生成的代码进行序列化和反序列化虽然Avro为我们提供了根据schema自动生成类的方法，我们也可以自己创建类，不使用Avro的自动生成工具。 创建User： 首先使用Parser读取schema信息并且创建Schema类： Schema schema = new Schema.Parser().parse(new File(&quot;user.avsc&quot;)); 有了Schema之后可以创建record： GenericRecord user1 = new GenericData.Record(schema); user1.put(&quot;name&quot;, &quot;Format&quot;); user1.put(&quot;favorite_number&quot;, 666); user1.put(&quot;favorite_color&quot;, &quot;red&quot;); GenericRecord user2 = new GenericData.Record(schema); user2.put(&quot;name&quot;, &quot;Format2&quot;); user2.put(&quot;favorite_number&quot;, 66); user2.put(&quot;favorite_color&quot;, &quot;blue&quot;); 使用GenericRecord表示User，GenericRecord会根据schema验证字段是否正确，如果put进了不存在的字段 user1.put(“favorite_animal”, “cat”) ，那么运行的时候会得到AvroRuntimeException异常。 序列化： 序列化跟生成的User类似，只不过schema是自己构造的，不是User中拿的。 Schema schema = new Schema.Parser().parse(new File(&quot;user.avsc&quot;)); GenericRecord user1 = new GenericData.Record(schema); user1.put(&quot;name&quot;, &quot;Format&quot;); user1.put(&quot;favorite_number&quot;, 666); user1.put(&quot;favorite_color&quot;, &quot;red&quot;); GenericRecord user2 = new GenericData.Record(schema); user2.put(&quot;name&quot;, &quot;Format2&quot;); user2.put(&quot;favorite_number&quot;, 66); user2.put(&quot;favorite_color&quot;, &quot;blue&quot;); DatumWriter&lt;GenericRecord&gt; datumWriter = new SpecificDatumWriter&lt;GenericRecord&gt;(schema); DataFileWriter&lt;GenericRecord&gt; dataFileWriter = new DataFileWriter&lt;GenericRecord&gt;(datumWriter); dataFileWriter.create(schema, new File(&quot;users2.avro&quot;)); dataFileWriter.append(user1); dataFileWriter.append(user2); dataFileWriter.close(); 反序列化： 反序列化跟生成的User类似，只不过schema是自己构造的，不是User中拿的。 Schema schema = new Schema.Parser().parse(new File(&quot;user.avsc&quot;)); File file = new File(&quot;users2.avro&quot;); DatumReader&lt;GenericRecord&gt; datumReader = new SpecificDatumReader&lt;GenericRecord&gt;(schema); DataFileReader&lt;GenericRecord&gt; dataFileReader = new DataFileReader&lt;GenericRecord&gt;(file, datumReader); GenericRecord user = null; while(dataFileReader.hasNext()) { user = dataFileReader.next(user); System.out.println(user); } 打印出： {&quot;name&quot;: &quot;Format&quot;, &quot;favorite_number&quot;: 666, &quot;favorite_color&quot;: &quot;red&quot;} {&quot;name&quot;: &quot;Format2&quot;, &quot;favorite_number&quot;: 66, &quot;favorite_color&quot;: &quot;blue&quot;} 一些注意点Avro解析json文件的时候，如果类型是Record并且里面有字段是union并且允许空值的话，需要进行转换。因为[“bytes”, “string”]和[“int”,”long”]这2个union类型在json中是有歧义的，第一个union在json中都会被转换成string类型，第二个union在json中都会被转换成数字类型。 所以如果json值的null的话，在avro提供的json中直接写null，否则使用只有一个键值对的对象，键是类型，值的具体的值。 比如： { &quot;namespace&quot;: &quot;example.avro&quot;, &quot;type&quot;: &quot;record&quot;, &quot;name&quot;: &quot;User&quot;, &quot;fields&quot;: [ {&quot;name&quot;: &quot;name&quot;, &quot;type&quot;: &quot;string&quot;}, {&quot;name&quot;: &quot;favorite_number&quot;, &quot;type&quot;: [&quot;int&quot;,&quot;null&quot;]}, {&quot;name&quot;: &quot;favorite_color&quot;, &quot;type&quot;: [&quot;string&quot;,&quot;null&quot;]} ] } 在要转换成json文件的时候要写成这样： {&quot;name&quot;:&quot;format&quot;,&quot;favorite_number&quot;:{&quot;int&quot;:1},&quot;favorite_color&quot;:{&quot;string&quot;:&quot;red&quot;}} {&quot;name&quot;:&quot;format2&quot;,&quot;favorite_number&quot;:null,&quot;favorite_color&quot;:{&quot;string&quot;:&quot;black&quot;}} {&quot;name&quot;:&quot;format3&quot;,&quot;favorite_number&quot;:{&quot;int&quot;:66},&quot;favorite_color&quot;:null} Spark读取Avro文件直接遍历avro文件，得到GenericRecord进行处理： val conf = new SparkConf().setMaster(&quot;local&quot;).setAppName(&quot;AvroTest&quot;) val sc = new SparkContext(conf) val rdd = sc.hadoopFile[AvroWrapper[GenericRecord], NullWritable, AvroInputFormat[GenericRecord]](this.getClass.getResource(&quot;/&quot;).toString + &quot;users.avro&quot;) val nameRdd = rdd.map(s =&gt; s._1.datum().get(&quot;name&quot;).toString) nameRdd.collect().foreach(println) 使用Avro需要注意的地方笔者使用Avro的时候暂时遇到了下面2个坑。先记录一下，以后遇到新的坑会更新这篇文章。 1.如果定义了unions类型的字段，而且unions中有null选项的schema，比如如下schema： { &quot;namespace&quot;: &quot;example.avro&quot;, &quot;type&quot;: &quot;record&quot;, &quot;name&quot;: &quot;User2&quot;, &quot;fields&quot;: [ {&quot;name&quot;: &quot;name&quot;, &quot;type&quot;: &quot;string&quot;}, {&quot;name&quot;: &quot;favorite_number&quot;, &quot;type&quot;: [&quot;null&quot;,&quot;int&quot;]}, {&quot;name&quot;: &quot;favorite_color&quot;, &quot;type&quot;: [&quot;null&quot;,&quot;string&quot;]} ] } 这样的schema，如果不使用Avro自动生成的model代码进行insert，并且insert中的model数据有null数据的话。然后用spark读avro文件的话，会报org.apache.avro.AvroTypeException: Found null, expecting int … 这样的错误。 这一点很奇怪，但是使用Avro生成的Model进行insert的话，sprak读取就没有任何问题。 很困惑。 2.如果使用了Map类型的字段，avro生成的model中的Map的Key默认类型为CharSequence。这种model我们insert数据的话，用String是没有问题的。但是spark读取之后要根据Key拿这个Map数据的时候，永远得到的是null。 stackoverflow上有一个页面说到了这个问题。http://stackoverflow.com/questions/19728853/apache-avro-map-uses-charsequence-as-key 需要在map类型的字段里加上”avro.java.string”: “String”这个选项, 然后compile的时候使用-string参数即可。 比如以下这个schema： { &quot;namespace&quot;: &quot;example.avro&quot;, &quot;type&quot;: &quot;record&quot;, &quot;name&quot;: &quot;User3&quot;, &quot;fields&quot;: [ {&quot;name&quot;: &quot;name&quot;, &quot;type&quot;: &quot;string&quot;}, {&quot;name&quot;: &quot;favorite_number&quot;, &quot;type&quot;: [&quot;null&quot;,&quot;int&quot;]}, {&quot;name&quot;: &quot;favorite_color&quot;, &quot;type&quot;: [&quot;null&quot;,&quot;string&quot;]}, {&quot;name&quot;: &quot;scores&quot;, &quot;type&quot;: [&quot;null&quot;, {&quot;type&quot;: &quot;map&quot;, &quot;values&quot;: &quot;string&quot;, &quot;avro.java.string&quot;: &quot;String&quot;}]} ] }","raw":null,"content":null,"categories":[{"name":"avro","slug":"avro","permalink":"http://fangjian0423.github.io/categories/avro/"}],"tags":[{"name":"big data","slug":"big-data","permalink":"http://fangjian0423.github.io/tags/big-data/"},{"name":"avro","slug":"avro","permalink":"http://fangjian0423.github.io/tags/avro/"}]},{"title":"Spark DataFrame介绍","slug":"spark-sql","date":"2016-02-17T01:22:22.000Z","updated":"2016-05-10T14:59:25.000Z","comments":true,"path":"2016/02/17/spark-sql/","link":"","permalink":"http://fangjian0423.github.io/2016/02/17/spark-sql/","excerpt":"","text":"DataFrame是什么DataFrame是一个以命名列方式组织的分布式数据集。在概念上，它跟关系型数据库中的一张表或者1个Python(或者R)中的data frame一样，但是比他们更优化。DataFrame可以根据结构化的数据文件、hive表、外部数据库或者已经存在的RDD构造。 DataFrame的创建Spark DataFrame可以从一个已经存在的RDD、hive表或者数据源中创建。 以下一个例子就表示一个DataFrame基于一个json文件创建： val sc: SparkContext // An existing SparkContext. val sqlContext = new org.apache.spark.sql.SQLContext(sc) val df = sqlContext.read.json(&quot;examples/src/main/resources/people.json&quot;) // Displays the content of the DataFrame to stdout df.show() DataFrame的操作直接以1个例子来说明DataFrame的操作： json文件内容： {&quot;name&quot;:&quot;Michael&quot;} {&quot;name&quot;:&quot;Andy&quot;, &quot;age&quot;:30} {&quot;name&quot;:&quot;Justin&quot;, &quot;age&quot;:19} 程序内容： val conf = new SparkConf().setMaster(&quot;local&quot;).setAppName(&quot;DataFrameTest&quot;) val sc = new SparkContext(conf) val sqlContext = new SQLContext(sc) val df = sqlContext.read.json(this.getClass.getResource(&quot;/&quot;).toString + &quot;people.json&quot;) /** 展示DataFrame的内容 +----+-------+ | age| name| +----+-------+ |null|Michael| | 30| Andy| | 19| Justin| +----+-------+ **/ df.show() /** 以树的形式打印出DataFrame的schema root |-- age: long (nullable = true) |-- name: string (nullable = true) **/ df.printSchema() /** 打印出name列的数据 +-------+ | name| +-------+ |Michael| | Andy| | Justin| +-------+ **/ df.select(&quot;name&quot;).show() /** 打印出name列和age列+1的数据，DataFrame的apply方法返回Column +-------+---------+ | name|(age + 1)| +-------+---------+ |Michael| null| | Andy| 31| | Justin| 20| +-------+---------+ **/ df.select(df(&quot;name&quot;), df(&quot;age&quot;) + 1).show() /** 添加过滤条件，过滤出age字段大于21的数据 +---+----+ |age|name| +---+----+ | 30|Andy| +---+----+ **/ df.filter(df(&quot;age&quot;) &gt; 21).show() /** 以age字段分组进行统计 +----+-----+ | age|count| +----+-----+ |null| 1| | 19| 1| | 30| 1| +----+-----+ **/ df.groupBy(df(&quot;age&quot;)).count().show() 使用反射推断出SchemaSpark SQL的Scala接口支持将包括case class数据的RDD转换成DataFrame。 case class定义表的schema，case class的属性会被读取并且成为列的名字，这里case class也可以被当成别的case class的属性或者是复杂的类型，比如Sequence或Array。 RDD会被隐式转换成DataFrame并且被注册成一个表，这个表可以被用在查询语句中： // sc is an existing SparkContext. val sqlContext = new org.apache.spark.sql.SQLContext(sc) // this is used to implicitly convert an RDD to a DataFrame. import sqlContext.implicits._ // Define the schema using a case class. // Note: Case classes in Scala 2.10 can support only up to 22 fields. To work around this limit, // you can use custom classes that implement the Product interface. case class Person(name: String, age: Int) // Create an RDD of Person objects and register it as a table. val people = sc.textFile(&quot;examples/src/main/resources/people.txt&quot;).map(_.split(&quot;,&quot;)).map(p =&gt; Person(p(0), p(1).trim.toInt)).toDF() people.registerTempTable(&quot;people&quot;) // SQL statements can be run by using the sql methods provided by sqlContext. val teenagers = sqlContext.sql(&quot;SELECT name, age FROM people WHERE age &gt;= 13 AND age &lt;= 19&quot;) // The results of SQL queries are DataFrames and support all the normal RDD operations. // The columns of a row in the result can be accessed by field index: teenagers.map(t =&gt; &quot;Name: &quot; + t(0)).collect().foreach(println) // or by field name: teenagers.map(t =&gt; &quot;Name: &quot; + t.getAs[String](&quot;name&quot;)).collect().foreach(println) // row.getValuesMap[T] retrieves multiple columns at once into a Map[String, T] teenagers.map(_.getValuesMap[Any](List(&quot;name&quot;, &quot;age&quot;))).collect().foreach(println) // Map(&quot;name&quot; -&gt; &quot;Justin&quot;, &quot;age&quot; -&gt; 19) 使用编程指定Schema当case class不能提前确定（例如，记录的结构是经过编码的字符串，或者一个文本集合将会被解析，不同的字段投影给不同的用户），一个 DataFrame 可以通过三步来创建。 1.从原来的 RDD 创建一个行的 RDD2.创建由一个 StructType 表示的模式与第一步创建的 RDD 的行结构相匹配3.在行 RDD 上通过 applySchema 方法应用模式 // sc is an existing SparkContext. val sqlContext = new org.apache.spark.sql.SQLContext(sc) // Create an RDD val people = sc.textFile(&quot;examples/src/main/resources/people.txt&quot;) // The schema is encoded in a string val schemaString = &quot;name age&quot; // Import Row. import org.apache.spark.sql.Row; // Import Spark SQL data types import org.apache.spark.sql.types.{StructType,StructField,StringType}; // Generate the schema based on the string of schema val schema = StructType( schemaString.split(&quot; &quot;).map(fieldName =&gt; StructField(fieldName, StringType, true))) // Convert records of the RDD (people) to Rows. val rowRDD = people.map(_.split(&quot;,&quot;)).map(p =&gt; Row(p(0), p(1).trim)) // Apply the schema to the RDD. val peopleDataFrame = sqlContext.createDataFrame(rowRDD, schema) // Register the DataFrames as a table. peopleDataFrame.registerTempTable(&quot;people&quot;) // SQL statements can be run by using the sql methods provided by sqlContext. val results = sqlContext.sql(&quot;SELECT name FROM people&quot;) // The results of SQL queries are DataFrames and support all the normal RDD operations. // The columns of a row in the result can be accessed by field index or by field name. results.map(t =&gt; &quot;Name: &quot; + t(0)).collect().foreach(println) 数据源Spark SQL默认使用的数据源是parquet(可以通过spark.sql.sources.default修改)。 val df = sqlContext.read.load(&quot;examples/src/main/resources/users.parquet&quot;) df.select(&quot;name&quot;, &quot;favorite_color&quot;).write.save(&quot;namesAndFavColors.parquet&quot;) 可以在读取数据源的时候指定一些往外的参数。数据源也可以使用全名称，比如org.apache.spark.sql.parquet，但是内置的数据源可以使用短名称，比如json, parquet, jdbc。任何类型的DataFrame都可以使用这种方式转换成其他类型： val df = sqlContext.read.format(&quot;json&quot;).load(&quot;examples/src/main/resources/people.json&quot;) df.select(&quot;name&quot;, &quot;age&quot;).write.format(&quot;parquet&quot;).save(&quot;namesAndAges.parquet&quot;) 使用read方法读取数据源得到DataFrame，还可以使用sql直接查询文件的方式： val df = sqlContext.sql(&quot;SELECT * FROM parquet.`examples/src/main/resources/users.parquet`&quot;) 保存模式： 保存方法会需要一个可选参数SaveMode，用于处理已经存在的数据。这些保存模式内部不会用到锁的概念，也不是一个原子操作。如果使用了Overwrite这种保存模式，那么写入数据前会清空之前的老数据。 Scala/Java 具体值 含义 SaveMode.ErrorIfExists (默认值) “error” (默认值) 当保存DataFrame到数据源的时候，如果数据源文件已经存在，那么会抛出异常 SaveMode.Append “append” 如果数据源文件已经存在，append到文件末尾 SaveMode.Overwrite “overwrite” 如果数据源文件已经存在，清空数据 SaveMode.Ignore “ignore” 如果数据源文件已经存在，不做任何处理。跟SQL中的 CREATE TABLE IF NOT EXISTS 类似 持久化表： 当使用HiveContext的时候，使用saveAsTable方法可以把DataFrame持久化成表。跟registerTempTable方法不一样，saveAsTable方法会把DataFrame持久化成表，并且创建一个数据的指针到HiveMetastore对象中。只要获得了同一个HiveMetastore对象的链接，当Spark程序重启的时候，saveAsTable持久化后的表依然会存在。一个DataFrame持久化成一个table也可以通过SQLContext的table方法，参数就是表的名字。 默认情况下，saveAsTable方法会创建一个”被管理的表”，被管理的表的意思是说表中数据的位置会被HiveMetastore所控制，如果表被删除了，HiveMetastore中的数据也相当于被删除了。 Parquet Filesparquet是一种基于列的存储格式，并且可以被很多框架所支持。Spark SQL支持parquet文件的读和写操作，并且会自动维护原始数据的schema，当写一个parquet文件的时候，所有的列都允许为空。 加载Parquet文件// sqlContext from the previous example is used in this example. // This is used to implicitly convert an RDD to a DataFrame. import sqlContext.implicits._ val people: RDD[Person] = ... // An RDD of case class objects, from the previous example. // The RDD is implicitly converted to a DataFrame by implicits, allowing it to be stored using Parquet. people.write.parquet(&quot;people.parquet&quot;) // Read in the parquet file created above. Parquet files are self-describing so the schema is preserved. // The result of loading a Parquet file is also a DataFrame. val parquetFile = sqlContext.read.parquet(&quot;people.parquet&quot;) //Parquet files can also be registered as tables and then used in SQL statements. parquetFile.registerTempTable(&quot;parquetFile&quot;) val teenagers = sqlContext.sql(&quot;SELECT name FROM parquetFile WHERE age &gt;= 13 AND age &lt;= 19&quot;) teenagers.map(t =&gt; &quot;Name: &quot; + t(0)).collect().foreach(println) Parquet文件的PartitionParquet文件可以根据列自动进行分区，只需要调用DataFrameWriter的partitionBy方法即可，该方法需要的参数是需要进行分区的列。比如需要分区成这样： path └── to └── table ├── gender=male │ ├── ... │ │ │ ├── country=US │ │ └── data.parquet │ ├── country=CN │ │ └── data.parquet │ └── ... └── gender=female ├── ... │ ├── country=US │ └── data.parquet ├── country=CN │ └── data.parquet └── ... 这个需要DataFrame就需要4列，分别是name，age，gender和country，write的时候如下： dataFrame.write.partitionBy(&quot;gender&quot;, &quot;country&quot;).parquet(&quot;path&quot;) Schema Merging像ProtocolBuffer，Avro，Thrift一样，Parquet也支持schema的扩展。 由于schema的自动扩展是一次昂贵的操作，所以默认情况下不是开启的，可以根据以下设置打开： 读parquet文件的时候设置参数mergeSchema为true或者设置全局的sql属性spark.sql.parquet.mergeSchema为true： // sqlContext from the previous example is used in this example. // This is used to implicitly convert an RDD to a DataFrame. import sqlContext.implicits._ // Create a simple DataFrame, stored into a partition directory val df1 = sc.makeRDD(1 to 5).map(i =&gt; (i, i * 2)).toDF(&quot;single&quot;, &quot;double&quot;) df1.write.parquet(&quot;data/test_table/key=1&quot;) // Create another DataFrame in a new partition directory, // adding a new column and dropping an existing column val df2 = sc.makeRDD(6 to 10).map(i =&gt; (i, i * 3)).toDF(&quot;single&quot;, &quot;triple&quot;) df2.write.parquet(&quot;data/test_table/key=2&quot;) // Read the partitioned table val df3 = sqlContext.read.option(&quot;mergeSchema&quot;, &quot;true&quot;).parquet(&quot;data/test_table&quot;) df3.printSchema() // The final schema consists of all 3 columns in the Parquet files together // with the partitioning column appeared in the partition directory paths. // root // |-- single: int (nullable = true) // |-- double: int (nullable = true) // |-- triple: int (nullable = true) // |-- key : int (nullable = true) JSON数据源本文之前的一个例子就是使用的JSON数据源，使用SQLContext.read.json()读取一个带有String类型的RDD或者一个json文件。 需要注意的是json文件不是一个典型的json格式的文件，每一行都是一个json对象。 // sc is an existing SparkContext. val sqlContext = new org.apache.spark.sql.SQLContext(sc) // A JSON dataset is pointed to by path. // The path can be either a single text file or a directory storing text files. val path = &quot;examples/src/main/resources/people.json&quot; val people = sqlContext.read.json(path) // The inferred schema can be visualized using the printSchema() method. people.printSchema() // root // |-- age: integer (nullable = true) // |-- name: string (nullable = true) // Register this DataFrame as a table. people.registerTempTable(&quot;people&quot;) // SQL statements can be run by using the sql methods provided by sqlContext. val teenagers = sqlContext.sql(&quot;SELECT name FROM people WHERE age &gt;= 13 AND age &lt;= 19&quot;) // Alternatively, a DataFrame can be created for a JSON dataset represented by // an RDD[String] storing one JSON object per string. val anotherPeopleRDD = sc.parallelize( &quot;&quot;&quot;{&quot;name&quot;:&quot;Yin&quot;,&quot;address&quot;:{&quot;city&quot;:&quot;Columbus&quot;,&quot;state&quot;:&quot;Ohio&quot;}}&quot;&quot;&quot; :: Nil) val anotherPeople = sqlContext.read.json(anotherPeopleRDD) Hive表需要使用HiveContext。 // sc is an existing SparkContext. val sqlContext = new org.apache.spark.sql.hive.HiveContext(sc) sqlContext.sql(&quot;CREATE TABLE IF NOT EXISTS src (key INT, value STRING)&quot;) sqlContext.sql(&quot;LOAD DATA LOCAL INPATH &apos;examples/src/main/resources/kv1.txt&apos; INTO TABLE src&quot;) // Queries are expressed in HiveQL sqlContext.sql(&quot;FROM src SELECT key, value&quot;).collect().foreach(println) JDBC直接使用load方法加载： sqlContext.load(&quot;jdbc&quot;, Map(&quot;url&quot; -&gt; &quot;jdbc:mysql://localhost:3306/your_database?user=your_user&amp;password=your_password&quot;, &quot;dbtable&quot; -&gt; &quot;your_table&quot;))","raw":null,"content":null,"categories":[{"name":"spark","slug":"spark","permalink":"http://fangjian0423.github.io/categories/spark/"}],"tags":[{"name":"big data","slug":"big-data","permalink":"http://fangjian0423.github.io/tags/big-data/"},{"name":"spark","slug":"spark","permalink":"http://fangjian0423.github.io/tags/spark/"}]},{"title":"Spark Streaming编程指南笔记","slug":"sparkstreaming-programming-guide","date":"2016-02-09T17:36:17.000Z","updated":"2016-05-10T14:59:25.000Z","comments":true,"path":"2016/02/10/sparkstreaming-programming-guide/","link":"","permalink":"http://fangjian0423.github.io/2016/02/10/sparkstreaming-programming-guide/","excerpt":"","text":"概述Spark Streaming是Spark核心API的扩展，用于处理实时数据流。Spark Streaming处理的数据源可以是Kafka，Flume，Twitter，ZeroMQ，Kinesis或者Tcp Sockets，这些数据可以使用map，reduce，join，window方法进行处转换，还可以直接使用Spark内置的机器学习算法，图算法包来处理数据。 最终处理后的数据可以存入文件系统，数据库。 Spark Streaming内部接收到实时数据之后，会把数据分成几个批次，这些批次数据会被Spark引擎处理并生成各个批次的结果。 Spark Streaming提供了一个叫做discretized stream 或者 DStream的抽象概念，表示一段连续的数据流。DStream会在数据源中的数据流中创建，或者在别的DStream中使用类似map，join方法创建。一个DStream表示一个RDD序列。 一个快速例子以一个TCP Socket监听接收数据，并计算单词的个数为例子讲解。 首先，需要import Spark Streaming中的一些类和StreamingContext中的一些隐式转换。我们会创建一个带有2个线程，1秒一个批次的StreamingContext。 import org.apache.spark.SparkConf import org.apache.spark.streaming.{Seconds, StreamingContext} object SparkStreamTest extends App { val conf = new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;NetworkWordCount&quot;) // 创建一个StreamingContext，每1秒钟处理一次计算程序 val ssc = new StreamingContext(conf, Seconds(1)) // 使用StreamingContext创建DStream，DStream表示TCP源中的流数据. lines这个DStream表示接收到的服务器数据，每一行都是文本 val lines = ssc.socketTextStream(&quot;localhost&quot;, 9999) // 使用flatMap将每一行中的文本转换成每个单词，并产生一个新的DStream。 val words = lines.flatMap(_.split(&quot; &quot;)) // 使用map方法将每个单词转换成tuple val pairs = words.map(word =&gt; (word, 1)) // 使用reduceByKey计算出每个单词的出现次数 val wordCounts = pairs.reduceByKey(_ + _) wordCounts.print() ssc.start() // 开始计算 ssc.awaitTermination() // 等待计算结束 } 在运行这段代码之前，首先先起一个netcat服务： nc -lk 9999 之后比如输入hello world之后，控制台会打印出如下数据： ------------------------------------------- Time: 1454684570000 ms ------------------------------------------- (hello,1) (world,1) 基础概念Linking(SparkStreaming的连接)写Spark Streaming程序需要一些依赖。使用maven的话加入以下依赖： &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming_2.10&lt;/artifactId&gt; &lt;version&gt;1.6.0&lt;/version&gt; &lt;/dependency&gt; 使用sbt的话，加入以下依赖： libraryDependencies += &quot;org.apache.spark&quot; % &quot;spark-streaming_2.10&quot; % &quot;1.6.0&quot; SparkStreaming核心不提供一些数据源的依赖，需要手动添加，一些数据源对应的Artifact如下： 数据源 Artifact Kafka spark-streaming-kafka_2.10 Flume spark-streaming-flume_2.10 Kinesis spark-streaming-kinesis-asl_2.10 [Amazon Software License] Twitter spark-streaming-twitter_2.10 ZeroMQ spark-streaming-zeromq_2.10 MQTT spark-streaming-mqtt_2.10 StreamingContext的初始化StreamingContext的创建是Spark Streaming程序中最重要的一环。 可以根据SparkConf对象创建出StreamingContext对象： import org.apache.spark._ import org.apache.spark.streaming._ val conf = new SparkConf().setAppName(appName).setMaster(master) val ssc = new StreamingContext(conf, Seconds(1)) appName参数是应用程序的名字，在cluster UI中显示的就是这个名字。master参数的意义跟spark中master参数的意义是一样的。 StreamingContext内部会创建SparkContext，可以使用StreamingContext内部的sparkContext获得。 ssc.sparkContext // 得到SparkContext StreamingContext也可以根据SparkContext创建： import org.apache.spark.streaming._ val sc = ... // existing SparkContext val ssc = new StreamingContext(sc, Seconds(1)) StreamingContext创建之后，可以做以下几点： 1.创建DStreams定义数据源2.使用DStreams的transformation和output operations用于计算3.使用streamingContext的start方法接收数据4.使用streamingContext的awaitTermination方法等待处理结果5.可以使用streamingContext的stop方法停止程序 一些需要注意的点： 1.context开始启动之后，一些streaming的计算不允许发生2.context停掉之后不能重启3.一个JVM在同一时刻只能有一个StreamingContext可以激活4.StreamingContext中的stop方法内部也会stop SparkContext。如果只想stop StreamingContext，那么调用stop方法的时候参数设置为false5.一个SparkContext可以用来创建多个StreamingContexts，只要上一个StreamingContext在下一个StreamingContext创建之前停掉 Discretized Streams (DStreams)DStreams和Discretized Streams在Spark Streaming中代表相同的意思： 1.一段连续的数据流2.数据源中接收到的数据流3.使用transforming处理过的流数据 Spark内部一个DStream表示一段连续的RDD。DStream中每段RDD表示一段时间内的RDD，效果如下： DStream可以使用一些transformation操作将内部的RDD转换成另外一种RDD。比如之前的一个单词统计例子中就将一行文本的DStream转换成每个单词的DStream，过程如下： Input DStreams and Receivers(数据源和接收器)Input DStreams是DStreams从streaming source中接收到的输入流数据。在之前分析的一个单词统计例子中，lines就是个Input DStream，表示接收到的服务器数据，每一行都是文本。 每一个Input DStream(除了file stream)都会关联一个Receiver对象，这个Receiver对象的作用是接收数据源中的数据并存储在内存中。 Spark Streaming提供了两种类型的内置数据源： 1.基础数据源。可以直接使用StreamingContext的API，比如文件系统，socket连接，Akka。2.高级数据源。比如Flume，Kafka，Kinesis，Twitter等可以使用工具类的数据源。使用这些数据源需要对应的依赖，在Linking章节中已经介绍过。 如果想在streaming程序中并行地接收多个数据源，需要创建多个Input DStream，有个多个Input DStream的话那就会对应地有多个Receiver。但是需要记住的是，Spark的worker/executor模式是一个相当耗时的任务，因此服务器的配置需要够好才能支撑多个Input DStream。 一些注意点： 1.当本地跑Spark Streaming程序的时候，不要使用”local”或者”local[1]”设置master URL。因为这两种master URL只会使用1个线程。当使用比如Flume，Kafka，socket这些数据源的时候，因为只有一个线程跑receiver接收数据，那么没有其他线程去处理接收后的数据了。所以，当在本地跑Spark Streaming程序的时候，需要将master URL设置为local[n]，n需要大于receiver的个数。2.服务器的核数需要大于receiver的个数。否则程序只会接收数据，而不会处理数据。 Basic Sources(基础数据源)基础数据源刚刚分析过，StreamingContext的API可以使用如文件系统，socket连接，Akka作为输入源。socket连接本文以开始的例子中已经使用过了。 文件系统的输入源会读取文件或任何支持HDFS API(比如HDFS，S3，NFS)的文件系统的数据： streamingContext.fileStream[KeyClass, ValueClass, InputFormatClass](dataDirectory) Spark Streaming会监测dataDirectory目录并且会处理这个目录中新创建的文件(老文件写新数据的话不会被支持)。使用文件数据源还需要这几点： 1.所有文件的数据格式必须相同2.dataDirectory目录中的文件必须是新创建的，也可以是从别的目录move进来的3.文件内部的数据更改之后，新更改的数据不会被处理 对于简单的文件，可以使用streamingContext的textFileStream方法处理。 Advanced Sources(高级数据源)高级数据源需要一些非Spark依赖。Spark Streaming把创建DStream的API移到了各自的API里。如果想创建一个使用Twitter的数据源，需要做以下三步： 1.添加对应的Twitter依赖spark-streaming-twitter_2.10到项目里2.import这个类TwitterUtils，使用TwitterUtils.createStream创建DStream3.部署 Custom Sources(自定义数据源)要实现一个自定义的数据源，需要实现一个自定义的receiver Receiver Reliability(接收器的可靠性)基于可靠性的数据源分为两种。 1.可靠的接收器(Reliable Receiver)：一个可靠的接收器接收到数据之后会给数据源发送消息表示自己已经接收到数据2.不可靠的接收器(Unreliable Receiver)：一个不可靠的接收器不会发送消息给数据源。 想写出一个可靠的接收器可以参考 http://spark.apache.org/docs/latest/streaming-custom-receivers.html DStreams的Transformations操作DStream的Transformations操作跟RDD的Transformations操作类似， Transformation 含义 map(func) 根据func函数生成一个新的DStream flatMap(func) 跟map方法类似，但是每一项可以返回多个值。func函数的返回值是一个集合 filter(func) 根据func函数返回true的数据集 repartition(numPartitions) 重新给 DStream 分区 union(otherStream) 取2个DStream的并集，得到一个新的DStream count() 返回一个新的只有一个元素的DStream，这个元素就是DStream中的所有RDD的个数 reduce(func) 返回一个新的只有一个元素的DStream，这个元素就是DStream中的所有RDD通过func函数聚合得到的结果 countByValue() 如果DStream的类型为K，那么返回一个新的DStream，这个新的DStream中的元素类型是(K, Long)，K是原先DStream的值，Long表示这个Key有多少次 reduceByKey(func, [numTasks]) 本文的例子使用过这个方法，对于是键值对(K,V)的DStream，返回一个新的DStream以K为键，各个value使用func函数操作得到的聚合结果为value join(otherStream, [numTasks]) 基于(K, V)键值对的DStream，如果对(K, W)的键值对DStream使用join操作，可以产生(K, (V, W))键值对的DStream cogroup(otherStream, [numTasks]) 跟join方法类似，不过是基于(K, V)的DStream，cogroup基于(K, W)的DStream，产生(K, (Seq[V], Seq[W]))的DStream transform(func) 基于DStream中的每个RDD调用func函数，func函数的参数是个RDD，返回值也是个RDD updateStateByKey(func) 对于每个key都会调用func函数处理先前的状态和所有新的状态。比如就可以用来做累加，这个方法跟reduceByKey类似，但比它更加灵活 UpdateStateByKey操作使用UpdateStateByKey方法需要做以下两步： 1.定义状态：状态可以是任意的数据类型2.定义状态更新函数：这个函数需要根据输入流把先前的状态和所有新的状态 不管有没有新数据进来，在每个批次中，Spark都会对所有存在的key调用func方法，如果func函数返回None，那么key-value键值对不会被处理。 以一个例子来讲解updateStateByKey方法，这个例子会统计每个单词的个数在一个文本输入流里： runningCount是一个状态并且是Int类型，所以这个状态的类型是Int，runningCount是先前的状态，newValues是所有新的状态，是一个集合，函数如下： def updateFunction(newValues: Seq[Int], runningCount: Option[Int]): Option[Int] = { val newCount = ... // add the new values with the previous running count to get the new count Some(newCount) } updateStateByKey方法的调用： val runningCounts = pairs.updateStateByKey[Int](updateFunction _) Transform操作Transform操作针对的是RDD-RDD的操作，所以可以用来处理那些没有在DStream API中暴露的处理任意的RDD操作。比如在DStream中的每次批次没有join rdd的API，所以可以使用transform操作： val spamInfoRDD = ssc.sparkContext.newAPIHadoopRDD(...) // RDD containing spam information val cleanedDStream = wordCounts.transform(rdd =&gt; { rdd.join(spamInfoRDD).filter(...) // join data stream with spam information to do data cleaning ... }) Window操作window操作效果图如下图所示，把几个批次的DStream合并成一个DStream： 每个window操作都需要2个参数： 1.window length。每个window对应的批次数(上图中是3，time1-time3是一个window, time3-time5也是一个window)2.sliding interval。每个window之间的间隔时间，上图下方的window1，window3，window5的间隔。上图这个值为2 这两个参数必须是批次间隔的倍数。上个批次间隔值为1。 以1个例子来讲解window操作，基于本文一开始的那个例子，生成最后30秒的数据，每10秒为单位，这里就需要使用reduceByKeyAndWindow方法： val windowedWordCounts = pairs.reduceByKeyAndWindow((a:Int,b:Int) =&gt; (a + b), Seconds(30), Seconds(10)) 其他的一些window操作： Transformation 含义 window(windowLength, slideInterval) 根据window操作的2个参数得到新的DStream countByWindow(windowLength, slideInterval) 基于window操作的count操作 reduceByWindow(func, windowLength, slideInterval) 基于window操作的reduce操作 reduceByKeyAndWindow(func, windowLength, slideInterval, [numTasks]) 基于window操作的reduceByKey操作 reduceByKeyAndWindow(func, invFunc, windowLength, slideInterval, [numTasks]) 跟reduceByKeyAndWindow方法类似，更有效率，invFunc方法跟func方法的参数返回值一样，表示从window离开的数据 countByValueAndWindow(windowLength, slideInterval, [numTasks]) 基于window操作的countByValue操作 Join操作DStream可以很容易地join其他DStream： val stream1: DStream[String, String] = ... val stream2: DStream[String, String] = ... val joinedStream = stream1.join(stream2) 还可以使用leftOuterJoin，rightOuterJoin，fullOuterJoin等。同样地，也可以在window操作后的DStream中使用join： val windowedStream1 = stream1.window(Seconds(20)) val windowedStream2 = stream2.window(Minutes(1)) val joinedStream = windowedStream1.join(windowedStream2) 基于rdd的join： val dataset: RDD[String, String] = ... val windowedStream = stream.window(Seconds(20))... val joinedStream = windowedStream.transform { rdd =&gt; rdd.join(dataset) } DStream的输出操作输出操作允许DStream中的数据输出到外部系统，比如像数据库、文件系统等。 输出操作 含义 print() 打印出DStream中每个批次的前10条数据 saveAsTextFiles(prefix, [suffix]) 把DStream中的数据保存到文本文件里。每次批次的文件名根据参数prefix和suffix生成：”prefix-TIME_IN_MS[.suffix]” saveAsObjectFiles(prefix, [suffix]) 把DStream中的数据按照Java序列化的方式保存Sequence文件里，文件名规则跟saveAsTextFiles方法一样 saveAsHadoopFiles(prefix, [suffix]) 把DStream中的数据保存到Hadoop文件里，文件名规则跟saveAsTextFiles方法一样 foreachRDD(func) 遍历DStream中的每段RDD，遍历的过程中可以将RDD中的数据保存到外部系统中 foreachRDD中的设计模式foreachRDD方法会遍历DStream中的每段RDD，遍历的过程中可以将RDD中的数据保存到外部系统中。这个方法很实用，所以理解foreachRDD方法显得很重要。 将数据写到外部系统通常都需要一个connection对象，所以很多时候都会不经意地创建这个connection对象： dstream.foreachRDD { rdd =&gt; val connection = createNewConnection() // executed at the driver rdd.foreach { record =&gt; connection.send(record) // executed at the worker } } 这种写法是不正确的。这里connection需要被序列化并且发送到worker，而且connection对象会跨机器传递，会发生序列化错误(connection对象是不可序列化的)，初始化错误(connection对象需要在worker中初始化)。这个错误的解决方案就是在worker中创建connection对象。 为每条记录创建connection也是一个很常见的错误： dstream.foreachRDD { rdd =&gt; rdd.foreach { record =&gt; val connection = createNewConnection() connection.send(record) connection.close() } } 因为创建connection对象是一种很耗资源，很耗时间的操作。对于每条数据都创建一个connection代驾更大。所有可以使用rdd.foreachPartition方法，这个方法会创建单一的connection并且在一个RDD分区中所有数据都使用这个connection： dstream.foreachRDD { rdd =&gt; rdd.foreachPartition { partitionOfRecords =&gt; val connection = createNewConnection() partitionOfRecords.foreach(record =&gt; connection.send(record)) connection.close() } } 一种更好的方式就是使用ConnectionPool，ConnectionPool可以重用connection对象在多个批次和RDD中。 dstream.foreachRDD { rdd =&gt; rdd.foreachPartition { partitionOfRecords =&gt; // ConnectionPool is a static, lazily initialized pool of connections val connection = ConnectionPool.getConnection() partitionOfRecords.foreach(record =&gt; connection.send(record)) ConnectionPool.returnConnection(connection) // return to the pool for future reuse } } 其他需要注意的点： 1.DStream的输出操作也是延迟执行的，就像RDD的action操作一样。RDD的action操作在DStream的输出操作内部执行的话会强制Spark Streaming执行。如果程序里没有任何输出操作，或者有比如像dstream.foreachRDD操作一样内部没有rdd的action操作的话，这样就不会执行任意操作，会被Spark忽略。2.默认情况下，在一个时间点下，只有一个输出操作被执行。它们是根据程序里的编写顺序执行的。 DataFrame and SQL Operations在Spark Streaming中可以使用DataFrames and SQL操作。 /** DataFrame operations inside your streaming program */ val words: DStream[String] = ... words.foreachRDD { rdd =&gt; // Get the singleton instance of SQLContext val sqlContext = SQLContext.getOrCreate(rdd.sparkContext) import sqlContext.implicits._ // Convert RDD[String] to DataFrame val wordsDataFrame = rdd.toDF(&quot;word&quot;) // Register as table wordsDataFrame.registerTempTable(&quot;words&quot;) // Do word count on DataFrame using SQL and print it val wordCountsDataFrame = sqlContext.sql(&quot;select word, count(*) as total from words group by word&quot;) wordCountsDataFrame.show() } Caching / Persistence跟RDD类似，DStream也允许将数据保存到内存中，使用persist方法可以做到这一点。 但是基于window和state的操作，reduceByWindow,reduceByKeyAndWindow,updateStateByKey它们就是隐式的保存了，系统已经帮它自动保存了。 从网络接收的数据(比如Kafka, Flume, sockets等)，默认是保存在两个节点来实现容错性，以序列化的方式保存在内存当中。 Checkpointing一个Spark Streaming程序必须是全天工作的，所以如果万一系统挂掉了或者JVM挂掉之后是要有容错性的。Spark Streaming需在容错存储系统做checkpoint，这样才能够处理错误信息。有两种类型的数据需要做checkpoint： 1.metadata checkpointing：元数据检查点。主要包括3个元数据：配置：创建streaming程序的的配置信息DStream操作：streaming程序中DStream的操作集合未完成的批次：在队列中未完成的批次2.data checkpointing：数据检查点。保存已经生成的RDD数据。在一些有状态的transformation操作中，一些RDD数据会依赖之前批次的RDD数据，随时时间的推移，这种依赖情况就会越发严重。为了解决这个问题，需要保存这些有依赖关系的RDD数据到存储系统中(比如HDFS)来剪断这种依赖关系 什么时候需要启用checkpoint？ 满足以下2个条件中的任意1个即可启用checkpoint: 1.使用了有状态的transformation。比如使用了updateStateByKey或reduceByKeyAndWindow方法后，就需要启用checkpoint2.恢复挂掉的程序。可以根据metadata数据恢复程序 一些比较简单的streaming程序没有用到有状态的transformation，并且也可以接受程序挂掉之后丢失部分数据，那么就没有必要启用checkpoint。 如何配置checkpoint？ checkpoint的配置需要设置一个目录，使用streamingContext.checkpoint(checkpointDirectory)方法。 // Function to create and setup a new StreamingContext def functionToCreateContext(): StreamingContext = { val ssc = new StreamingContext(...) // new context val lines = ssc.socketTextStream(...) // create DStreams ... ssc.checkpoint(checkpointDirectory) // set checkpoint directory ssc } // Get StreamingContext from checkpoint data or create a new one val context = StreamingContext.getOrCreate(checkpointDirectory, functionToCreateContext _) // Do additional setup on context that needs to be done, // irrespective of whether it is being started or restarted context. ... // Start the context context.start() context.awaitTermination() 因为检查操作会导致保存到hdfs上的开销，所以设置这个时间间隔，要很慎重。对于小批次的数据，比如一秒的，检查操作会大大降低吞吐量。但是检查的间隔太长，会导致任务变大。通常来说，5-10秒的检查间隔时间是比较合适的。","raw":null,"content":null,"categories":[{"name":"spark","slug":"spark","permalink":"http://fangjian0423.github.io/categories/spark/"}],"tags":[{"name":"big data","slug":"big-data","permalink":"http://fangjian0423.github.io/tags/big-data/"},{"name":"spark","slug":"spark","permalink":"http://fangjian0423.github.io/tags/spark/"}]},{"title":"Spark编程指南笔记","slug":"spark-programming-guide","date":"2016-01-26T16:22:21.000Z","updated":"2016-05-10T14:59:25.000Z","comments":true,"path":"2016/01/27/spark-programming-guide/","link":"","permalink":"http://fangjian0423.github.io/2016/01/27/spark-programming-guide/","excerpt":"","text":"Spark初始化使用Spark编程第一件要做的事就是初始化SparkContext对象，SparkContext对象会告诉Spark如何使用Spark集群。 SparkContext会使用SparkConf中的一些配置信息，所以构造SparkContext对象之前需要构造一个SparkConf对象。 一个JVM上的SparkContext只有一个是激活的，如果要构造一个新的SparkContext，必须stop一个已经激活的SparkContext。 val conf = new SparkConf().setMaster(&quot;local&quot;).setAppName(&quot;Test&quot;) val sc = new SparkContext(conf) appName为Test，这个name会在cluster UI上展示，master是一个Spark，Mesos，YARN cluster URL或者local。 具体的值可以参考 master-url解释。 RDD(Resilient Distributed Datasets)Spark提出的最主要抽象概念是RDD(弹性分布式数据集)，它是一个有容错机制并且可以被并行操作的元素集合。 有两种方式可以创建RDD: 1.使用一个已存在的集合进行并行计算2.使用外部数据集，比如共享的文件系统，HDFS，HBase以及任何支持Hadoop InputFormat的数据源 并行集合使用SparkContext的parallelize方法构造并行集合。 val dataSet = Array(1,2,3,4,5) val rdd = sc.parallelize(dataSet) rdd.reduce(_ + _) // 15 parallelize方法有一个参数slices，表示数据集切分的份数。Spark会在集群上为每一个分片起一个任务。如果不设置的话，Spark会根据集群的情况，自动设置slices的数字。 外部数据集文本文件可以使用SparkContext的textFile方法构造RDD。这个方法接收一个URI参数(也可以包括本地的文件)，并且以每行的方式读取文件内容。 比如data.txt文件里一行一个数字，取所有数字的和： val rdd = sc.textFile(this.getClass.getResource(&quot;/data.txt&quot;).toString) rdd.reduce { (a, b) =&gt; (a.toInt + b.toInt).toString } // 另外一种方式 rdd.map(s =&gt; s.toInt).reduce(_ + _) Spark中所有基于文件的输入方法，都支持目录，压缩文件，通配符读取文件。比如 sc.textFile(&quot;/data/*.txt&quot;) sc.textFile(&quot;/data&quot;) textFile方法也可以传入第二个可选参数来控制文件的分片数量。默认情况下，Spark会为文件的每一个块（在HDFS中块的大小默认是64MB）创建一个分片。但是你也可以通过传入一个更大的值来要求Spark建立更多的分片。注意，分片的数量绝不能小于文件块的数量。 除了文本文件之外，Spark还支持其他格式的输入： 1.SparkContext的wholeTextFiles方法会读取一个包含很多小文件的目录，并以filename，content为键值对的方式返回结果。2.对于SequenceFiles，可以使用SparkContext的sequenceFile[K, V]方法创建。像 IntWritable和Text一样，它们必须是 Hadoop 的 Writable 接口的子类。另外，对于几种通用 Writable 类型，Spark 允许你指定原生类型来替代。例如：sequencFile[Int, String] 将会自动读取 IntWritable 和 Texts。3.对于其他类型的 Hadoop 输入格式，你可以使用 SparkContext.hadoopRDD 方法，它可以接收任意类型的 JobConf 和输入格式类，键类型和值类型。按照像 Hadoop 作业一样的方法设置输入源就可以了。4.RDD.saveAsObjectFile 和 SparkContext.objectFile 提供了以 Java 序列化的简单方式来保存 RDD。虽然这种方式没有 Avro 高效，但也是一种简单的方式来保存任意的 RDD。 RDD操作RDD操作基础RDD支持两种类型的操作。 1.transformations。从一个数据集产生一个新的数据集。比如map方法，就可以根据旧的数据集产生新的数据集。2.actions。在一个数据集中进行聚合操作，并且返回一个最终的结果。 Spark中所有的transformations操作都是lazy的，就是说它们并不会立刻真的计算出结果。相反，它们仅仅是记录下了转换操作的操作对象（比如：一个文件）。只有当一个启动操作被执行，要向驱动程序返回结果时，转化操作才会真的开始计算。这样的设计使得Spark运行更加高效——比如，我们会发觉由map操作产生的数据集将会在reduce操作中用到，之后仅仅是返回了reduce的最终的结果而不是map产生的庞大数据集。 在默认情况下，每一个由转化操作得到的RDD都会在每次执行启动操作时重新计算生成。但是，你也可以通过调用persist(或cache)方法来将RDD持久化到内存中，这样Spark就可以在下次使用这个数据集时快速获得。Spark同样提供了对将RDD持久化到硬盘上或在多个节点间复制的支持。 一个计算文件中每行的字符串个数和所有字符串个数的和例子： val rdd = sc.textFile(&quot;data.txt&quot;) val lineLengths = rdd.map(s =&gt; s.length) val totalLength = lineLengths.reduce(_ + _) lineLengths对象是一个transformations结果，所以它不是马上就开始执行的，当运行lineLengths.reduce的时候lineLengths才会开始去计算。如果之后还会用到这个lineLengths。可以在reduce方法之前加上: lineLengths.persist() 使用函数Spark很多方法都可以使用函数完成。 使用对象： object SparkFunction { def strLength = (s: String) =&gt; s.length } val lineLengths = rdds.map(SparkFunction.strLength) lineLengths.reduce(_ + _) 使用类： class SparkCls { def func = (s: String) =&gt; s.length def buildRdd(rdd: RDD[String]) = rdd.map(func) } new SparkCls().buildRdd(rdds).reduce(_ + _) 闭包var counter = 0 var rdd = sc.parallelize(data) // Wrong: Don&apos;t do this!! rdd.foreach(x =&gt; counter += x) println(&quot;Counter value: &quot; + counter) 上述代码如果在local模式并且在一个JVM的情况下使用是可以得到正确的值的。这是因为所有的RDD和变量counter都是同一块内存上。 然后在集群模式下，上述代码的结果可能就不会是我们想要的正确结果。集群模式下，Spark会在RDD分成多个任务，每个任务都会被对应的executor执行。在executor执行之前，Spark会计算每个闭包。上面这个例子foreach方法和counter就组成了一个闭包。这个闭包会被序列化并且发送给每个executor。在local模式下，因为只有一个executor，所以共享相同的闭包。然后在集群模式下，有多个executor，并且各个executor在不同的节点上都有自己的闭包的拷贝。 所以counter变量就已经不再是节点上的变量了。虽然counter变量在内存上依然存在，但是它对于executor已经不可见，executor只知道它是序列化后的闭包的一份拷贝。因此如果counter的操作都是在闭包下的话，counter的值还是为0。 Spark提供了一种Accumulator的概念用来处理集群模式下的变量更新问题。 另外一个要注意的是不要使用foreach或者map方法打印数据。在一台机器上，这个操作是没有问题的。但是如果在集群上，不一定会打印出全部的数据。可以使用collect方法将RDD放到调用节点上。所以rdd.collect().foreach(println)是可以打印出数据的，但是可能数据量过大，会导致OOM。所以最好的方式还是使用take方法：rdd.take(100).foreach(println)。 使用键值对Spark也支持键值对的操作，这在分组和聚合操作时候用得到。当键值对中的键为自定义对象时，需要自定义该对象的equals()和hashCode()方法。 一个使用键值对的单词统计例子： // 使用map方法将单词文本转换成一个键值对，(word, num)。 num初始值为1 val pairs = rdd.map(s =&gt; (s, 1)) val reduceRdd = pairs.reduceByKey(_ + _) val result = reduceRdd.sortByKey().collect() result.foreach(println) Spark内置的Transformations 转换 含义 map(func) 根据func函数生成一个新的rdd数据集 filter(func) 根据func函数返回true的数据集 flatMap(func) 跟map方法类似，但是每一项可以返回多个值。func函数的返回值是一个集合 mapPartitions(func) 跟map方法类似，但是是在每个partition上运行的。func函数的参数是一个Iteraror，返回值也是一个Iterator。如果map方法需要创建一个额外的对象，使用mapPartitions方法比map方法高效得多 mapPartitionsWithIndex(func) 作用跟mapPartitions方法一样，只是func方法多了一个index参数。 func方法定义 (Int, Iterator[T]) =&gt; Iterator[U] sample(withReplacement, fraction, seed) 根据 fraction 指定的比例，对数据进行采样，可以选择是否用随机数进行替换，seed 用于指定随机数生成器种子 union(otherDataset) 取2个rdd的并集，得到一个新的rdd intersection(otherDataset) 取2个rdd的交集，得到一个新的rdd。这个新的rdd没有重复的数据 distinct([numTasks]) 返回一个新的没有重复数据的数据集 groupByKey([numTasks]) 将一个(K,V)的键值对RDD转换成一个(K, Iterable[V])的新的键值对RDD。注意点：如果group的目的是为了做聚合计算(比如总和或者平均值)，使用reduceByKey或者aggregateByKey性能更好。 reduceByKey(func, [numTasks]) 跟groupByKey方法一样，也是操作(K, V)的键值对RDD。返回值同样是一个(K, V)的键值对RDD，func函数的定义：(V, V) =&gt; V，也就是每两个值的值 aggregateByKey(zeroValue)(seqOp, combOp, [numTasks]) 跟reduceByKey作用类似，zeroValue参数表示初始值，这个初始值的类型可以跟rdd中的键值对的值的类型不同。seqOp参数是个函数，定义为(U, V) =&gt; U，U类型是初始化zeroValue的类型，V类型是一开始rdd的键值对的值的类型。这个函数表示用来与初始值zeroValue进行比较，取一个新的值，需要注意的是这个新的值会作为参数出现在下一次key相等的情况下。 combOp参数也是个函数，定义为(U, U) =&gt; U，U类型也是初始值的类型。这个函数相当于reduce方法中的函数，用来做聚合操作 sortByKey([ascending], [numTasks]) 对一个(K, V)键值对的RDD进行排行，返回一个基于K排序的新的RDD join(otherDataset, [numTasks]) 基于(K, V)键值对的rdd，如果对(K, W)的键值对rdd使用join操作，可以产生(K, (V, W))键值对的rdd。类似数据库中的join操作，spark还提供leftOuterJoin, rightOuterJoin, fullOuterJoin方法 cogroup(otherDataset, [numTasks]) 跟join方法类似，不过是基于(K, V)的rdd，cogroup基于(K, W)的rdd，产生(K, (Iterable[V], Iterable[W]))的rdd。这个方法也叫做groupWith cartesian(otherDataset) 笛卡尔积。 有K的rdd与V的rdd进行笛卡尔积，会生成(K, V)的rdd pipe(command, [envVars]) 对rdd进行管道操作。 就像shell命令一样 coalesce(numPartitions) 减少 RDD 的分区数到指定值。在过滤大量数据之后，可以执行此操作 repartition(numPartitions) 重新给 RDD 分区 repartitionAndSortWithinPartitions(partitioner) 重新给 RDD 分区，并且每个分区内以记录的 key 排序 以这个数据为例： i am format let us go hoho good nice format is nice haha haha haha scala is cool, nice 一些Transformations操作： rdd.map(s =&gt; s.length) // 每一行文本转换成长度 rdd.filter(s =&gt; s.length == 1) // 取文本长度为1的数据 rdd.flatMap(s =&gt; s.split(&quot;,&quot;)) // 把有 , 的字符串转换成多行 rdd.sample(false, 1.0) rdd.union(rdd2) rdd.intersection(rdd2) rdd.distinct(rdd2) rdd.map(s =&gt; (s, 1)).groupByKey().foreach { pair =&gt; { print(pair._1) ; print(&quot; ** &quot;) println(pair._2.mkString(&quot;-&quot;)) } } rdd.map(s =&gt; (s, 1)).reduceByKey { (x, y) =&gt; x + y }.foreach { (pair) =&gt; println(pair._1 + &quot; &quot; + pair._2) } rdd.map(s =&gt; (s, 1)).aggregateByKey(0)( (a, b) =&gt; { math.max(a, b) }, (a, b) =&gt; { a + b } ).foreach { pair =&gt; println(pair._1 + &quot; &quot; + pair._2) } rdd.map(s =&gt; (s, 1)).sortByKey(true).foreach { pair =&gt; println(pair._1 + &quot; &quot; + pair._2) } rdd.map(s =&gt; (s, s.length)).join(rdd.map(s =&gt; (s, s.charAt(0).toUpper.toString))).foreach { pair =&gt; println(pair._1 + &quot; &quot; + pair._2._1 + &quot; &quot; + pair._2._2) } rdd.map(s =&gt; (s, s.length)).cogroup(rdd.map(s =&gt; (s, s.charAt(0).toUpper.toString))).foreach { pair =&gt; { println(pair._1 + &quot;======&quot;) println(pair._2._1.toList.mkString(&quot;-&quot;)) println(pair._2._2.toList.mkString(&quot;-&quot;)) println(&quot;**&quot; * 8) } } rdd.map(s =&gt; s.length).cartesian(rdd).foreach { pair =&gt; { println(pair._1) println(pair._2) println(&quot;**&quot; * 8) } } Spark内置的Actions 动作 含义 reduce(func) 聚合操作。之前很多例子都使用了reduce方法。这个功能必须可交换且可关联的，从而可以正确的被并行执行。 collect() 返回rdd中所有的元素，返回值类型是Array。这个方法经常用来取数据量比较小的集合 count() rdd中的元素个数 first() 返回数据集的第一个元素 take(n) 返回数据集的前n个元素，返回值是个Array takeSample(withReplacement, num, [seed]) 返回一个数组，在数据集中随机采样 num 个元素组成，可以选择是否用随机数替换不足的部分，seed 用于指定的随机数生成器种子返回数据集的前n个元素，返回值是个Array takeOrdered(n, [ordering]) 返回自然顺序或者自定义顺序的前 n 个元素 saveAsTextFile(path) 把数据集中的元素写到文件里，可以写到本地文件系统上，hdfs上或者任意Hadoop支持的文件系统上。Spark会调用元素的toString方法将其转换成文本的一行 saveAsSequenceFile(path) 跟saveAsTextFile方法类似，但是是写成SequenceFile文件格式，也是支持写到本地文件系统上，hdfs上或者任意Hadoop支持的文件系统上。这个方法只能作用于键值对的RDD saveAsObjectFile(path) 跟saveAsTextFile方法类似，是使用Java的序列化的方式保存文件 countByKey() 计算键值的数量。对键值对(K, V)的rdd数据集，返回(K, Int)的Map foreach(func) 使用func遍历rdd数据集中的各个元素。这通常用于边缘效果，例如更新一个Accumulator，或者和外部存储系统进行交互 一些Actions操作： rdd.saveAsTextFile(&quot;file:///tmp/data01&quot;) rdd.map(s =&gt; (s, s.length)).saveAsSequenceFile(&quot;file:///tmp/data02&quot;) rdd.map(s =&gt; (s, s.length)).countByKey().foreach { pair =&gt; println(pair._1 + &quot; &quot; + pair._2) } RDD的持久化(Persistence)Spark的一个重要功能就是在将数据集持久化（或缓存）到内存中以便在多个操作中重复使用。当持久化一个RDD的时候，每个存储着这个RDD的分片节点都会计算然后保存到内存中以便下次再次使用。这使得接下来的计算过程速度能够加快（经常能加快超过十倍的速度）。缓存是加快迭代算法和快速交互过程速度的关键工具。 可以使用persist或者cache方法让rdd持久化。在第一次被计算产生之后，它就会始终停留在节点的内存中。Spark的缓存是具有容错性的——如果RDD的任意一个分片丢失了，Spark就会依照这个RDD产生的转化过程自动重算一遍。 另外，每个持久化后的RDD可以使用不用级别的存储级别。比如可以存在硬盘中，可以存在内存中，还可以将这个数据集在节点之间复制，或者使用 Tachyon 将它储存到堆外。这些存储级别都是通过向 persist() 传递一个 StorageLevel 对象（Scala, Java, Python）来设置的。 Spark的一些存储级别如下： 存储级别 含义 MEMORY_ONLY 默认级别。将RDD作为反序列化的的对象存储在JVM中。如果不能被内存装下，一些分区将不会被缓存，并且在需要的时候被重新计算 MEMORY_AND_DISK 默认级别。将RDD作为反序列化的的对象存储在JVM中。如果不能被内存装下，会存在硬盘上，并且在需要的时候被重新计算 MEMORY_ONLY_SER 将RDD作为序列化的的对象进行存储（每一分区占用一个字节数组）。通常来说，这比将对象反序列化的空间利用率更高，尤其当使用fast serializer,但在读取时会比较占用CPU MEMORY_AND_DISK_SER 与MEMORY_ONLY_SER相似，但是把超出内存的分区将存储在硬盘上而不是在每次需要的时候重新计算 DISK_ONLY 只存储RDD分区到硬盘上 MEMORY_ONLY_2, MEMORY_AND_DISK_2 等 与上述的存储级别一样，但是将每一个分区都复制到两个集群结点上 存储级别的选择： 如果你的 RDD 可以很好的与默认的存储级别契合，就不需要做任何修改了。这已经是 CPU 使用效率最高的选项，它使得 RDD的操作尽可能的快。 如果不行，试着使用 MEMORY_ONLY_SER 并且选择一个快速序列化的库使得对象在有比较高的空间使用率的情况下，依然可以较快被访问。 尽可能不要存储到硬盘上，除非计算数据集的函数，计算量特别大，或者它们过滤了大量的数据。否则，重新计算一个分区的速度，和与从硬盘中读取基本差不多快。 如果你想有快速故障恢复能力，使用复制存储级别。例如：用 Spark 来响应web应用的请求。所有的存储级别都有通过重新计算丢失数据恢复错误的容错机制，但是复制存储级别可以让你在 RDD 上持续的运行任务，而不需要等待丢失的分区被重新计算。 如果你想要定义你自己的存储级别，比如复制因子为3而不是2，可以使用 StorageLevel 单例对象的 apply()方法。 共享变量通常情况下，当一个函数在远程集群节点上通过Spark操作(比如map或者reduce)，Spark会对涉及到的变量的所有副本执行这个函数。这些变量都会被拷贝到每台机器上，而且这个过程不会被反馈到驱动程序。通常情况下，在任务之间读写共享变量是很低效的。但是Spark仍然提供了有限的两种共享变量类型用于常见的使用场景：broadcast variables 和 accumulators。 broadcast variables(广播变量)广播变量允许程序员在每台机器上保持一个只读变量的缓存而不是将一个变量的拷贝传递给各个任务。这些变量是可以被使用的，比如，给每个节点传递一份大输入数据集的拷贝是很耗时的。Spark试图使用高效的广播算法来分布广播变量，以此来降低通信花销。可以通过SparkContext.broadcast(v)来从变量v创建一个广播变量。这个广播变量是v的一个包装，同时它的值可以调用value方法获得： val broadcastVar = sc.broadcast(Array(1, 2, 3)) broadcastVar.value // Array(1, 2, 3) 一个广播变量被创建之后，在所有函数中都应当使用它来代替原来的变量v，这样就可以包装v在节点之间只被传递一次。另外，v变量在被广播之后不应该再被修改，这样可以确保每一个节点上储存的广播变量的一致性。 accumulators(累加器)累加器是在一个相关过程中只能被”累加”的变量，对这个变量的操作可以有效地被并行化。它们可以被用于实现计数器（就像在MapReduce过程中）或求和运算。Spark原生支持对数字类型的累加器，程序员也可以为其他新的类型添加支持。累加器被以一个名字创建之后，会在Spark的UI中显示出来。这有助于了解计算的累进过程（注意：目前Python中不支持这个特性）。 可以通过SparkContext.accumulator(v)来从变量v创建一个累加器。在集群中运行的任务随后可以使用add方法或+=操作符（在Scala和Python中）来向这个累加器中累加值。但是，他们不能读取累加器中的值。只有驱动程序可以读取累加器中的值，通过累加器的value方法。 以下的代码展示了向一个累加器中累加数组元素的过程： val accum = sc.accumulator(0, &quot;My Accumulator&quot;) sc.parallelize(Array(1, 2, 3, 4)).foreach(x =&gt; accum += x) accum.value // 10 这段代码利用了累加器对Int类型的内建支持，程序员可以通过继承 AccumulatorParam 类来创建自己想要的类型支持。AccumulatorParam 的接口提供了两个方法：zero 用于为你的数据类型提供零值；addInPlace 用于计算两个值得和。比如，假设我们有一个 Vector类表示数学中的向量，我们可以这样写： object VectorAccumulatorParam extends AccumulatorParam[Vector] { def zero(initialValue: Vector): Vector = { Vector.zeros(initialValue.size) } def addInPlace(v1: Vector, v2: Vector): Vector = { v1 += v2 } } // Then, create an Accumulator of this type: val vecAccum = sc.accumulator(new Vector(...))(VectorAccumulatorParam) 累加器的更新操作只会被运行一次，Spark 提供了保证，每个任务中对累加器的更新操作都只会被运行一次。比如，重启一个任务不会再次更新累加器。在转化过程中，用户应该留意每个任务的更新操作在任务或作业重新运算时是否被执行了超过一次。 累加器不会改变Spark的惰性求值模型。如果累加器在对RDD的操作中被更新了，它们的值只会在启动操作中作为 RDD 计算过程中的一部分被更新。所以，在一个懒惰的转化操作中调用累加器的更新，并没法保证会被及时运行。下面的代码段展示了这一点： accum = sc.accumulator(0) data.map(lambda x =&gt; acc.add(x); f(x)) 参考资料http://spark.apache.org/docs/latest/programming-guide.html","raw":null,"content":null,"categories":[{"name":"spark","slug":"spark","permalink":"http://fangjian0423.github.io/categories/spark/"}],"tags":[{"name":"big data","slug":"big-data","permalink":"http://fangjian0423.github.io/tags/big-data/"},{"name":"spark","slug":"spark","permalink":"http://fangjian0423.github.io/tags/spark/"}]},{"title":"记录Flume使用KafkaSource的时候Channel队列满了之后发生的怪异问题","slug":"flume-channel-full-exception","date":"2016-01-19T12:07:35.000Z","updated":"2016-05-10T14:59:25.000Z","comments":true,"path":"2016/01/19/flume-channel-full-exception/","link":"","permalink":"http://fangjian0423.github.io/2016/01/19/flume-channel-full-exception/","excerpt":"","text":"Flume的这个问题纠结了2个月，因为之前实在太忙了，没有时间来研究这个问题产生的原理，今天终于研究出来了，找出了这个问题所在。 先来描述一下这个问题的现象： Flume的Source用的是KafkaSource，Sink用的是Custom Sink，由于这个Custom Sink写的有一点小问题，比如batchSize是5000次，第4000条就会发生exception，这样每次都会写入4000条数据。Sink处理的时候都会发生异常，每次都会rollback，rollback方面的知识可以参考Flume Transaction介绍。 这样造成的后果有3个： 1.Channel中的数据满了。会发生以下异常： Caused by: org.apache.flume.ChannelFullException: Space for commit to queue couldn&apos;t be acquired. Sinks are likely not keeping up with sources, or the buffer size is too tight 2.Sink会一直写数据，造成数据量暴增。 3.如果用了interceptor，且修改了event中的数据，那么会重复处理这些修改完后的event数据。 前面2个很容易理解，Sink发生异常，transaction rollback，导致channel中的队列满了。 关键是第三点，很让人费解。 以一段伪需求和伪代码为例，TestInterceptor的intercept方法： 比如处理一段json： {&quot;name&quot;: &quot;format&quot;, &quot;languages&quot;: [&quot;java&quot;, &quot;scala&quot;, &quot;javascript&quot;]} 使用interceptor处理成: [{&quot;name&quot;: &quot;format&quot;, &quot;language&quot;: &quot;java&quot;}, {&quot;name&quot;: &quot;format&quot;, &quot;language&quot;: &quot;scala&quot;}, {&quot;name&quot;: &quot;format&quot;, &quot;language&quot;: &quot;javascript&quot;}] interceptor代码如下： public Event intercept(Event event) { Model model = null; String jsonStr = new String(event.getBody(), &quot;UTF-8&quot;); try { model = parseJsonStr(jsonStr); } catch (Exception e) { log.error(&quot;convert json data error&quot;); } event.setBody(model.getJsonString().getBytes()); return event; } 当Channel中的队列已经满了以后，上述代码会打印出convert json data error，而且jsonStr的内容居然是转换后的数据，这一点一开始让我十分费解，误以为transaction rollback之后会修改source中的数据。后来debug源码发现错误在Source中。 后来发现并不是这样的。 KafkaSource中有一个属性eventList，是个ArrayList。用来接收kafka consume的message。 直接说明KafkaSource的process方法源码： public Status process() throws EventDeliveryException { byte[] kafkaMessage; byte[] kafkaKey; Event event; Map&lt;String, String&gt; headers; long batchStartTime = System.currentTimeMillis(); long batchEndTime = System.currentTimeMillis() + timeUpperLimit; try { /** 这里读取kafka中的message **/ boolean iterStatus = false; while (eventList.size() &lt; batchUpperLimit &amp;&amp; System.currentTimeMillis() &lt; batchEndTime) { iterStatus = hasNext(); if (iterStatus) { // get next message MessageAndMetadata&lt;byte[], byte[]&gt; messageAndMetadata = it.next(); kafkaMessage = messageAndMetadata.message(); kafkaKey = messageAndMetadata.key(); // Add headers to event (topic, timestamp, and key) headers = new HashMap&lt;String, String&gt;(); headers.put(KafkaSourceConstants.TIMESTAMP, String.valueOf(System.currentTimeMillis())); headers.put(KafkaSourceConstants.TOPIC, topic); headers.put(KafkaSourceConstants.KEY, new String(kafkaKey)); if (log.isDebugEnabled()) { log.debug(&quot;Message: {}&quot;, new String(kafkaMessage)); } event = EventBuilder.withBody(kafkaMessage, headers); eventList.add(event); } if (log.isDebugEnabled()) { log.debug(&quot;Waited: {} &quot;, System.currentTimeMillis() - batchStartTime); log.debug(&quot;Event #: {}&quot;, eventList.size()); } } /** 这里读取kafka中的message **/ // If we have events, send events to channel // clear the event list // and commit if Kafka doesn&apos;t auto-commit if (eventList.size() &gt; 0) { // 使用ChannelProcess将Source中读取的数据给各个Channel // 如果getChannelProcessor().processEventBatch(eventList);发生了异常，eventList不会被清空，而且processEventBatch方法会调用Interceptor处理event中的数据，event中的数据已经被转换。所以下一次会将转换后的event数据再次传给Interceptor。 getChannelProcessor().processEventBatch(eventList); eventList.clear(); if (log.isDebugEnabled()) { log.debug(&quot;Wrote {} events to channel&quot;, eventList.size()); } if (!kafkaAutoCommitEnabled) { // commit the read transactions to Kafka to avoid duplicates consumer.commitOffsets(); } } if (!iterStatus) { if (log.isDebugEnabled()) { log.debug(&quot;Returning with backoff. No more data to read&quot;); } return Status.BACKOFF; } return Status.READY; } catch (Exception e) { log.error(&quot;KafkaSource EXCEPTION, {}&quot;, e); return Status.BACKOFF; } } 上述代码已经加了备注，再重申一下：ChannelProcess的processEventBatch方法会调用Interceptor处理event中的数据。所以如果Channel中的队列满了，那么processEventBatch方法会发生异常，发生异常之后eventList中的没有进入channel的数据已经被Interceptor修改，且不会被清空。因此下次还是会使用这些数据，所以会发生convert json data error错误。 画了一个序列图如下： 第6步添加event到channel中的时候，队列已满，所以会抛出异常。最终异常被KafkaSource捕捉，但是eventList内部的部分数据已经被interceptor修改过。 多个channel的影响： 如果有多个channel，这个问题也会影响。比如有2个channel，c1和c2。c1的sink没有问题，一直稳定执行，c2对应的sink是一个CustomSink，会有问题。这样c2中的队列迟早会爆满，爆满之后，ChannelProcess批量处理event的时候，由于c2的队列满了，所以Source中的eventList永远不会被清空，eventList永远不会被清空的话，所有的channel都会被影响到，这就好比水源被污染之后，所有的用水都会受到影响。 举个例子：source为s1，c1对应的sink是k1，c2对应的sink是k2。k1和k2的batchSize都是5000，k2处理第4000条数据的时候总会发生异常，进行回滚。k1很稳定。这样c2迟早会爆满，爆满之后s1的eventList一直不能clear，这样也会导致c1一直在处理，所以k1的数据量跟k2一样也会暴增。 要避免本文所说的这一系列情况，最好的做法就是sink必须要加上很好的异常处理机制，不是任何情况都可以rollback的，要根据需求做对应的处理。","raw":null,"content":null,"categories":[{"name":"flume","slug":"flume","permalink":"http://fangjian0423.github.io/categories/flume/"}],"tags":[{"name":"big data","slug":"big-data","permalink":"http://fangjian0423.github.io/tags/big-data/"},{"name":"flume","slug":"flume","permalink":"http://fangjian0423.github.io/tags/flume/"}]},{"title":"Kafka介绍","slug":"kafka-intro","date":"2016-01-13T15:54:51.000Z","updated":"2016-05-10T14:59:25.000Z","comments":true,"path":"2016/01/13/kafka-intro/","link":"","permalink":"http://fangjian0423.github.io/2016/01/13/kafka-intro/","excerpt":"","text":"Kafka介绍Kafka是一个分布式的发布-订阅消息系统(Producer-Consumer)，是一种快速、可扩展的、分区的和可复制的日志服务。 kafka中的几个概念： Topic：用来区别各种message。比如A系统的所有message的Topic为A，B系统的所有message的Topic为B。 Broker：已发布的消息保存在一组服务器中，这组服务器就被称为Broker或Kafka集群。 Producer：生产者，用于发布消息，发布消息到kafka broker。 Consumer：消费者，订阅消息，订阅kafka broker中的已经被发布的消息。 下图是几个概念的说明： producer发布消息到kafka cluster(也就是kafka broker)，然后发布后的这些消息被consumer订阅。 从图中也可以看出来，kafka支持多个producer和多个consumer。 Kafka存储机制Partition：Kafka中每个Topic都会有一个或多个Partition，由于kafka将数据直接写到硬盘里，这里的Partition对应一个文件夹，文件夹下存储这个Partition的所有消息和索引。如果有2个Topic，分别有3个和4个Partition。那么总共有7个文件夹。Kafka内部会根据一个算法，根据消息得出一个值，然后根据这个值放入对应的partition目录中的段文件里。 比如在一台机器上创建partition为3，topic为test01和partition为4，topic为test02的2个topic。 创建完之后 /tmp/kafka-logs中就会有7个文件夹，分别是 test01-0test01-1test01-2test02-0test02-1test02-2test02-3 Segment：组成Partiton的组件。一个Partition代表一个文件夹，而Segment则是这个文件夹下的各个文件。每个Segmenet文件有大小限制，在配置文件中用log.segment.bytes配置。 log.segment.bytes=1073741824 当文件的大小超过1073741824字节的时候，会创建第一个段文件。需要注意的是这里每个段文件中的消息数量不一定相等，因为虽然他们的字节数一样，但是每个消息的字节数是不一样的，所以每个段文件中的消息数量不一定相等。 每个段文件由2部分组成，分别是index file和log file，表示索引文件和日志(数据)文件。这2个文件一一对应。 第一个segment文件从0开始，后续每个segment文件名是上一个segment文件的最后一条message的offset值，数值最大为64位long大小，19位数字字符长度，没有数字用0填充。 下面是做的一个例子，partition和replication-factor都为1，每个segmenet文件的大小是5M。有500000条message，一共生成了4对文件，这里00000000000000137200.log文件表示是00000000000000000000.log中存储了137199个message，这个文件开始存储第137200个message。 00000000000000000000.index00000000000000000000.log 00000000000000137200.index00000000000000137200.log 00000000000000271600.index00000000000000271600.log 00000000000000406000.index00000000000000406000.log offset：用来标识message在partition中的下标，用来定位message。 Kafka内部存储结构可以参考Kafka文件存储机制那些事文章里的讲解。 一个kafka producer例子： import java.util.Properties import kafka.producer.{KeyedMessage, Producer, ProducerConfig} object TestProducer extends App { val events = 500000 val props = new Properties() val brokers = &quot;localhost:9092&quot; props.put(&quot;metadata.broker.list&quot;, brokers) props.put(&quot;serializer.class&quot;, &quot;kafka.serializer.StringEncoder&quot;) props.put(&quot;producer.type&quot;, &quot;async&quot;) val config = new ProducerConfig(props) val topic = &quot;format03&quot; val producer = new Producer[String, String](config) for(nEvents &lt;- Range(0, events)) { val msg = &quot;Message&quot; + nEvents val data = new KeyedMessage[String, String](topic, msg) producer.send(data) } producer.close() } 参考资料Kafka文件存储机制那些事Kafka剖析（一）：Kafka背景及架构介绍Kafka设计解析（二）：Kafka High Availability （上）Kafka设计解析（三）：Kafka High Availability （下）","raw":null,"content":null,"categories":[{"name":"kafka","slug":"kafka","permalink":"http://fangjian0423.github.io/categories/kafka/"}],"tags":[{"name":"big data","slug":"big-data","permalink":"http://fangjian0423.github.io/tags/big-data/"},{"name":"log","slug":"log","permalink":"http://fangjian0423.github.io/tags/log/"}]},{"title":"Flume Transaction介绍","slug":"flume-transaction","date":"2016-01-03T09:35:53.000Z","updated":"2016-05-10T14:59:25.000Z","comments":true,"path":"2016/01/03/flume-transaction/","link":"","permalink":"http://fangjian0423.github.io/2016/01/03/flume-transaction/","excerpt":"","text":"Flume中有一个Transaction的概念。本文仅分析Transaction的实现类MemoryTransaction的实现原理，JdbcTransaction的原理跟数据库中的Transaction类似。 Transaction接口定义如下： public void begin(); public void commit(); public void rollback(); public void close(); Transaction跟数据库中的Transaction概念类似，都有begin，commit，rollback，close方法。 Flume中的Transaction是在Channel中使用的，主要用来处理Source数据进入Channel的过程和Channel中的数据被Sink处理的过程。下面会从这2个方面根据源码分析Transaction的原理。 在分析具体的事务操作之前，看一下MemoryTransaction中的各个方法实现原理。 首先先看一下事务的获取方法： @Override public Transaction getTransaction() { if (!initialized) { synchronized (this) { if (!initialized) { initialize(); initialized = true; } } } // currentTransaction是一个ThreadLocal对象 BasicTransactionSemantics transaction = currentTransaction.get(); // 如果是第一次获取事务或者当前事务的已经close。那么会重新create一个新的事务 if (transaction == null || transaction.getState().equals( BasicTransactionSemantics.State.CLOSED)) { transaction = createTransaction(); currentTransaction.set(transaction); } return transaction; } 再重复一下，第一次拿事务或者事务关闭之后，才会重新去构造一个新的事务。各个线程之间的事务都是独立的 MemoryTransaction是MemoryChannel中的一个内部类。 然后介绍一下MemoryTransaction和MemoryChannel中的几个重要属性。 MemoryTransaction中有2个阻塞队列，分别是putList和takeList。putList放Source进来的数据，Sink从MemoryChannel中的queue中拿数据，然后这个数据丢到takeList中。 MemoryChannel中有个阻塞队列queue。每次事务commit的时候都会把putList中的数据丢到queue中。 begin方法MemoryTransaction没做任何处理，就不分析了。 put方法： @Override protected void doPut(Event event) throws InterruptedException { channelCounter.incrementEventPutAttemptCount(); int eventByteSize = (int)Math.ceil(estimateEventSize(event)/byteCapacitySlotSize); if (!putList.offer(event)) { throw new ChannelException( &quot;Put queue for MemoryTransaction of capacity &quot; + putList.size() + &quot; full, consider committing more frequently, &quot; + &quot;increasing capacity or increasing thread count&quot;); } putByteCounter += eventByteSize; } put方法把数据丢入putList中，这个也就是之前分析的putList这个属性的作用，putList放Source进来的数据。 commit方法的关键性代码： @Override protected void doCommit() throws InterruptedException { int puts = putList.size(); int takes = takeList.size(); synchronized(queueLock) { if(puts &gt; 0 ) { // 清空putList，丢到外部类MemoryChannel中的queue队列里 while(!putList.isEmpty()) { // MemoryChannel中的queue队列 if(!queue.offer(putList.removeFirst())) { throw new RuntimeException(&quot;Queue add failed, this shouldn&apos;t be able to happen&quot;); } } } putList.clear(); takeList.clear(); } } rollback方法关键性代码： @Override protected void doRollback() { int takes = takeList.size(); synchronized(queueLock) { Preconditions.checkState(queue.remainingCapacity() &gt;= takeList.size(), &quot;Not enough space in memory channel &quot; + &quot;queue to rollback takes. This should never happen, please report&quot;); // 把takeList中的数据放回到queue中 while(!takeList.isEmpty()) { queue.addFirst(takeList.removeLast()); } putList.clear(); } } 发生异常后才会调用rollback方法。也就是说take方法被调用之后，由于take方法是从queue中拿数据，并且放到takeList里。所以回滚的时候需要把takeList中的数据还给queue。 MemoryTransaction的close方法只是把状态改成了CLOSED，其他没做什么，就不分析了。 MemoryTransaction的take方法： take方法从queue中拉出数据，然后放到takeList中。 @Override protected Event doTake() throws InterruptedException { channelCounter.incrementEventTakeAttemptCount(); if(takeList.remainingCapacity() == 0) { throw new ChannelException(&quot;Take list for MemoryTransaction, capacity &quot; + takeList.size() + &quot; full, consider committing more frequently, &quot; + &quot;increasing capacity, or increasing thread count&quot;); } if(!queueStored.tryAcquire(keepAlive, TimeUnit.SECONDS)) { return null; } Event event; synchronized(queueLock) { event = queue.poll(); } Preconditions.checkNotNull(event, &quot;Queue.poll returned NULL despite semaphore &quot; + &quot;signalling existence of entry&quot;); takeList.put(event); int eventByteSize = (int)Math.ceil(estimateEventSize(event)/byteCapacitySlotSize); takeByteCounter += eventByteSize; return event; } Source数据进入Channel过程中Transaction的处理过程： ChannelProcessor处理这个过程： for (Channel reqChannel : reqChannelQueue.keySet()) { // 获取事务 Transaction tx = reqChannel.getTransaction(); Preconditions.checkNotNull(tx, &quot;Transaction object must not be null&quot;); try { // 事务开始 tx.begin(); // 获取Source处理的一个批次中的所有Event List&lt;Event&gt; batch = reqChannelQueue.get(reqChannel); for (Event event : batch) { // MemoryChannel的put方法会MemoryTransaction的put方法。 reqChannel.put(event); } // 提交事务 tx.commit(); } catch (Throwable t) { // 发生异常回滚事务 tx.rollback(); if (t instanceof Error) { LOG.error(&quot;Error while writing to required channel: &quot; + reqChannel, t); throw (Error) t; } else { throw new ChannelException(&quot;Unable to put batch on required &quot; + &quot;channel: &quot; + reqChannel, t); } } finally { if (tx != null) { // 最后结束事务 tx.close(); } } } Channel中的数据被Sink处理的过程： 以hdfs sink为例讲解： public Status process() throws EventDeliveryException { Channel channel = getChannel(); Transaction transaction = channel.getTransaction(); List&lt;BucketWriter&gt; writers = Lists.newArrayList(); transaction.begin(); try { int txnEventCount = 0; for (txnEventCount = 0; txnEventCount &lt; batchSize; txnEventCount++) { Event event = channel.take(); if (event == null) { break; } ... transaction.commit(); if (txnEventCount &lt; 1) { return Status.BACKOFF; } else { sinkCounter.addToEventDrainSuccessCount(txnEventCount); return Status.READY; } } catch (IOException eIO) { transaction.rollback(); LOG.warn(&quot;HDFS IO error&quot;, eIO); return Status.BACKOFF; } catch (Throwable th) { transaction.rollback(); LOG.error(&quot;process failed&quot;, th); if (th instanceof Error) { throw (Error) th; } else { throw new EventDeliveryException(th); } } finally { transaction.close(); } } 也是一样的流程，begin，take，commit or rollback，close。 总结： MemoryTransaction是MemoryChannel中的一个内部类，内部有2个阻塞队列putList和takeList。MemoryChannel内部有个queue阻塞队列。 putList接收Source交给Channel的event数据，takeList保存Channel交给Sink的event数据。 如果是Source交给Channel任务完成，进行commit的时候。会把putList中的所有event放到MemoryChannel中的queue。 如果是Source交给Channel任务失败，进行rollback的时候。程序就不会继续走下去，比如KafkaSource需要commitOffsets，如果任务失败就不会commitOffsets。 如果是Sink处理完Channel带来的event，进行commit的时候。会清空takeList中的event数据，因为已经没consume。 如果是Sink处理Channel带来的event失败的话，进行rollback的时候。会把takeList中的event写回到queue中。 缺点： Flume的Transaction跟数据库的Transaction不一样。数据库中的事务回滚之后所有操作的数据都会进行处理。而Flume的却不能还原。比如HDFSSink写数据到HDFS的时候需要rollback，比如本来要写入10000条数据，但是写到5000条的时候rollback，那么已经写入的5000条数据不能回滚，而那10000条数据回到了阻塞队列里，下次再写入的时候还会重新写入这10000条数据。这样就多了5000条重复数据，这是flume设计上的缺陷。","raw":null,"content":null,"categories":[{"name":"flume","slug":"flume","permalink":"http://fangjian0423.github.io/categories/flume/"}],"tags":[{"name":"big data","slug":"big-data","permalink":"http://fangjian0423.github.io/tags/big-data/"},{"name":"flume","slug":"flume","permalink":"http://fangjian0423.github.io/tags/flume/"}]},{"title":"2015总结","slug":"2015_end","date":"2015-12-31T16:22:35.000Z","updated":"2016-05-10T14:59:25.000Z","comments":true,"path":"2016/01/01/2015_end/","link":"","permalink":"http://fangjian0423.github.io/2016/01/01/2015_end/","excerpt":"","text":"2015年总结又是一年年底，2015年年底了。总结，总结，总结，还是分3个部分。 工作今年2月份春节过完之后就换了工作，之后在这家公司工作到了现在。 来新公司主要的目的是为了做大数据的，但是一进来就是给别人擦屁股，改bug = =。 后来大数据的东西只做了一部分，大概3个月的时间，之后被拉过去做公众号相关的东西了。没办法，公司人手太少，自己负责的etl这部分还有很多不完善的地方，可惜没有时间继续做下去。 公司人少没时间做是一个原因，但是更重要的原因还是自己懒了。 技术2015年，对技术做以下总结： 继续写博客，15年写了43篇博客。相比去年的34篇博客，今年写的更多了。但是这43篇博客的质量其实都很一般，没有去年写的springmvc源码分析好。 但是想要写出好的文章得花费很长的时间。争取之后的文章写得多又能写得好。 了解了大数据方面的知识。包括hadoop，hdfs，flume，spark，hbase，elasticsearch，sqoop，hive等方面的知识。由于自己在公司负责的是etl方面的内容，所以对flume了解的比较多，然后又用base和elasticsearch存储了一些东西，所以对这两块内容也比较了解。但是对这些东西，也只是停留在使用的基础上，没有深入了解到内部的结构，明年会深入了解这些内容的。 github贡献了几个开源项目。包括flume，一个es教程，waterfall等。flume的HBaseSink在stop的时候居然没有把serializer关闭掉。给flume提了个pull request，但是flume居然不接受在github提出的pull request，只能在apache jira上处理ticket，但是新建了一个ticket之后居然不能assign给任何人，所以也就没人处理了，有点尴尬。 玩了会grails。由于公司内部的后台系统是用grails搭建的，所以自然就得会grails，grails内部用groovy写的。用了之后发现grails的调试在intellij中特别的慢，而且只要进了一个闭包，调试就特别麻烦。grails项目大了之后启动也非常地慢，有时候还会莫名其妙地出现一些错误，重启一下就好了。综上原因，对grails不是非常喜欢。 spring-boot的使用。公司内部发现grails项目大了之后启动会非常慢，后来开始使用micro-service就行新项目的开发。spring-boot其实是各个框架的整合，包括hibernate，spring，springdata等。提供了一些封装好的方便的方法，但是发现使用了spring-boot之后有些它内部定义好的内容你不看文档是不会知道的，而且有的东西文档里也没有说，所以只能看源码。这个算是使用spring-boot的一个弊端吧。 scala的学习。今年把scala in action这本书看完了。这本书很一般，很多scala的东西感觉都没有讲清楚。自己也把spray-json(scala写的一个json库，很小)的源码看了一遍。其实感觉看源码学语法也是不错的一个方法。 可以勉强算一个全栈了。公司技术少，前端更是只有一个。所以今年做了一些前端的工作，感觉自己一个人能搞定一个公众号的前后端了。写前端的时候使用了angularjs。后来又了解了js的一些打包工具，grunt，gulp等。给公司做了《超级邀请》这个公众号的开发任务，这个公众号的前台页面和接口都是自己写的。 了解了java高级部分的一些知识。知道了java并发的一些内容，知道了内置锁，信号量，栅栏，闭锁等知识。java内存模型也了解了一点，知道了happens-before，java的内存通信是通过共享内存实现的。jvm的书买了，但是还没开始看，是个弱项。 代码质量。今年写的代码质量感觉还是很烂，比去年也就稍微好了一点，依旧很烂。希望16年能写出更好的代码，而不仅仅只是为了完成任务的代码。 生活宅，宅，宅。 不过双12买了把ukulele。但是还没开始学 = =，尴尬。不过买了就不会浪费，之后会开始学。 玩了高达模型，跟同事学的。搭了2个MG，武者MK2和迅雷高达。1个bb版强袭高达，年底又买了个pg强袭。pg还没开始搭，搭完绝壁炫酷到爆炸。 附上自己搭的高达图片，就放一张吧。毕竟是写总结的，不是介绍高达的： 2016年计划去年定的2015年计划： 今年真的没看书。 有的书看了一点点就没看了，java并发的书看了100多页就没下文了。。 要养成看书的习惯。 (☑。今年把这本书看完了，还看了scala in action。其实还远远不够) 技术方面的计划：redis(×。看了elasticsearch)docker(×，而是看了大数据)github贡献开源框架(☑)nio(×)并发包(☑)Mina(×)Netty(×)Python深入(×，深入学习了scala)搜索相关的知识(×)看源码的时候多想想作者的思路以及架构方面，不用特别在意细节(☑) 继续写博客(☑) 做让生活变得更有趣的事，比如guitar(☑。ukulele，高达) 2015的计划虽然只完成了一半，但是由于工作中接触大数据。所以很多内容都没看，转而去看大数据方面的知识了。所以总体完成度还是可以的，算80%。 2016年计划以及展望： 继续看书，要看更多的书。 15年居然只看了两本书，对不起自己…. 16年要5本+。 技术方面大数据的深入学习，不仅仅局限于会使用。包括spark，es，hbase，hadoop等。分布式方面的学习dockerscala的继续深入，要开始用scala写代码nettygithub继续贡献开源项目jvm 继续写博客 继续玩高达 学会ukulele 希望能做一些逼格高一点的东西，而不仅仅是做一些功能性的东西","raw":null,"content":null,"categories":[{"name":"总结","slug":"总结","permalink":"http://fangjian0423.github.io/categories/总结/"}],"tags":[{"name":"杂事","slug":"杂事","permalink":"http://fangjian0423.github.io/tags/杂事/"}]},{"title":"spray-json源码分析","slug":"scala-spray-json","date":"2015-12-22T17:47:35.000Z","updated":"2016-05-10T14:59:25.000Z","comments":true,"path":"2015/12/23/scala-spray-json/","link":"","permalink":"http://fangjian0423.github.io/2015/12/23/scala-spray-json/","excerpt":"","text":"spray-json介绍以及内部结构spray-json是scala的一个轻量的，简洁的，简单的关于JSON实现。 同时也是spray项目的json模块。 本文分析spray-json的源码。 在分析spray-json的源码之前，我们先介绍一下spray-json的使用方法以及里面的几个概念。 首先是spray-json的一个使用例子，里面有各种黑魔法： val str = &quot;&quot;&quot;{ &quot;name&quot;: &quot;Ed&quot;, &quot;age&quot;: 24 }&quot;&quot;&quot; // 黑魔法。不是String的parseJson方法，而是使用了隐式转换，隐式转换成PimpedString类。PimpedString里有parseJson方法，转换成JsValue对象 val jsonVal = str.parseJson // jsonVal是个JsObject对象的实例 // jsonVal是个JsObject对象，也是个JsValue实例。JsValue对象都有compactPrint和prettyPrint方法 println(jsonVal.compactPrint) // 压缩打印 println(jsonVal.prettyPrint) // 格式化打印 // 手动构建一个JsObject val jsonObj = JsObject( (&quot;name&quot;, JsString(&quot;format&quot;)), (&quot;age&quot;, JsNumber(99)) ) println(jsonObj.compactPrint) println(jsonObj.prettyPrint) // 黑方法，不是List的toJson方法，而是使用了隐式转换，隐式转换成PimpedAny类，PimpedAny类里有toList方法，转换成对应的类型 val jsonList = List(1,2,3).toJson // JsValue的toString方法引用了compactPrint方法 println(jsonList) 然后是spray-json里的几个概念介绍： 1.JsValue: 抽象类。对原生json中的各种数据类型的抽象。它的实现类JsArray对应json里的数组；JsBoolean对应json里的布尔值；JsNumber对应json里的数值 … 2.JsonFormat: 一个trait，是json序列化和反序列话的抽象。继承JsonReader(JsValue转换成对象的抽象)和JsonWriter(对象转换成JsValue的抽象)。BasicFormats和CollectionFormats、AdditionalFormats等这些trait里都有各种JsonFormat的隐式转换。 3.RootJsonFormat。json文档的抽象，跟JsonFormat一样，只不过RootJsonFormat只支持JsArray和JsObject。因为这是一个json文档对象，只有一个json对象或者一个json数组才能称得上是一个json文档对象。 4.JsonPrinter: 一个trait。json打印成字符串的抽象。具体的实现特质有CompactPrinter(压缩后的字符串)和PrettyPrinter(格式化后的字符串) 5.DefaultJsonProtocol: 继承BasicFormats，混入StandardFormats、CollectionFormats、ProductFormats、AdditionalFormats的特质。我们需要转换一些基础类型或者集合类型的时候需要import这个trait。 json package对象json package对象里定义了一些隐式转换方法和一些实用方法。 package object json { // JsField。 一个二元元组，代表json中的一个项(key为String，value为任意json类型) type JsField = (String, JsValue) // 反序列化异常 def deserializationError(msg: String, cause: Throwable = null, fieldNames: List[String] = Nil) = throw new DeserializationException(msg, cause, fieldNames) // 序列化异常 def serializationError(msg: String) = throw new SerializationException(msg) // jsonReader方法，是个泛型。使用了隐式参数，返回值是这个隐式参数的引用。也就是说只要调用了jsonReader方法，那么就会自动去找对应泛型类型的实现 def jsonReader[T](implicit reader: JsonReader[T]) = reader // 跟jsonReader方法一个道理。只要调用了jsonWriter方法，那么就会自动去找对应泛型类型的实现 def jsonWriter[T](implicit writer: JsonWriter[T]) = writer // 隐式转换方法。上面例子的toList使用了这个隐式转换 implicit def pimpAny[T](any: T) = new PimpedAny(any) // 隐式方法。上面例子的parseJson使用了这个隐式转换 implicit def pimpString(string: String) = new PimpedString(string) } package json { // 反序列异常类的定义，上面的deserializationError方法实例化了这个类 case class DeserializationException(msg: String, cause: Throwable = null, fieldNames: List[String] = Nil) extends RuntimeException(msg, cause) // 序列异常类的定义，上面的serializationError方法实例化了这个类 class SerializationException(msg: String) extends RuntimeException(msg) // 上面的隐式方法pimpAny实例化了这个类。黑魔法toJson方法，不是List的toJson方法，而是List隐式转换成PimpedAny，然后调用PimpedAny的toJson方法。toJson方法的参数是个隐式参数，跟上面代码里的jsonWriter方法一样，会找对应泛型类型的JsonWriter实现类，然后调用JsonWriter的write方法 private[json] class PimpedAny[T](any: T) { def toJson(implicit writer: JsonWriter[T]): JsValue = writer.write(any) } // 上面的隐式方法pimpString实例化了这个类。黑魔法parseJson方法，不是String的parseJson方法，而是String隐式转换成PimpedString，然后调用PimpedString的parseJson方法 private[json] class PimpedString(string: String) { @deprecated(&quot;deprecated in favor of parseJson&quot;, &quot;1.2.6&quot;) def asJson: JsValue = parseJson def parseJson: JsValue = JsonParser(string) } } JsValue(原生json中的各种数据类型的抽象)JsValue是原生json中各种数据类型的抽象，是个抽象类，直接看JsValue的定义: sealed abstract class JsValue { // 重载的toString方法引用了compactPrint方法，会打印出json的压缩格式 override def toString = compactPrint def toString(printer: (JsValue =&gt; String)) = printer(this) // 压缩打印，使用了CompactPrinter。CompactPrinter是一个JsonPrinter的子类 def compactPrint = CompactPrinter(this) // 格式化打印，PrettyPrinter。PrettyPrinter也是一个JsonPrinter的子类 def prettyPrint = PrettyPrinter(this) // 转换成对应的类。jsonReader方法在json package里定义，已经分析过。会找对应那个的JsonReader实现类。然后调用read方法 def convertTo[T :JsonReader]: T = jsonReader[T].read(this) // 转换成JsObject对象，除了JsObject对象重写了这个，返回了自身。其他类型的JsValue都会抛出DeserializationException异常 def asJsObject(errorMsg: String = &quot;JSON object expected&quot;): JsObject = deserializationError(errorMsg) def asJsObject: JsObject = asJsObject() @deprecated(&quot;Superceded by &apos;convertTo&apos;&quot;, &quot;1.1.0&quot;) def fromJson[T :JsonReader]: T = convertTo } JsNumber是数值类型的抽象： case class JsNumber(value: BigDecimal) extends JsValue // JsNumber中定义了几个方便的构造方法 object JsNumber { val zero: JsNumber = apply(0) def apply(n: Int) = new JsNumber(BigDecimal(n)) def apply(n: Long) = new JsNumber(BigDecimal(n)) def apply(n: Double) = n match { case n if n.isNaN =&gt; JsNull case n if n.isInfinity =&gt; JsNull case _ =&gt; new JsNumber(BigDecimal(n)) } def apply(n: BigInt) = new JsNumber(BigDecimal(n)) def apply(n: String) = new JsNumber(BigDecimal(n)) def apply(n: Array[Char]) = new JsNumber(BigDecimal(n)) } JsString是字符串类型的抽象： case class JsString(value: String) extends JsValue // JsString中定义了几个方便的方法 object JsString { val empty = JsString(&quot;&quot;) def apply(value: Symbol) = new JsString(value.name) } JsObject是对象类型的抽象： case class JsObject(fields: Map[String, JsValue]) extends JsValue { // 重写了asJsObject方法 override def asJsObject(errorMsg: String) = this // 根据字段名获得对应的JsValue def getFields(fieldNames: String*): immutable.Seq[JsValue] = fieldNames.flatMap(fields.get)(collection.breakOut) } object JsObject { val empty = JsObject(Map.empty[String, JsValue]) // 使用多个JsField构造JsObject。这里的JsField就是代表一个(String, JsValue) def apply(members: JsField*) = new JsObject(Map(members: _*)) @deprecated(&quot;Use JsObject(JsValue*) instead&quot;, &quot;1.3.0&quot;) def apply(members: List[JsField]) = new JsObject(Map(members: _*)) } JsonFormat(JsonWriter和JsonReader的子类)一个trait，是json序列化和反序列话的抽象。继承JsonReader(JsValue转换成对象的抽象)和JsonWriter(对象转换成JsValue的抽象)。 JsonReader的定义，把一个JsValue转换成对应的类型： trait JsonReader[T] { def read(json: JsValue): T } JsonWriter的定义，把一个类型转换成JsValue： trait JsonWriter[T] { def write(obj: T): JsValue } json package对象里的jsonReader和jsonWriter方法有个隐式参数，我们也分析过：只要调用了jsonReader(JsonWriter)方法，那么就会自动去找对应泛型类型的实现。 AdditionalFormats、BasicFormats、CollectionFormats、StandardFormats等都定义了各种JsonFormat。 比如Int类型就找IntJsonFormat，String类型就找StringJsonFormat … 这些基础类型的JsonFormat都定义在BasicFormats这个trait中。 我们就分析几个BasicFormats中定义的基础类型JsonFormat： Int基本类型的JsonFormat： implicit object IntJsonFormat extends JsonFormat[Int] { // write方法继承自JsonWriter。 直接实例化一个JsNumber对象 def write(x: Int) = JsNumber(x) // read方法继承自JsonReader。读取JsNumber中对应的值 def read(value: JsValue) = value match { case JsNumber(x) =&gt; x.intValue case x =&gt; deserializationError(&quot;Expected Int as JsNumber, but got &quot; + x) } } String基本类型的JsonFormat： implicit object StringJsonFormat extends JsonFormat[String] { // write方法实例化一个JsString def write(x: String) = { require(x ne null) JsString(x) } // read方法读取JsString中对应的字符串 def read(value: JsValue) = value match { case JsString(x) =&gt; x case x =&gt; deserializationError(&quot;Expected String as JsString, but got &quot; + x) } } ….. CollectionFormats中定义了几个集合类的JsonFormat: List类型的JsonFormat： implicit def listFormat[T :JsonFormat] = new RootJsonFormat[List[T]] { // 将List转换成JsArray对象。遍历list中的各个元素，对每个元素调用toJson方法。最后JsArray里的每个元素都是JsValue def write(list: List[T]) = JsArray(list.map(_.toJson).toVector) // JsArray转换成List。对JsArray里的各个JsValue调用convertTo转换成对应的类型 def read(value: JsValue): List[T] = value match { case JsArray(elements) =&gt; elements.map(_.convertTo[T])(collection.breakOut) case x =&gt; deserializationError(&quot;Expected List as JsArray, but got &quot; + x) } } Map类型的JsonFormat： implicit def mapFormat[K :JsonFormat, V :JsonFormat] = new RootJsonFormat[Map[K, V]] { // 遍历Map中的每一个二元元组。如果元组的第一项不是String，直接抛出SerializationException异常。否则构造key为元组第一项字符串，value为元组第二项的JsVaue对象。 def write(m: Map[K, V]) = JsObject { m.map { field =&gt; field._1.toJson match { case JsString(x) =&gt; x -&gt; field._2.toJson case x =&gt; throw new SerializationException(&quot;Map key must be formatted as JsString, not &apos;&quot; + x + &quot;&apos;&quot;) } } } def read(value: JsValue) = value match { case x: JsObject =&gt; x.fields.map { field =&gt; (JsString(field._1).convertTo[K], field._2.convertTo[V]) } (collection.breakOut) case x =&gt; deserializationError(&quot;Expected Map as JsObject, but got &quot; + x) } } AdditionalFormats中定义了一些helper和额外的JsonFormat： // JsValue的JsonFormat，JsValue调用convertTo或者toJson返回的就是自身 implicit object JsValueFormat extends JsonFormat[JsValue] { def write(value: JsValue) = value def read(value: JsValue) = value } implicit object RootJsObjectFormat extends RootJsonFormat[JsObject] { def write(value: JsObject) = value def read(value: JsValue) = value.asJsObject } StandardFormats中定义了Option、Either的JsonFormat，1元-7元元组的JsonFormat。 implicit def tuple1Format[A :JF] = new JF[Tuple1[A]] { def write(t: Tuple1[A]) = t._1.toJson def read(value: JsValue) = Tuple1(value.convertTo[A]) } implicit def tuple2Format[A :JF, B :JF] = new RootJsonFormat[(A, B)] { def write(t: (A, B)) = JsArray(t._1.toJson, t._2.toJson) def read(value: JsValue) = value match { case JsArray(Seq(a, b)) =&gt; (a.convertTo[A], b.convertTo[B]) case x =&gt; deserializationError(&quot;Expected Tuple2 as JsArray, but got &quot; + x) } } JsonPrinter(将JsValue打印成原生json字符串)JsonPrinter继承一个函数对象，这个函数的输入是个JsValue，输出是String： trait JsonPrinter extends (JsValue =&gt; String) 内部定义了一个抽象方法： def print(x: JsValue, sb: JStringBuilder) CompactPrinter继承JsonPrinter，压缩打印： 实现的print方法： def print(x: JsValue, sb: StringBuilder) { x match { case JsObject(x) =&gt; printObject(x, sb) case JsArray(x) =&gt; printArray(x, sb) case _ =&gt; printLeaf(x, sb) } } PrettyPrinter也继承JsonPrinter，格式化打印： 实现的print方法： def print(x: JsValue, sb: StringBuilder) { print(x, sb, 0) } // indent参数是格式化打印的关键 protected def print(x: JsValue, sb: StringBuilder, indent: Int) { x match { case JsObject(x) =&gt; printObject(x, sb, indent) case JsArray(x) =&gt; printArray(x, sb, indent) case _ =&gt; printLeaf(x, sb) } } DefaultJsonProtocol(整合了多个JsonFormat)直接看源码： trait DefaultJsonProtocol extends BasicFormats with StandardFormats with CollectionFormats with ProductFormats with AdditionalFormats object DefaultJsonProtocol extends DefaultJsonProtocol","raw":null,"content":null,"categories":[{"name":"scala","slug":"scala","permalink":"http://fangjian0423.github.io/categories/scala/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"http://fangjian0423.github.io/tags/jvm/"},{"name":"scala","slug":"scala","permalink":"http://fangjian0423.github.io/tags/scala/"}]},{"title":"Scala 隐式转换和隐式参数","slug":"scala-implicit","date":"2015-12-20T07:38:22.000Z","updated":"2016-05-10T14:59:25.000Z","comments":true,"path":"2015/12/20/scala-implicit/","link":"","permalink":"http://fangjian0423.github.io/2015/12/20/scala-implicit/","excerpt":"","text":"前言Scala的implicit功能很强大，可以自动地给对象”添加一个属性”。 这里打上引号的原因是Scala内部进行编译的时候会自动加上隐式转换函数。 很多Scala开源框架内部都大量使用了implicit。因为implicit真的很强大，写得好的implicit可以让代码更优雅。但个人感觉implicit也有一些缺点，比如使用了implicit之后，看源码或者使用一些library的时候无法下手，因为你根本不知道作者哪里写了implicit。这个也会对初学者造成一些困扰。 比如Scala中Option就有一个implicit可以将Option转换成Iterable： val list = List(1, 2) val map = Map(1 -&gt; 11, 2 -&gt; 22, 3 -&gt; 33) val newList = list.flatMap { num =&gt; map.get(num) // map.get方法返回的是Option，可以被隐式转换成Iterable } 以下是implicit的一个小例子。 比如以下一个例子，定义一个Int类型的变量num，但是赋值给了一个Double类型的数值。这时候就会编译错误： val num: Int = 3.5 // Compile Error 但是我们加了一个隐式转换之后，就没问题了: implicit def double2Int(d: Double) = d.toInt val num: Int = 3.5 // 3， 这段代码会被编译成 val num: Int = double2Int(3.5) 隐式转换规则标记规则(Marking Rule)任何变量，函数或者对象都可以用implicit这个关键字进行标记，表示可以进行隐式转换。 implicit def intToString(x: Int) = x.toString 编译器可能会将x + y 转换成 convert(x) + y 如果convert被标记成implicit。 作用域规则(Scope Rule)在一个作用域内，一个隐式转换必须是一个唯一的标识。 比如说MyUtils这个object里有很多隐式转换。x + y 不会使用MyUtils里的隐式转换。 除非import进来。 import MyUtils._ Scala编译器还能在companion class中去找companion object中定义的隐式转换。 object Player { implicit def getClub(player: Player): Club = Club(player.clubName) } class Player(val name: String, val age: Int, val clubName: String) { } val p = new Player(&quot;costa&quot;, 27, &quot;Chelsea&quot;) println(p.welcome) // Chelsea welcome you here! println(p.playerNum) // 21 一次编译只隐式转换一次(One-at-a-time Rule)Scala不会把 x + y 转换成 convert1(convert2(x)) + y 隐式转换类型隐式转换成正确的类型这种类型是Scala编译器对隐式转换的第一选择。 比如说编译器看到一个类型的X的数据，但是需要一个类型为Y的数据，那么就会去找把X类型转换成Y类型的隐式转换。 本文一开始的double2Int方法就是这种类型的隐式转换。 implicit def double2Int(d: Double) = d.toInt val num: Int = 3.5 // 3 当编译器发现变量num是个Int类型，并且用Double类型给它赋值的时候，会报错。 但是在报错之前，编译器会查找Double =&gt; Int的隐式转换。然后发现了double2Int这个隐式转换函数。于是就使用了隐式转换。 方法调用的隐式转换比如这段代码 obj.doSomeThing。 比如obj对象没有doSomeThing这个方法，编译器会会去查找拥有doSomeThing方法的类型，并且看obj类型是否有隐式转换成有doSomeThing类型的函数。有的话就是将obj对象隐式转换成拥有doSomeThing方法的对象。 以下是一个例子： case class Person(name: String, age: Int) { def +(num: Int) = age + num def +(p: Person) = age + p.age } val person = Person(&quot;format&quot;, 99) println(person + 1) // 100 // println(1 + person) 报错，因为Int的+方法没有有Person参数的重载方法 implicit def personAddAge(x: Int) = Person(&quot;unknown&quot;, x) println(1 + person) // 100 有了隐式转换方法之后，编译器检查 1 + person 表达式，发现Int的+方法没有有Person参数的重载方法。在放弃之前查看是否有将Int类型的对象转换成以Person为参数的+方法的隐式转换函数，于是找到了，然后就进行了隐式转换。 Scala的Predef中也使用了方法调用的隐式转换。 Map(1 -&gt; 11, 2 -&gt; 22) 上面这段Map中的参数是个二元元组。 Int没有 -&gt; 方法。 但是在Predef中定义了： implicit final class ArrowAssoc[A](private val self: A) extends AnyVal { @inline def -&gt; [B](y: B): Tuple2[A, B] = Tuple2(self, y) def →[B](y: B): Tuple2[A, B] = -&gt;(y) } 隐式参数隐式参数的意义是当方法需要多个参数的时候，可以定义一些隐式参数，这些隐式参数可以被自动加到方法填充的参数里，而不必手填充。 def implicitParamFunc(name: String)(implicit tiger: Tiger, lion: Lion): Unit = { println(name + &quot; have a tiget and a lion, their names are: &quot; + tiger.name + &quot;, &quot; + lion.name) } object Zoo { implicit val tiger = Tiger(&quot;tiger1&quot;) implicit val lion = Lion(&quot;lion1&quot;) } import Zoo._ implicitParamFunc(&quot;format&quot;) 上面这个代码中implicitParamFunc中的第二个参数定义成了隐式参数。 然后在Zoo对象里定义了两个隐式变量，import进来之后，调用implicitParamFunc方法的时候这两个变量被自动填充到了参数里。 这里需要注意的是不仅仅方法中的参数需要被定义成隐式参数，对应的隐式参数的变量也需要被定义成隐式变量。 其他对象中的隐式转换可以只import自己需要的。 object MyUtils { implicit def a ... implicit def b ... } import MyUtils.a 隐式转换修饰符implicit可以修饰class，method，变量，object。 修饰方法和变量的隐式转换本文已经介绍过，就不继续说了。 修饰class的隐式转换，它的作用跟修饰method的隐式转换类似： implicit class RangeMarker(val start: Int) { def --&gt;(end: Int) = start to end } 1 --&gt; 10 // Range(1, 10) 上段代码可以改造成使用Value Class完成类的隐式转换： implicit class RangeMaker(start: Int) extends AnyVal { def --&gt;(end: Int) = start to end } 修饰object的隐式转换： trait Calculate[T] { def add(x: T, y: T): T } implicit object IntCal extends Calculate[Int] { def add(x: Int, y: Int): Int = x + y } implicit object ListCal extends Calculate[List[Int]] { def add(x: List[Int], y: List[Int]): List[Int] = x ::: y } def implicitObjMethod[T](x: T, y: T)(implicit cal: Calculate[T]): Unit = { println(x + &quot; + &quot; + y + &quot; = &quot; + cal.add(x, y)) } implicitObjMethod(1, 2) // 1 + 2 = 3 implicitObjMethod(List(1, 2), List(3, 4)) // List(1, 2) + List(3, 4) = List(1, 2, 3, 4)","raw":null,"content":null,"categories":[{"name":"scala","slug":"scala","permalink":"http://fangjian0423.github.io/categories/scala/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"http://fangjian0423.github.io/tags/jvm/"},{"name":"scala","slug":"scala","permalink":"http://fangjian0423.github.io/tags/scala/"}]},{"title":"elasticsearch查询模板","slug":"elasticsearch-search-template","date":"2015-11-07T08:04:25.000Z","updated":"2016-05-10T14:59:25.000Z","comments":true,"path":"2015/11/07/elasticsearch-search-template/","link":"","permalink":"http://fangjian0423.github.io/2015/11/07/elasticsearch-search-template/","excerpt":"","text":"最近在公司又用到了elasticsearch，也用到了查询模板，顺便写篇文章记录一下查询模板的使用。 以1个需求为例讲解es模板的使用： 页面上某个按钮在一段时间内的点击次数统计，并且可以以小时，天，月为单位进行汇总，并且需要去重。 创建索引，只定义3个字段，user_id, user_name和create_time: -POST /$ES/event_index { &quot;mappings&quot;: { &quot;event&quot;: { &quot;_ttl&quot;: { &quot;enabled&quot;: false }, &quot;_timestamp&quot;: { &quot;enabled&quot;: true, &quot;format&quot;: &quot;yyyy-MM-dd HH:mm:ss&quot; }, &quot;properties&quot;: { &quot;user_id&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;store&quot;: &quot;no&quot;, &quot;index&quot;: &quot;not_analyzed&quot; }, &quot;create_time&quot;: { &quot;type&quot;: &quot;date&quot;, &quot;store&quot;: &quot;no&quot;, &quot;index&quot;: &quot;not_analyzed&quot;, &quot;format&quot;: &quot;yyyy-MM-dd HH:mm:ss&quot; }, &quot;user_name&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;store&quot;: &quot;no&quot; } } } } } 定义对应的查询模板，模板名字stats，使用了Cardinality和DateHistogram这两个Aggregation，其中Date Histogram嵌套在Cardinality里。在定义模板的时候，{ { } } 的表示是个参数，需要调用模板的时候传递进来: -POST /$ES/_search/template/stats { &quot;template&quot;: { &quot;query&quot;: { &quot;bool&quot;: { &quot;must&quot;: [ { &quot;range&quot;: { &quot;create_time&quot;: { &quot;gte&quot;: &quot;{{earliest}}&quot;, &quot;lte&quot;: &quot;{{latest}}&quot; } } } ] } }, &quot;size&quot;: 0, &quot;aggs&quot;: { &quot;stats_data&quot;: { &quot;date_histogram&quot;: { &quot;field&quot;: &quot;create_time&quot;, &quot;interval&quot;: &quot;{{interval}}&quot; }, &quot;aggs&quot;: { &quot;time&quot;: { &quot;cardinality&quot;: { &quot;field&quot;: &quot;user_id&quot; } } } } } } } Cardinality Aggregation的作用就是类似sql中的distinct，去重。 Date Histogram Aggregation的作用是根据时间进行统计。内部有个interval属性表面统计的范畴。 下面加几条数据到event_index里： -POST $ES/event_index/event { &quot;user_id&quot;: &quot;1&quot;, &quot;user_name&quot;: &quot;format1&quot;, &quot;create_time&quot;: &quot;2015-11-07 12:00:00&quot; } -POST $ES/event_index/event { &quot;user_id&quot;: &quot;2&quot;, &quot;user_name&quot;: &quot;format2&quot;, &quot;create_time&quot;: &quot;2015-11-07 13:30:00&quot; } -POST $ES/event_index/event { &quot;user_id&quot;: &quot;3&quot;, &quot;user_name&quot;: &quot;format3&quot;, &quot;create_time&quot;: &quot;2015-11-07 13:30:00&quot; } -POST $ES/event_index/event { &quot;user_id&quot;: &quot;1&quot;, &quot;user_name&quot;: &quot;format1&quot;, &quot;create_time&quot;: &quot;2015-11-07 13:50:00&quot; } -POST $ES/event_index/event { &quot;user_id&quot;: &quot;1&quot;, &quot;user_name&quot;: &quot;format1&quot;, &quot;create_time&quot;: &quot;2015-11-07 13:55:00&quot; } 11-07 12-13点有1条数据，1个用户11-07 13-14点有4条数据，3个用户 使用模板查询： curl -XGET &quot;$ES/event_index/_search/template&quot; -d&apos;{ &quot;template&quot;: { &quot;id&quot;: &quot;stats&quot; }, &quot;params&quot;: { &quot;earliest&quot;: &quot;2015-11-07 00:00:00&quot;, &quot;latest&quot;: &quot;2015-11-07 23:59:59&quot;, &quot;interval&quot;: &quot;hour&quot; } }&apos; 结果： { &quot;took&quot;: 3, &quot;timed_out&quot;: false, &quot;_shards&quot;: { &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 }, &quot;hits&quot;: { &quot;total&quot;: 5, &quot;max_score&quot;: 0, &quot;hits&quot;: [] }, &quot;aggregations&quot;: { &quot;stats_data&quot;: { &quot;buckets&quot;: [ { &quot;key_as_string&quot;: &quot;2015-11-07 12:00:00&quot;, &quot;key&quot;: 1446897600000, &quot;doc_count&quot;: 1, &quot;time&quot;: { &quot;value&quot;: 1 } }, { &quot;key_as_string&quot;: &quot;2015-11-07 13:00:00&quot;, &quot;key&quot;: 1446901200000, &quot;doc_count&quot;: 4, &quot;time&quot;: { &quot;value&quot;: 3 } } ] } } } 12点-13点的只有1条数据，1个用户。13-14点的有4条数据，3个用户。 以天(day)统计： curl -XGET &quot;$ES/event_index/_search/template&quot; -d&apos;{ &quot;template&quot;: { &quot;id&quot;: &quot;stats&quot; }, &quot;params&quot;: { &quot;earliest&quot;: &quot;2015-11-07 00:00:00&quot;, &quot;latest&quot;: &quot;2015-11-07 23:59:59&quot;, &quot;interval&quot;: &quot;day&quot; } }&apos; 结果： { &quot;took&quot;: 4, &quot;timed_out&quot;: false, &quot;_shards&quot;: { &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 }, &quot;hits&quot;: { &quot;total&quot;: 5, &quot;max_score&quot;: 0, &quot;hits&quot;: [] }, &quot;aggregations&quot;: { &quot;stats_data&quot;: { &quot;buckets&quot;: [ { &quot;key_as_string&quot;: &quot;2015-11-07 00:00:00&quot;, &quot;key&quot;: 1446854400000, &quot;doc_count&quot;: 5, &quot;time&quot;: { &quot;value&quot;: 3 } } ] } } } 11-07这一天有5条数据，3个用户。 本文只是简单说明了es查询模板的使用，也简单使用了2个aggregation。更多内容可以去官网查看相关资料。","raw":null,"content":null,"categories":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://fangjian0423.github.io/categories/elasticsearch/"}],"tags":[{"name":"big data","slug":"big-data","permalink":"http://fangjian0423.github.io/tags/big-data/"},{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://fangjian0423.github.io/tags/elasticsearch/"}]},{"title":"html页面左右滑动菜单效果的实现","slug":"html-left-right-menu","date":"2015-10-29T13:15:13.000Z","updated":"2016-05-10T14:59:25.000Z","comments":true,"path":"2015/10/29/html-left-right-menu/","link":"","permalink":"http://fangjian0423.github.io/2015/10/29/html-left-right-menu/","excerpt":"","text":"正文最近要实现一个微信端页面左边弹出菜单的实现，效果如下： 绿色部分是左菜单内容，高度填充满整个页面。且有滚动条，并且滚动条内容随着滚动条的滚动不会影响正文的滚动，正文内容的滚动不会影响左菜单的滚动。 下面说下自己实现这种效果的思路。 1.首先由于左右两边的滚动不影响双方，这就需要将菜单和内容的position设置为绝对定位，设置都需要滚动条: overflow: auto; 。 2.菜单的内容会覆盖正文的内容：所以菜单的z-index比正文要大。 3.给左边的菜单加点width动画，需要显示的时候设置width，需要隐藏的时候width设置为0即可。 如果不想把菜单的内容覆盖在正文内容上面，而是正文内容向右偏移菜单的宽度： 这个效果与上一个效果一样，唯一的区别就是正文的z-index比菜单大，而且正文需要配置一个背景色。最后切换菜单的时候正文内容加点向右便宜的动画即可。 刚换了next主题…. 发现这个主题右边的sidebar也是这样的效果实现 →_→ 。 囧 。 基础小知识上面第二个例子中内容的z-index比菜单的z-index要大，而且正文需要配置一个背景色。为什么正文需要配置一个背景色呢？ 因为html中的元素没指定背景色的话，那说明这个元素的背景色是透明的 比如下面这个效果，上面的内容没设置背景色，所以是透明的，虽然它的z-index比上面的块要大，但是还是显示了。 给上面的内容设置黄色的背景色就可以隐藏下面的内容的。","raw":null,"content":null,"categories":[{"name":"css","slug":"css","permalink":"http://fangjian0423.github.io/categories/css/"}],"tags":[{"name":"css","slug":"css","permalink":"http://fangjian0423.github.io/tags/css/"},{"name":"html","slug":"html","permalink":"http://fangjian0423.github.io/tags/html/"}]},{"title":"最近前端的一点总结","slug":"front-end","date":"2015-10-24T12:15:13.000Z","updated":"2016-05-10T14:59:25.000Z","comments":true,"path":"2015/10/24/front-end/","link":"","permalink":"http://fangjian0423.github.io/2015/10/24/front-end/","excerpt":"","text":"这段时间被拉去当做前端了，做了快1个多月了，也好久没写博客了。 做了两个项目的前端，这周是第二个项目的启动，而且这周简直是灾难的一周，基本上都是23点后下班的，有两天还是凌晨2点。悲剧。 做前端经验不是很多，这个月还是学到了一些前端的自己没掌握的知识。做个总结吧。 1.box-sizing属性。 box-sizing是css3引入的。有两个值，分别content-box和border-box。 默认为content-box。这个属性的作用是这样的： 比如我们定义一个div，宽度和高度都是50px。padding 5px， border: 5px。 那么这个div实际的宽度和高度是： 50 + 2 * 5 + 2 * 5 = 70。 如果使用border-box的话，那么这个div的宽度还是50。 因为border-box会把padding和border都一起算到宽度和高度里面。所以使用border-box后div的高度和宽度为还是50。但是实际上真正显示内容的高度和宽度是 50 - 5 2 - 5 2 = 30。 直接来点实际的代码，使用content-box，也就是默认情况： &lt;div style=&quot;background-color: blue; width: 100px; height: 100px;&quot;&gt; &lt;div style=&quot;background-color: yellow; width: 50px; height: 50px; padding: 5px; border: 5px solid red; box-sizing: content-box;&quot;&gt;&lt;/div&gt; &lt;/div&gt; &lt;div style=&quot;background-color: blue; width: 100px; height: 100px;&quot;&gt; &lt;div style=&quot;background-color: yellow; width: 50px; height: 50px; padding: 5px; border: 5px solid red; box-sizing: border-box;&quot;&gt;&lt;/div&gt; &lt;/div&gt; Bootstrap3中也大量使用了border-box。 2.white-space属性。 使用white-space是为了让多张图片在同一行展示，不会换行。 &lt;div style=&quot;width: 200px;&quot;&gt; &lt;img src=&quot;http://7x2wh6.com1.z0.glb.clouddn.com/border-box1.png&quot; width=&quot;50px&quot;/&gt; &lt;img src=&quot;http://7x2wh6.com1.z0.glb.clouddn.com/border-box1.png&quot; width=&quot;50px&quot;/&gt; &lt;img src=&quot;http://7x2wh6.com1.z0.glb.clouddn.com/border-box1.png&quot; width=&quot;50px&quot;/&gt; &lt;img src=&quot;http://7x2wh6.com1.z0.glb.clouddn.com/border-box1.png&quot; width=&quot;50px&quot;/&gt; &lt;img src=&quot;http://7x2wh6.com1.z0.glb.clouddn.com/border-box1.png&quot; width=&quot;50px&quot;/&gt; &lt;/div&gt; 将外层的div改为： &lt;div style=&quot;width: 200px; white-space: nowrap; overflow: scroll;&quot;&gt; white-space设置为nowrap后，文本内容不会换行直到遇到br标签为止。 3.CSS3的动画。 一开始没有使用CSS3的动画，使用了JQuery的animate，结果发现页面动画有点卡，后来改成了CSS3的动画。 因为都是一些比较简单的动画，所以只能写下简单的动画属性了。 &lt;div style=&quot;width: 50px; height: 50px; background-color: yellow; transition: width .2s;&quot;&gt; &lt;/div&gt; div的点击事件： $(&quot;div&quot;).click(function() { var $div = $(this); if($div.width() == 50) { $div.width(&quot;100&quot;); } else { $div.width(&quot;50&quot;); } }); 动画还有延迟效果，这里就不举例了。 4.垂直居中 高度固定的垂直居中，设置line-height为容器高度，text-align为center即可： &lt;div style=&quot;width: 100px; height: 100px; background-color: yellow; line-height: 100px; text-align: center;&quot;&gt; 我居中了 &lt;/div&gt; 高度不固定的垂直居中： html, body { height: 100%; } body { display: -webkit-box; display: -webkit-flex; display: -moz-box; display: -moz-flex; display: -ms-flexbox; display: flex; /* 水平居中*/ -webkit-box-align: center; -moz-box-align: center; -ms-flex-pack:center;/* IE 10 */ -webkit-justify-content: center; -moz-justify-content: center; justify-content: center;/* IE 11+,Firefox 22+,Chrome 29+,Opera 12.1*/ /* 垂直居中 */ -webkit-box-pack: center; -moz-box-pack: center; -ms-flex-align:center;/* IE 10 */ -webkit-align-items: center; -moz-align-items: center; align-items: center; } ... &lt;body&gt; 垂直居中 &lt;/body&gt; ... 还有其他的一些比如绝对定位，固定定位，相对定位，overflow等问题就不一一举例了。 估计之后还是得做前端的一些工作，到时候用到了一些新内容的话还会继续更新前端相关的博客的。","raw":null,"content":null,"categories":[{"name":"css","slug":"css","permalink":"http://fangjian0423.github.io/categories/css/"}],"tags":[{"name":"css","slug":"css","permalink":"http://fangjian0423.github.io/tags/css/"},{"name":"html","slug":"html","permalink":"http://fangjian0423.github.io/tags/html/"}]},{"title":"Scala的Predef介绍","slug":"scala-predef","date":"2015-10-06T16:37:20.000Z","updated":"2016-05-10T14:59:25.000Z","comments":true,"path":"2015/10/07/scala-predef/","link":"","permalink":"http://fangjian0423.github.io/2015/10/07/scala-predef/","excerpt":"","text":"前言Scala每个程序都会自动import以下3个包，分别是 java.lang. , scala. 和 Predef._ 。跟Java程序都会自动import java.lang一样。 Predef是一个单例对象，所以我们import进来之后，可以直接使用Predef中定义的方法。 Predef中定义的方法和属性常用方法和类def classOf[T]: Class[T] = null // This is a stub method. The actual implementation is filled in by the compiler. type String = java.lang.String // scala中的String使用jdk中的String type Class[T] = java.lang.Class[T] // scala中的Class使用jdk中的Class type Function[-A, +B] = Function1[A, B] // Function1取别名Function type Map[A, +B] = immutable.Map[A, B] // Map类型默认使用immutable包下的Map type Set[A] = immutable.Set[A] // Set类型默认使用immutable包下的Set val Map = immutable.Map // Map对象默认使用immutable包下的Map对象(下面测试用到) val Set = immutable.Set // Set对象默认使用immutable包下的Set对象(下面测试用到) 一些测试： // 默认的Map和Set都是immutable包下的，这里的Set和Map都是在Predef中定义的一个变量。 Map(1 -&gt; 1, 2 -&gt; 2) // scala.collection.immutable.Map[Int,Int] = Map(1 -&gt; 1, 2 -&gt; 2) Set(1,2,3) // scala.collection.immutable.Set[Int] = Set(1, 2, 3) 打印方法def print(x: Any) = Console.print(x) def println() = Console.println() def println(x: Any) = Console.println(x) def printf(text: String, xs: Any*) = Console.print(text.format(xs: _*)) 因此，平时我们使用的println，print，printf这些打印方法都是在Predef中定义的， 一些调试和错误方法// 过期方法，抛出带有message消息的RuntimeException @deprecated(&quot;Use `sys.error(message)` instead&quot;, &quot;2.9.0&quot;) def error(message: String): Nothing = sys.error(message) // 断言。 参数是一个Boolean类型，失败抛出java.lang.AssertionError异常 @elidable(ASSERTION) def assert(assertion: Boolean) { if (!assertion) throw new java.lang.AssertionError(&quot;assertion failed&quot;) } // 跟上一个方法类似，多了一个message参数。抛出的异常就打印出这个message参数 @elidable(ASSERTION) @inline final def assert(assertion: Boolean, message: =&gt; Any) { if (!assertion) throw new java.lang.AssertionError(&quot;assertion failed: &quot;+ message) } // 跟assert类似，唯一的区别是assume支持静态经验(static checker) @elidable(ASSERTION) def assume(assumption: Boolean) { if (!assumption) throw new java.lang.AssertionError(&quot;assumption failed&quot;) } // 跟assume一样，多了个message参数 @elidable(ASSERTION) @inline final def assume(assumption: Boolean, message: =&gt; Any) { if (!assumption) throw new java.lang.AssertionError(&quot;assumption failed: &quot;+ message) } // 跟assert类似，只不过抛出的是IllegalArgumentException异常 def require(requirement: Boolean) { if (!requirement) throw new IllegalArgumentException(&quot;requirement failed&quot;) } // 多个参数，作用如上一样 @inline final def require(requirement: Boolean, message: =&gt; Any) { if (!requirement) throw new IllegalArgumentException(&quot;requirement failed: &quot;+ message) } 调试和错误方法测试： assert(1 == 2) // java.lang.AssertionError: assertion failed assert(1 == 2, &quot;test&quot;) // java.lang.AssertionError: assertion failed: test assume(1 == 2) // java.lang.AssertionError: assumption failed assume(1 == 2, &quot;test&quot;) // java.lang.AssertionError: assumption failed: test require(1 == 2) // java.lang.IllegalArgumentException: requirement failed require(1 == 2, &quot;test&quot;) // java.lang.IllegalArgumentException: requirement failed: test 一个特殊的属性Predef中有个 ??? 属性，抛出一个NotImplementedError： def ??? : Nothing = throw new NotImplementedError 比如定义一些方法的时候，这个方法还没有实现，这个时候可以使用 ???， 而非TODO： def todoMethod(x: Int): Int = ??? todoMethod(2) // scala.NotImplementedError: an implementation is missing Predef还有大量的隐式转换和隐式转换类再讲Predef中的隐式转换和隐式转换类之前，先介绍一下这2个概念。 隐式转换隐式转换的意思是一个方法中有一个类型的参数，并返回另外一个类型的返回值。比如一个Double类型的方法返回一个Int类型的返回值。 def double2Int(d: Double) = d.toInt double2Int(2.4) // 2 val a: Int = 2.3 // 报错 重新定义double2Int，使其支持隐式转换： implicit def double2Int(d: Double) = d.toInt val a: Int = 2.3 // a: Int = 2 隐式转换类当需要给Int类型添加一个 –&gt; 方法的时候，需要使用到隐式转换类。因为隐式转换只支持1个参数，所以只能通过隐式转换类完成。 implicit class RangeMaker(left: Int) { def --&gt;(right: Int) = left to right } val range = 1 --&gt; 10 // Range(1, 2, 3, 4, 5, 6, 7, 8, 9, 10) 隐式转换类有个弊端，那就是每次都会创建一个类的实例，有时候这完全没有必要。 scala提供了一个将隐式转换类转换成value class的方法，只需要继承AnyVal即可，还需要注意参数需要加上val标识符。 implicit class RangeMaker(val left: Int) extends AnyVal { def --&gt;(right: Int) = left to right } Predef中的隐式转换和隐式转换类-&gt; 方法： implicit final class ArrowAssoc[A](private val self: A) extends AnyVal { @inline def -&gt; [B](y: B): Tuple2[A, B] = Tuple2(self, y) def →[B](y: B): Tuple2[A, B] = -&gt;(y) } 生成一个二元元组，Tuple2。 1 → 2 // (Int, Int) = (1, 2) 1 -&gt; 2 // (Int, Int) = (1, 2) ensuring 方法： implicit final class Ensuring[A](private val self: A) extends AnyVal { def ensuring(cond: Boolean): A = { assert(cond); self } def ensuring(cond: Boolean, msg: =&gt; Any): A = { assert(cond, msg); self } def ensuring(cond: A =&gt; Boolean): A = { assert(cond(self)); self } def ensuring(cond: A =&gt; Boolean, msg: =&gt; Any): A = { assert(cond(self), msg); self } } ensuring内部调用assert方法。 def doublePositive(n: Int): Int = { n * 2 } ensuring(n =&gt; n &gt;= 0 &amp;&amp; n % 2 == 0) doublelPositive(1) // 2 doublelPositive(-1) // java.lang.AssertionError: assertion failed formatted 方法： implicit final class StringFormat[A](private val self: A) extends AnyVal { @inline def formatted(fmtstr: String): String = fmtstr format self } 格式化字符串，使用java.lang.String.format方法。 &quot;Format&quot; formatted &quot;%s, let&apos;s go&quot; // Format, let&apos;s go + 方法： implicit final class any2stringadd[A](private val self: A) extends AnyVal { def +(other: String): String = String.valueOf(self) + other } case class使用+方法： case class Student(name: String, age: Int) Student(&quot;format&quot;, 11) + &quot; 22&quot; // Student(format,11) 22 StringOps类： @inline implicit def augmentString(x: String): StringOps = new StringOps(x) @inline implicit def unaugmentString(x: StringOps): String = x.repr StringOps提供了丰富的原生String没提供的方法： &quot;format&quot;.length // 6 原生String是没有提供length方法的 &quot;format&quot;.foreach(println(_)) &quot;format&quot;.stripPrefix(&quot;for&quot;) // mat &quot;format&quot;.slice(1, 3) // or &quot;format&quot; * 5 // formatformatformatformatformat &quot;true&quot;.toBoolean // true ArrayOps类： implicit def genericArrayOps[T](xs: Array[T]): ArrayOps[T] = (xs match { case x: Array[AnyRef] =&gt; refArrayOps[AnyRef](x) case x: Array[Boolean] =&gt; booleanArrayOps(x) case x: Array[Byte] =&gt; byteArrayOps(x) case x: Array[Char] =&gt; charArrayOps(x) case x: Array[Double] =&gt; doubleArrayOps(x) case x: Array[Float] =&gt; floatArrayOps(x) case x: Array[Int] =&gt; intArrayOps(x) case x: Array[Long] =&gt; longArrayOps(x) case x: Array[Short] =&gt; shortArrayOps(x) case x: Array[Unit] =&gt; unitArrayOps(x) case null =&gt; null }).asInstanceOf[ArrayOps[T]] implicit def booleanArrayOps(xs: Array[Boolean]): ArrayOps[Boolean] = new ArrayOps.ofBoolean(xs) implicit def byteArrayOps(xs: Array[Byte]): ArrayOps[Byte] = new ArrayOps.ofByte(xs) implicit def charArrayOps(xs: Array[Char]): ArrayOps[Char] = new ArrayOps.ofChar(xs) implicit def doubleArrayOps(xs: Array[Double]): ArrayOps[Double] = new ArrayOps.ofDouble(xs) implicit def floatArrayOps(xs: Array[Float]): ArrayOps[Float] = new ArrayOps.ofFloat(xs) implicit def intArrayOps(xs: Array[Int]): ArrayOps[Int] = new ArrayOps.ofInt(xs) implicit def longArrayOps(xs: Array[Long]): ArrayOps[Long] = new ArrayOps.ofLong(xs) implicit def refArrayOps[T &lt;: AnyRef](xs: Array[T]): ArrayOps[T] = new ArrayOps.ofRef[T](xs) implicit def shortArrayOps(xs: Array[Short]): ArrayOps[Short] = new ArrayOps.ofShort(xs) implicit def unitArrayOps(xs: Array[Unit]): ArrayOps[Unit] = new ArrayOps.ofUnit(xs) ArrayOps提供了丰富的原生数组没提供的方法： Array(1, 2, 3) :+ 4 // Array(1, 2, 3, 4) Scala程序员可以较少关心装箱和拆箱操作，这也是由于Predef对象里定义了Scala值类型与java基本类型直接的隐式转换： implicit def byte2Byte(x: Byte) = java.lang.Byte.valueOf(x) implicit def short2Short(x: Short) = java.lang.Short.valueOf(x) implicit def char2Character(x: Char) = java.lang.Character.valueOf(x) implicit def int2Integer(x: Int) = java.lang.Integer.valueOf(x) implicit def long2Long(x: Long) = java.lang.Long.valueOf(x) implicit def float2Float(x: Float) = java.lang.Float.valueOf(x) implicit def double2Double(x: Double) = java.lang.Double.valueOf(x) implicit def boolean2Boolean(x: Boolean) = java.lang.Boolean.valueOf(x) implicit def Byte2byte(x: java.lang.Byte): Byte = x.byteValue implicit def Short2short(x: java.lang.Short): Short = x.shortValue implicit def Character2char(x: java.lang.Character): Char = x.charValue implicit def Integer2int(x: java.lang.Integer): Int = x.intValue implicit def Long2long(x: java.lang.Long): Long = x.longValue implicit def Float2float(x: java.lang.Float): Float = x.floatValue implicit def Double2double(x: java.lang.Double): Double = x.doubleValue implicit def Boolean2boolean(x: java.lang.Boolean): Boolean = x.booleanValue","raw":null,"content":null,"categories":[{"name":"scala","slug":"scala","permalink":"http://fangjian0423.github.io/categories/scala/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"http://fangjian0423.github.io/tags/jvm/"},{"name":"scala","slug":"scala","permalink":"http://fangjian0423.github.io/tags/scala/"}]},{"title":"Spring Boot 部分特性记录","slug":"springboot-intro","date":"2015-09-20T16:21:49.000Z","updated":"2016-05-10T14:59:25.000Z","comments":true,"path":"2015/09/21/springboot-intro/","link":"","permalink":"http://fangjian0423.github.io/2015/09/21/springboot-intro/","excerpt":"","text":"SpringBoot是Java的一个micro-service框架。它设计的目的是简化Spring应用的初始搭建以及开发过程。使用SpringBoot可以避免大量的xml配置文件，它内部使用很多约定的方式。 以一个最简单的MVC例子来说，使用SpringBoot进行开发的话定义好对应的Controller，Repository和Entity之后，加上各自的Annotation即可。 Repository框架可以选择Spring Data或者Hibernate，可通过自由配置。 视图框架也可通过配置选择freemarker或者velocity等视图框架。 下面，介绍一下SpringBoot的一些功能。 SpringBoot框架的启动SpringBoot使用SpringApplication这个类进行项目的启动。一般都会这么写： @SpringBootApplication public class Application { public static void main(String[] args) { SpringApplication.run(Application.class, args); } } 使用SpringBootApplication注解相当于使用了3个注解，分别是@ComponentScan，@Configuration，@EnableAutoConfiguration。 这里需要注意的是Application这个类所有的package位置。 比如你的项目的包路径是 me.format.project1，对应的controller和repository包是 me.format.project1.controller和me.format.project1.repository。 那么这个Application需要在的包路径为 me.format.project1。 因为SpringBootApplication注解内部是使用ComponentScan注解，这个注解会扫描Application包所在的路径下的各个bean。 Profile的使用可以在springboot项目中加入配置文件application.yml。 yaml中可以定义多个profile，也可以指定激活的profile： spring: profiles.active: dev --- spring: profiles: dev myconfig: config1: dev-enviroment --- spring: profiles: test myconfig: config1: test-enviroment --- spring: profiles: prod myconfig: config1: prod-envioment 也可以在运行的执行指定profile： java -Dspring.profiles.active=&quot;prod&quot; -jar yourjar.jar 还可以使用Profile注解，MyConfig只会在prod这个profile下才会生效，其他profile不会生效： @Profile(&quot;prod&quot;) @Component public class MyConfig { .... } 自定义的一些Conveter，Interceptor如果想配置springmvc的HandlerInterceptorAdapter或者HttpMessageConverter。 只需要定义自己的interceptor或者converter，然后加上Component注解。这样SpringBoot会自动处理这些类，不用自己在配置文件里指定对应的内容。 这个也是相当方便的。 @Component public class AuthInterceptor extends HandlerInterceptorAdapter { ... } @Component public class MyConverter implements HttpMessageConverter&lt;MyObj&gt; { ... } 模板的使用比如使用freemarker的时候，加入以下依赖： &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-freemarker&lt;/artifactId&gt; &lt;/dependency&gt; 然后在resources目录下建立一个templates目录即可，视图将会从这个templates位置开始找。 其他关于其他的特性可以参考官方文档： http://docs.spring.io/spring-boot/docs/current/reference/html/ springboot还提供了一系列sample供参考： https://github.com/spring-projects/spring-boot/tree/master/spring-boot-samples","raw":null,"content":null,"categories":[{"name":"springboot","slug":"springboot","permalink":"http://fangjian0423.github.io/categories/springboot/"}],"tags":[{"name":"springboot","slug":"springboot","permalink":"http://fangjian0423.github.io/tags/springboot/"},{"name":"microservice","slug":"microservice","permalink":"http://fangjian0423.github.io/tags/microservice/"}]},{"title":"Scala持久层框架Slick介绍","slug":"slick-intro","date":"2015-08-18T15:22:33.000Z","updated":"2016-05-10T14:59:25.000Z","comments":true,"path":"2015/08/18/slick-intro/","link":"","permalink":"http://fangjian0423.github.io/2015/08/18/slick-intro/","excerpt":"","text":"FRM介绍最近看到了一个FRM的框架Slick。 FRM的意思是Functional Relational Mapping， 一种基于函数式的ORM。 举一个最简单的例子： val queryResult = db.query(queryStr) queryResult.onSuccess { result =&gt; result.doSomething ... } 数据库db查询一条sql语句。查询成功的时候使用闭包完成处理。 看到这段代码的第一反应就是js的ajax处理，代码几乎是一样的，也发现之前在学校里写nodejs的时候查询db也是这样的语法。 var jqxhr = $.ajax( { url: &apos;url&apos;, method: &apos;GET&apos;, data: [user: &apos;format&apos;] }); jqxhr.success(function() { ... }); FRM相比ORM最明显的优势就是FRM基于多线程的Future的数据查询，而ORM是单线程的线性执行。 FRM构造sql查询也是相当简单的： // 构造查询 val newQuery = students.filter(_.age &gt; 24).sortBy(_.name) // 执行查询 db.run(newQuery) FRM其他的优势可以参考官方文档。 Slick实例下面以一个Students和Classrooms的实例来说明一下Slick的使用。 首先是创建对应的domain，学生与教室的关系是1对多。 Students domain(使用Option类型说明该列是可为空的)： class Student(tag: Tag) extends Table[(Int, String, Int, Int, Option[Date])](tag, &quot;Students&quot;) { def id: Rep[Int] = column[Int](&quot;id&quot;, O.PrimaryKey, O.AutoInc) def name: Rep[String] = column[String](&quot;name&quot;) def age: Rep[Int] = column[Int](&quot;age&quot;) def birthDate: Rep[Option[Date]] = column[Option[Date]](&quot;birth_date&quot;) def classroomId = column[Int](&quot;classroom_id&quot;) def * : ProvenShape[(Int, String, Int, Int, Option[Date])] = (id, name, age, classroomId, birthDate) def classroom: ForeignKeyQuery[Classroom, (Int, String)] = foreignKey(&quot;FK_CLASSROOM&quot;, classroomId, TableQuery[Classroom])(_.id) } Classrooms domain： class Classroom(tag: Tag) extends Table[(Int, String)](tag, &quot;Classrooms&quot;) { def id = column[Int](&quot;id&quot;, O.PrimaryKey, O.AutoInc) def name = column[String](&quot;name&quot;) def * = (id, name) } 各个db操作，schema创建，sql插入，sql查询等操作如下，加了几句备注，具体的代码就不分析了： object SampleSlickDemo extends App { val db = Database.forConfig(&quot;h2mem1&quot;) try { val classrooms = TableQuery[Classroom] val students = TableQuery[Student] val setupAction: DBIO[Unit] = DBIO.seq( // create student and classroom table in database (classrooms.schema ++ students.schema).create, // insert some rows in classroom classrooms += (1, &quot;classroom1&quot;), classrooms += (2, &quot;classroom2&quot;), classrooms += (3, &quot;classroom2&quot;) ) val setupFuture = db.run(setupAction) val f = setupFuture.flatMap { _ =&gt; val insertAction: DBIO[Option[Int]] = students ++= Seq ( (1, &quot;format1&quot;, 11, 1, new Date(System.currentTimeMillis())), (2, &quot;format2&quot;, 22, 2, new Date((System.currentTimeMillis()))), (3, &quot;format3&quot;, 33, 3, new Date((System.currentTimeMillis()))) ) val insertAndPrintAction = insertAction.map { studentResult =&gt; studentResult.foreach { numRows =&gt; println(s&quot;inserted $numRows students&quot;) } } db.run(insertAndPrintAction) }.flatMap { _ =&gt; // print All Classrooms db.run(classrooms.result).map { classroom =&gt; classroom.foreach(println); } // print All Students db.run(students.result).map { studnet =&gt; studnet.foreach(println); } // condition search val studentQuery = students.filter(_.age &gt; 20).sortBy(_.name) db.run(studentQuery.result).map { student =&gt; student.foreach(println) } } Await.result(f, Duration.Inf) } finally db.close() } 数据库配置在配置文件application.conf里配置数据库配置信息： h2mem1 = { url = &quot;jdbc:h2:mem:test1&quot; driver = org.h2.Driver connectionPool = disabled keepAliveConnection = true } 然后就可使用Database初始化数据库，参数就是配置文件里对应的数据库name： val db = Database.forConfig(&quot;h2mem1&quot;) DBIOAction介绍DBIOAction就是数据库的一个操作，比如Insert，Update，Delete，Query等操作。 可以使用上面分析的数据库配置变量db进行操作。 db有个run方法使用DBIOAction作为参数，返回Future类型的返回值。 DBIO是一个单例对象，它的seq方法可以传入多个DBIOAction，然后返回一个新的DBIOAction。 += 方法返回的也是DBIOAction。 val setupAction: DBIO[Unit] = DBIO.seq( (classrooms.schema ++ students.schema).create, classrooms += (1, &quot;classroom1&quot;), classrooms += (2, &quot;classroom2&quot;), classrooms += (3, &quot;classroom2&quot;) ) ++=方法跟+=方法一样会返回DBIOAction，只不过它的参数是个Iterable： val insertAction: DBIO[Option[Int]] = students ++= Seq ( (1, &quot;format1&quot;, 11, 1, new Date(System.currentTimeMillis())), (2, &quot;format2&quot;, 22, 2, new Date((System.currentTimeMillis()))), (3, &quot;format3&quot;, 33, 3, new Date((System.currentTimeMillis()))) ) DBIOAction提供许多好用的方法： map方法：参数是个函数，这个函数可以返回任意类型的值，返回是个DBIOAction。 所以可以使用map关联起来多个DBIOAction。 flatMap方法：参数是个函数，这个函数的返回值必须是个DBIOAction，返回值是个DBIOAction。作用跟map类似，只不过函数参数的返回值不一样。 filter方法：参数是个函数，这个函数的返回值必须是个Boolean，返回值是个DBIOAction。过滤作用。 andThen方法：参数是个DBIOAction，返回值是个DBIOAction。在Action完成后执行另外一个Action。 增删改查操作查询Slick的查询可以直接通过TableQuery操作，使用TableQuery提供的filter可以实现过滤操作，使用drop和take完成分页操作，使用sortBy完成排序操作。 students.filter(_.classroomId === 1) students.drop(1).take(2) students.sortBy(_.age.desc) 可以使用map方法找出需要的列。 多列： students.map { student =&gt; (student.name, student.age) } 一列： students.map(_.name) Join方法： cross join操作： val crossJoin = for { (s, c) &lt;- students join classrooms } yield (s.name, c.name) inner Join操作： val innerJoin = for { (s, c) &lt;- students join classrooms on (_.classroomId === _.id) } yield (s.name, c.name) 另外一个inner join： val innerJoin = for { s &lt;- students c &lt;- classrooms if c.id === s.classroomId } yield (s.name, c.name) left join操作： val leftJoin = for { (s, c) &lt;- students joinLeft classrooms on (_.classroomId === _.id) } yield (s.name, c.map(_.name)) right join操作： val rightJoin = for { (s, c) &lt;- students joinRight classrooms on (_.classroomId === _.id) } yield (s.map(_.name), c.name) 新增所有列都有值： val insertAction = DBIO.seq( students += (4, &quot;format4&quot;, 44, 3, new Date(System.currentTimeMillis())), students += (5, &quot;format5&quot;, 55, 3, new Date(System.currentTimeMillis())), students ++= Seq ( (6, &quot;format6&quot;, 66, 3, new Date(System.currentTimeMillis())), (7, &quot;format7&quot;, 77, 3, new Date(System.currentTimeMillis())) ) ) 部分列有值： students.map(s =&gt; (s.name, s.age, s.classroomId)) += (&quot;format8&quot;, 88, 3) 删除删除classroomId为3的所有数据： val q = students.filter(_.classroomId === 3) val affectedRowsCountFuture = db.run(q.delete) affectedRowsCountFuture.map { rows =&gt; println(rows) } 修改修改单列： val q = students.filter(_.id === 2).map(_.name) val updateSql = q.update(&quot;format2222&quot;) db.run(updateSql) 修改多列： val q = students.filter(_.id === 2).map(s =&gt; (s.name, s.age)) val updateSql = q.update((&quot;format2222&quot;, 222)) db.run(updateSql) CaseClass的使用之前的例子都是使用Tuple构造domain。 还有一种更方便的方式，那就是使用CaseClass。 case class People(id: Long, name: String, age: Int) 例子： private class PeopleTable(tag: Tag) extends Table[People](tag, &quot;people&quot;) { val id = column[Long](&quot;id&quot;, O.PrimaryKey, O.AutoInc) val name = column[String](&quot;name&quot;) val age = column[Int](&quot;age&quot;) def * = (id, name, age) &lt;&gt; ((People.apply _).tupled, People.unapply) } val db = Database.forConfig(&quot;h2mem1&quot;) try { val people = TableQuery[PeopleTable] val setupAction = DBIO.seq( people.schema.create, people += People(1, &quot;format1&quot;, 11) ) val setupFuture = db.run(setupAction); val f = setupFuture.flatMap { _ =&gt; db.run(people.result).map { p =&gt; p.foreach(println) } } Await.result(f, Duration.Inf) } finally db.close()","raw":null,"content":null,"categories":[{"name":"scala","slug":"scala","permalink":"http://fangjian0423.github.io/categories/scala/"}],"tags":[{"name":"scala","slug":"scala","permalink":"http://fangjian0423.github.io/tags/scala/"},{"name":"frm","slug":"frm","permalink":"http://fangjian0423.github.io/tags/frm/"},{"name":"orm","slug":"orm","permalink":"http://fangjian0423.github.io/tags/orm/"}]},{"title":"Scala并发的Future","slug":"scala-future","date":"2015-08-14T12:22:33.000Z","updated":"2016-05-10T14:59:25.000Z","comments":true,"path":"2015/08/14/scala-future/","link":"","permalink":"http://fangjian0423.github.io/2015/08/14/scala-future/","excerpt":"","text":"Future这个概念其实java里也已经有了， 表示一个未来的意思。 某线程执行一项操作，这个操作有延迟的话，Future会提供一系列方法来处理这个线程过程，可取消，可操作完成后执行其他操作等等。 使用Future是非阻塞的，在Future中可以使用回调函数可以避免阻塞操作。 Scala在Future中提供了flatMap，foreach，filter等方法。 Future概念Future的状态： 1.未完成：线程操作还未结束2.已完成：操作操作完成，并且有返回值或者有异常。 当一个Future完成的时候，它就变成了一个不可变对象，永远不会被重写 构造Future最简单的方法是使用Future这个object提供的apply方法： Future { ... } 来看一个最简单的Future操作， 计算和： import scala.concurrent.ExecutionContext.Implicits.global import scala.concurrent.duration.Duration import scala.concurrent.{Await, Future} val sumFuture = Future[Int] { var sum = 0 for(i &lt;- Range(1,100000)) sum = sum + i sum } sumFuture.onSuccess { case sum =&gt; println(sum) } Await.result(sumFuture, Duration.Inf) Await的作用是阻断Future等待Future的执行结果。 import scala.concurrent.ExecutionContext.Implicits.global 这个global表示一个ExecutionContext，类似线程池，所有线程的提交都得交给ExecutionContext。如果没有import这个global对象，那么执行的时候会报错。 文本文件中找关键字的例子，读io文件的时候如果文件很大，肯定会阻塞。使用Future完成，可以更有效率，等关键字索引找到的时候再去拿数据。这段时间完成可以去做其他事情： val keywordIndex = Future[Int] { val source = scala.io.Source.fromFile(&quot;intro.txx&quot;) source.toSeq.indexOfSlice(&quot;format&quot;) } 回调Future提供了3种Callback，分别是 onComplete，onFailure，onSuccess。 onComplete回调表示Future执行完毕了。需要1个Try[T] =&gt; U类型的参数，如果执行成功且没发生一次，那么匹配Success类型，否则匹配Failure类型。 onComplete例子： val calFuture = Future[Int] { val a = 1 / 1 a } calFuture.onComplete { case Success(result) =&gt; println(result) case Failure(e) =&gt; println(&quot;error: &quot; + e.getMessage) } onFailure回调表示Future已经执行完成，但是出错了。 val errorFuture = Future[Unit] { 1 / 0 } errorFuture.onFailure { case e =&gt; println(e.getMessage) } onSuccess回调表示Future已经执行完成，而且执行成功了。 val successFuture = Future[Int] { 1000 } successFuture.onSuccess { case num =&gt; println(num) } Future的组合使用map方法可以组合Future： val firstValFuture = Future[Int] { 1 } val secondFuture = firstValFuture.map { num =&gt; println(&quot;firstFuture: &quot; + num) num + &quot;111&quot; } secondFuture.onSuccess { case result =&gt; println(result) } flapMap方法也可以组合Future，map方法和flatMap方法唯一的区别是flatMap内部需要返回Future，而map不是。 val firstValFuture = Future[Int] { 1 } val secondFuture = firstValFuture.flatMap { num =&gt; println(&quot;firstFuture: &quot; + num) Future { num + &quot;111&quot; } } secondFuture.onSuccess { case result =&gt; println(result) } 其他资料参考Scala的官方文档即可。","raw":null,"content":null,"categories":[{"name":"scala","slug":"scala","permalink":"http://fangjian0423.github.io/categories/scala/"}],"tags":[{"name":"scala","slug":"scala","permalink":"http://fangjian0423.github.io/tags/scala/"},{"name":"concurrent","slug":"concurrent","permalink":"http://fangjian0423.github.io/tags/concurrent/"}]},{"title":"HBase介绍","slug":"hbase-intro","date":"2015-08-06T16:22:33.000Z","updated":"2016-05-10T14:59:25.000Z","comments":true,"path":"2015/08/07/hbase-intro/","link":"","permalink":"http://fangjian0423.github.io/2015/08/07/hbase-intro/","excerpt":"","text":"前言HBase在公司已经用过一段时间，在Flume中添加一个HBase sink将一些数据存储到HBase里。 当时HBase也没学，看了看几个例子，了解了它是基于列的表设计之后，就马上上手了，而且也把东西做出来了。 现在记录一下HBase的一些学习笔记。 HBase简介HBase是什么？ HBase是运行在hadoop上的数据库，是一个分布式的，扩展性高的，存储大数据的数据库。 HBase也是开源的，非关系型数据库。基于Google的Bigtable设计。 什么时候需要使用HBase？ 需要实时地读写大数据。HBase的目的就是管理亿级的数据。 HBase基本概念HBase是基于列设计的，那什么是基于列呢？ 首先看下关系型数据库的表结构，第一行是table的所有列，第二行开始就是各个列对应的值： id name age birth_date 1 format1 11 1980-01-01 2 format2 22 1985-01-01 3 format3 33 1990-01-01 HBase的表结构是这样的： Row Key Time Stamp ColumnFamily contents ColumnFamily names ‘me.format.hbase’ t1 contents:format = “format1” ‘me.format.hbase’ t2 contents:title = “title1” ‘me.format.hbase’ t3 names:gogogo = “data1” 从上面这个HBase表的例子来说明HBase的存储结构。 Row Key：行的键值，其实就相当于这一行的标识符。上面的数据其实只有1行，因为他们的标识符是一样的。 TimeStamp：时间戳，创建数据的时间戳，hbase默认会自动生成 ColumnFamily：列的前缀，一列可以存储多条数据，具体存储什么类型的数据还需要另外一个标示符qualify，上面那个例子中，contents和names就是两个Column Family ColumnFamily qualify：列前缀后的标识符，一个ColumnFamily可以有多个qualify。上面那个例子中format和title就是contents这个ColumnFamily的qualify。gogogo是names这个ColumnFamily的qualify HBase的启动HBase下载完之后解压，解压后使用以下命令启动hbase： $ ./bin/start-hbase.sh 启动之前注意，机器要装好jdk，并且启动hadoop。因为hbase底层数据是存储在hdfs上的。 HBase的基本操作表的创建创建一个表名位tableName，ColumnFamily有contents和names的表，qualify不需要声明，每次添加数据随意指定qualify即可： create &apos;tableName&apos;,[&apos;contents&apos;, &apos;names&apos;] 表的删除删除表的所有数据： truncate table tableName 删除表，删除之前需要先disable表，然后才可删除： disable &apos;tableName&apos; drop &apos;tableName&apos; 数据查询查询tableName表数据： scan tableName 返回： ROW COLUMN+CELL me.format.hbase column=contents:format, timestamp=1438875060466, value=format1 数据删除比如，表tableName里有如下数据： ROW COLUMN+CELL me.format.hbase column=contents:format, timestamp=1438875566613, value=format1 me.format.hbase column=contents:title, timestamp=1438875577687, value=title1 me.format.hbase column=names:gogogo, timestamp=1438875597592, value=data1 进行删除操作，删除ColumnFamily qulify为contents:format的数据： delete &apos;tableName&apos;, &apos;me.format.hbase&apos;, &apos;contents:format&apos; 数据修改HBase没有直接的update操作，只有put操作，put操作如果对应的地方有值，会覆盖： put &apos;tableName&apos;, &apos;me.format.hbase&apos;, &apos;contents:format&apos;, &apos;format111&apos; 添加数据在tableName表里插入一个Row Key为me.format.hbase, ColumnFamily为contents，qualify为format，值的format1的数据： put &apos;tableName&apos;, &apos;me.format.hbase&apos;, &apos;contents:format&apos;, &apos;format1&apos; 计数器HBase提供了一种计数器的概念，每次可以对某个值进行incr操作： incr &apos;tableName&apos;, &apos;me.format.hbase&apos;, &apos;contents:num&apos;, 1 查询数据： me.format.hbase column=contents:num, timestamp=1438875837362, value=\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01 可以使用get_counter命令获得计数器的值： get_counter &apos;tableName&apos;,&apos;me.format.hbase&apos;, &apos;contents:num&apos;, 0 返回： COUNTER VALUE = 1 再次修改： incr &apos;tableName&apos;, &apos;me.format.hbase&apos;, &apos;contents:num&apos;, 100 get_counter &apos;tableName&apos;,&apos;me.format.hbase&apos;, &apos;contents:num&apos;, 0 COUNTER VALUE = 101 incr &apos;tableName&apos;, &apos;me.format.hbase&apos;, &apos;contents:num&apos;, -102 get_counter &apos;tableName&apos;,&apos;me.format.hbase&apos;, &apos;contents:num&apos;, 0 COUNTER VALUE = -1 带条件的数据查询scan查询可以带几个参数。 COLUMNS： ColumnFamily和qualify的值LIMIT：展示的个数FILTER：过滤条件 比如有以下数据： ROW COLUMN+CELL me.format.hbase column=contents:format, timestamp=1438875707700, value=format1 me.format.hbase column=contents:num, timestamp=1438876106259, value=\\xFF\\xFF\\xFF\\xFF\\xFF\\xFF\\xFF\\xFF me.format.hbase column=contents:title, timestamp=1438875577687, value=title1 me.format.hbase column=names:gogogo, timestamp=1438875597592, value=data1 me.format.hbase1 column=contents:format, timestamp=1438877417358, value=format1 me.format.hbase2 column=contents:format, timestamp=1438877422756, value=format1 me.format.hbase3 column=contents:format, timestamp=1438877427312, value=format1 查询ColumnFamily，qualify为contents:format的数据： scan &apos;tableName&apos;, { COLUMNS =&gt; &quot;contents:format&quot;, LIMIT =&gt; 10 } 结果： ROW COLUMN+CELL me.format.hbase column=contents:format, timestamp=1438875707700, value=format1 me.format.hbase1 column=contents:format, timestamp=1438877417358, value=format1 me.format.hbase2 column=contents:format, timestamp=1438877422756, value=format1 me.format.hbase3 column=contents:format, timestamp=1438877427312, value=format1 查询ColumnFamily未contents的数据： scan &apos;tableName&apos;, { COLUMNS =&gt; &quot;contents&quot;, LIMIT =&gt; 10 } 结果： ROW COLUMN+CELL me.format.hbase column=contents:format, timestamp=1438875707700, value=format1 me.format.hbase column=contents:num, timestamp=1438876106259, value=\\xFF\\xFF\\xFF\\xFF\\xFF\\xFF\\xFF\\xFF me.format.hbase column=contents:title, timestamp=1438875577687, value=title1 me.format.hbase1 column=contents:format, timestamp=1438877417358, value=format1 me.format.hbase2 column=contents:format, timestamp=1438877422756, value=format1 me.format.hbase3 column=contents:format, timestamp=1438877427312, value=format1 查询ColumnFamily未contents的数据，并只展示2行数据： scan &apos;tableName&apos;, { COLUMNS =&gt; &quot;contents&quot;, LIMIT =&gt; 2 } 结果： ROW COLUMN+CELL me.format.hbase column=contents:format, timestamp=1438875707700, value=format1 me.format.hbase column=contents:num, timestamp=1438876106259, value=\\xFF\\xFF\\xFF\\xFF\\xFF\\xFF\\xFF\\xFF me.format.hbase column=contents:title, timestamp=1438875577687, value=title1 me.format.hbase1 column=contents:format, timestamp=1438877417358, value=format1 查询ColumnFamily未contents的数据，并只展示2行数据： scan &apos;tableName&apos;, { COLUMNS =&gt; &quot;contents&quot;, FILTER =&gt; &quot;ValueFilter( =, &apos;binaryprefix:title&apos; )&quot; } 结果： ROW COLUMN+CELL me.format.hbase column=contents:title, timestamp=1438875577687, value=title1 scan命令具体其他的参数就不一一列举了，可查询文档解决。","raw":null,"content":null,"categories":[{"name":"hbase","slug":"hbase","permalink":"http://fangjian0423.github.io/categories/hbase/"}],"tags":[{"name":"big data","slug":"big-data","permalink":"http://fangjian0423.github.io/tags/big-data/"},{"name":"hbase","slug":"hbase","permalink":"http://fangjian0423.github.io/tags/hbase/"}]},{"title":"Hive介绍","slug":"hive-intro","date":"2015-07-31T05:32:33.000Z","updated":"2016-05-10T14:59:25.000Z","comments":true,"path":"2015/07/31/hive-intro/","link":"","permalink":"http://fangjian0423.github.io/2015/07/31/hive-intro/","excerpt":"","text":"Hive是基于Hadoop的一个数据仓库工具，使用它可以查询和管理分布式存储系统上的大数据集。 Hive提供了一种叫做HiveQL的类似SQL查询语言用来查询数据，HiveQL也允许熟悉MapReduce开发者开发自定义的mapper和reducer来处理内建的mapper和reducer无法完成的复杂的分析工作。 Hive的工作模式是提交一个任务，等到任务结束时被通知，而不是实时查询。 Hive安装直接去Hive官网下载最新的文件，解压。 运行bin目录里的hive文件，运行之前先启动hadoop，运行的时候可能会出现： Missing Hive CLI Jar .... 将hive解压出来的lib目录里的jline.jar拷贝到$HADOOP/share/hadoop/yarn/lib里，同时将$HADOOP/share/hadoop/yarn/lib里的jline.jar删除，重启hadoop。 再次运行，可能还会出现 The reported blocks 2662 has reached the threshold 0.9990 of total blocks 2662. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 0 seconds. ... 类似的问题，关闭hdfs的安全模式即可： hadoop dfsadmin -safemode leave 基本命令HiveQL就是模仿sql而创建的，以SQL的角度来介绍HiveQL。 DDL操作创建表： hive&gt; create table users(age INT, name STRING); 查看所有的表： hive&gt; show tables; 查看以CLIENT开头的表： hive&gt; show tables &apos;CLIENT.*&apos;; 表加列： hive&gt; alter table users add columns(gender BOOLEAN); 改表名字： hive&gt; alter table users rename to user; 删除表： hive&gt; drop table user; 查看表的具体信息： hive&gt; descibe user; hive&gt; desc user; DML操作hive创建表的时候可以指定分隔符，由于hive操作的是hdfs，数据最终会存储在hdfs上，所以hdfs上的内容肯定是以某种分隔符分开各个列的。 hive默认的列分隔符是 ^A 。 我们可以自定义自己的分隔符，在创建表的时候指定分隔符即可。 本地导入数据到hive： hive&gt; load data local inpath &apos;localFile&apos; overwrite into table users; users表的结构只有2列，name和age，而且使用默认的分隔符。 比如本地文件的内容是这样的： format1^A11 format2^A22 导入之后进行查询： hive&gt; select * from users; 显示结果： OK format1 11 format2 22 Time taken: 0.362 seconds, Fetched: 2 row(s) 在创建表的时候可以指定列分隔符和数组分隔符： hive&gt; create table users(name string, age int) &gt; ROW FORMAT DELIMITED &gt; FIELDS TERMINATED BY &apos;\\t&apos; &gt; COLLECTION ITEMS TERMINATED BY &apos;,&apos;; 导入数据还有几个参数： local参数意味着从本地加载文件，如果没有local参数，那表示从hdfs加载文件。 关键字overwrite意味着当前表中已经存在的数据将会被删除掉，没有overwrite关键字，表示数据是追加，追加到原先数据集里面。 插入数据，插入数据后会起一个map reduce job去跑插入的数据： hive&gt; insert into table users values(&apos;formatgogo&apos;, 222); 带条件的查询数据： hive&gt; select * from users where age = 11; group by查询： hive&gt; select age, count(1) from users group by age; partition的使用，以部门表为例，用type进行partition： hive&gt; create table dept(name STRING) partitioned by (type INT); 以2个文件为例，dept1.txt： dept1^A1 dept11^A1 dept111^A1 dept1111^A1 dept11111^A1 dept111111^A1 dept2.txt dept2^A2 dept22^A2 dept222^A2 dept2222^A2 dept22222^A2 dept222222^A2 使用partition之后，导入数据的时候需要指定对应的partition： hive&gt; load data local inpath &apos;$PATH/dept1.txt&apos; overwrite into table dept partition(type=1); hive&gt; load data local inpath &apos;$PATH/dept2.txt&apos; overwrite into table dept partition(type=2); hive&gt; select * from dept; 结果： OK dept1 1 dept11 1 dept111 1 dept1111 1 dept11111 1 dept111111 1 dept2 2 dept22 2 dept222 2 dept2222 2 dept22222 2 dept222222 2 Time taken: 0.114 seconds, Fetched: 12 row(s) insert可以将数据导出到指定目录，将users表导入到本地文件。 去掉local关键字表示导出到hdfs目录： hive&gt; insert overwrite local directory &apos;localFileName&apos; select * from users; hive存储在hdfs的位置进入hive控制台之后，可以使用： hive&gt; set hive.metastore.warehouse.dir; 查看hive存储在hdfs的位置，默认是存在 /user/hive/warehouse 目录。 之前的users表，会存储在/user/hive/warehouse/user目录里。 有partition的表会存在的不同位置，比如之前的dept表的type为1和2的分别存储在 /user/hive/warehouse/dept/type=1 和 /user/hive/warehouse/dept/type=2。 数据类型hive中的数据类型分2种，简单类型和复杂类型。 简单类型有以下几种：TINYINT, SMALLINT, INT, BIGINT, BOOLEAN, FLOAT, DOUBLE, STRING。 复杂类型有以下几种：Structs(结构体，学过C都知道)，MAPS(key-value键值对)，Arrays(数组类型，数组内的元素类型都必须一致) 简单类型就不分析了，来看一下复杂类型的使用： Structshive&gt; create table employee(id INT, info struct&lt;name:STRING, age:INT&gt;) &gt; ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; &gt; COLLECTION ITEMS TERMINATED BY &apos;:&apos;; 要导入的数据： 1,format:11 2,fj:22 3,formatfj:33 导入数据： hive&gt; load data local inpath &apos;localFile&apos; overwrite into table employee; 查询： select * from employee; OK 1 {&quot;name&quot;:&quot;format&quot;,&quot;age&quot;:11} 2 {&quot;name&quot;:&quot;fj&quot;,&quot;age&quot;:22} 3 {&quot;name&quot;:&quot;formatfj&quot;,&quot;age&quot;:33} Time taken: 0.042 seconds, Fetched: 3 row(s) hive&gt; select info.name from employee; OK format fj formatfj Time taken: 0.061 seconds, Fetched: 3 row(s) Mapshive&gt; create table lessons(id string, score map&lt;string, int&gt;) &gt; ROW FORMAT DELIMITED &gt; FIELDS TERMINATED BY &apos;\\t&apos; &gt; COLLECTION ITEMS TERMINATED BY &apos;,&apos; &gt; MAP KEYS TERMINATED BY &apos;:&apos;; 要导入的数据： 1 chinese:80,english:60,math:70 2 computer:60,chemistry:80 导入数据： hive&gt; load data local inpath &apos;localFile&apos; overwrite into table lessons; 查询： hive&gt; select score[&apos;chinese&apos;] from lessions; OK 80 NULL Time taken: 0.05 seconds, Fetched: 2 row(s) hive&gt; select * from lessons; OK 1 {&quot;chinese&quot;:80,&quot;english&quot;:60,&quot;math&quot;:70} 2 {&quot;computer&quot;:60,&quot;chemistry&quot;:80} Time taken: 0.032 seconds, Fetched: 2 row(s) Arrayshive&gt; create table student(name string, hobby_list array&lt;STRING&gt;) &gt; ROW FORMAT DELIMITED &gt; FIELDS TERMINATED BY &apos;,&apos; &gt; COLLECTION ITEMS TERMINATED BY &apos;:&apos;; 要导入的数据： format,basketball:football:swimming fj,coding:running 导入数据： hive&gt; load data local inpath &apos;localFile&apos; overwrite into table student; 查询(数组下标没对应的值的话返回NULL)： hive&gt; select * from student; OK format [&quot;basketball&quot;,&quot;football&quot;,&quot;swimming&quot;] fj [&quot;coding&quot;,&quot;running&quot;] Time taken: 0.041 seconds, Fetched: 2 row(s) hive&gt; select hobby_list[2] from student; OK swimming NULL Time taken: 0.07 seconds, Fetched: 2 row(s)","raw":null,"content":null,"categories":[{"name":"hive","slug":"hive","permalink":"http://fangjian0423.github.io/categories/hive/"}],"tags":[{"name":"big data","slug":"big-data","permalink":"http://fangjian0423.github.io/tags/big-data/"},{"name":"hive","slug":"hive","permalink":"http://fangjian0423.github.io/tags/hive/"}]},{"title":"google guava类库介绍","slug":"google-guava-intro","date":"2015-07-26T08:32:33.000Z","updated":"2016-05-10T14:59:25.000Z","comments":true,"path":"2015/07/26/google-guava-intro/","link":"","permalink":"http://fangjian0423.github.io/2015/07/26/google-guava-intro/","excerpt":"","text":"Guava简介Guava是一个Google开发的基于java的扩展项目，提供了很多有用的工具类，可以让java代码更加优雅，更加简洁。 Guava包括诸多工具类，比如Collections，cache，concurrent，hash，reflect，annotations，eventbus等。 刚好在看flume源码的时候看到源码里面使用了很多guava提供的代码，于是记录学习一下这个类库。 各个模块介绍Guava的wiki已经很明细地介绍了各个工具类的作用和说明。 简单翻译一下各个工具类的说明，有用到的需要了解详情的直接去官网看就可以了。 Basic utilties 基础工具类基础工具类的作用是写java语言写的更轻松。它包括了5个子模块： 1.使用和避免null，null值是有歧义的，也会引起错误。有时候它会让人很不舒服，2.前置条件,让方法中的条件检查更简单3.公用的object方法，简化object对象的hashCode和toString4.排序，Guava提供了强大的fluent Comparator5.Throwables，简化了异常和错误的传播与检查 Collections 集合Guava扩展了jdk提供的集合机制 1.不可变集合用不变的集合进行防御性编程和性能提升2.新集合类型multisets，multimaps，tables，bidirectional map等3.强大的集合工具类提供了jdk中没有的集合工具类4.扩展工具类让实现和扩展集合类变得更容易，比如创建Collection的装饰器，或实现迭代器 Caches 缓存本地缓存实现，支持多种缓存过期策略 函数式风格Guava的函数式支持可以显著简化代码，但请谨慎使用它 并发1.ListenableFuture：完成后触发回调的Future2.Service框架：抽象可开启和关闭的服务，帮助你维护服务的状态逻辑 字符串处理非常有用的字符串工具，包括分割、连接、填充等操作 原生类型扩展 JDK 未提供的原生类型（如int、char）操作， 包括某些类型的无符号形式 区间可比较类型的区间API，包括连续和离散类型 IO简化I/O尤其是I/O流和文件的操作，针对Java5和6版本 散列提供比Object.hashCode()更复杂的散列实现，并提供布鲁姆过滤器的实现 事件总线发布-订阅模式的组件通信，但组件不需要显式地注册到其他组件中 数学运算优化的、充分测试的数学工具类 反射Guava的Java反射机制工具类","raw":null,"content":null,"categories":[{"name":"java","slug":"java","permalink":"http://fangjian0423.github.io/categories/java/"}],"tags":[{"name":"java","slug":"java","permalink":"http://fangjian0423.github.io/tags/java/"},{"name":"guava","slug":"guava","permalink":"http://fangjian0423.github.io/tags/guava/"}]},{"title":"java内置的线程池笔记","slug":"java-poolthread","date":"2015-07-24T12:32:33.000Z","updated":"2016-05-10T14:59:25.000Z","comments":true,"path":"2015/07/24/java-poolthread/","link":"","permalink":"http://fangjian0423.github.io/2015/07/24/java-poolthread/","excerpt":"","text":"目前的工作是接触大数据相关的内容，自己也缺少高并发的知识，刚好前几天看了flume的源码，里面也用到了各种线程池内容，刚好学习一下，做个笔记。 写这篇博客的时候又刚好想起了当时自己实习的时候遇到的一个问题。1000个爬虫任务使用了多线程的处理方式，比如开5个线程处理这1000个任务，每个线程分200个任务，然后各个线程处理那200个爬虫任务→_→，太笨了。其实更合理的方法是使用阻塞队列+线程池的方法。 线程池的使用Java把线程的调用封装成了一个Executor接口，Executor接口中定义了一个execute方法，用来提交线程的执行。 而ExecutorService接口是Executor接口的子接口，用来管理线程的执行。 ExecutorService的初始化可以使用Executors类的静态方法。 Executors类是Java提供的一个专门用于创建线程池的类。 Executors提供了很多方法用来初始化ExecutorService，可以初始化指定数目个线程的线城市或者单个线程的线程池。 比如构造一个10个线程的线程池，使用了guava的ThreadFactoryBuilder，guava的ThreadFactoryBuilder可以传入一个namFormat参数用户来表示线程的name，它内部会使用数字增量表示%d，比如一下的nameFormat，10个线程，名字分别是thread-call-runner-1，thread-call-runner-2 … thread-call-runner-10: Executors.newFixedThreadPool(10, new ThreadFactoryBuilder().setNameFormat(&quot;thread-call-runner-%d&quot;).build()); ExecutorService线程池使用线程执行任务例子： 1.阻塞队列里有10个元素，初始化带有2个线程的线程池，跑2个线程分别去阻塞队列里取数据执行。 @Test public void test01() throws Exception { ExecutorService es = Executors.newFixedThreadPool(2, new ThreadFactoryBuilder().setNameFormat(&quot;thread-call-runner-%d&quot;).build()); final LinkedBlockingDeque&lt;String&gt; deque = new LinkedBlockingDeque&lt;String&gt;(); for(int i = 1; i &lt;= 10; i ++) { deque.add(i + &quot;&quot;); } es.submit(new Runnable() { @Override public void run() { while(!deque.isEmpty()) { System.out.println(deque.poll() + &quot;-&quot; + Thread.currentThread().getName()); } } }); es.submit(new Runnable() { @Override public void run() { while(!deque.isEmpty()) { System.out.println(deque.poll() + &quot;-&quot; + Thread.currentThread().getName()); } } }); Thread.sleep(10000l); } 打印，2个线程都会执行： 1-thread-call-runner-0 2-thread-call-runner-0 3-thread-call-runner-1 4-thread-call-runner-0 5-thread-call-runner-0 6-thread-call-runner-1 7-thread-call-runner-0 8-thread-call-runner-1 9-thread-call-runner-0 10-thread-call-runner-0 2.执行Callable线程，Callable线程和Runnable线程的区别就是Callable的线程会有返回值，这个返回值是Future，未来的意思，而且这Future是个接口，提供了几个实用的方法，比如cancel, idDone, isCancelled, get等方法。 @Test public void test02() throws Exception { ExecutorService es = Executors.newFixedThreadPool(2, new ThreadFactoryBuilder().setNameFormat(&quot;thread-call-runner-%d&quot;).build()); final LinkedBlockingDeque&lt;String&gt; deque = new LinkedBlockingDeque&lt;String&gt;(); for(int i = 1; i &lt;= 500; i ++) { deque.add(i + &quot;&quot;); } Future&lt;String&gt; result = es.submit(new Callable&lt;String&gt;() { @Override public String call() throws Exception { while (!deque.isEmpty()) { System.out.println(deque.poll() + &quot;-&quot; + Thread.currentThread().getName()); } return &quot;done&quot;; } }); System.out.println(result.isDone()); // get方法会阻塞 System.out.println(result.get()); System.out.println(&quot;exec next&quot;); } 打印： 先打印出几百个 数字-thread-call-runner-0 然后打印出 isDone的结果， 是false Future的get是得到Callable线程执行完毕后的结果，该方法会阻塞，直到该Future对应的线程全部执行完才会继续执行下去。这个例子Callable线程执行完返回done，所以get方法也是返回done get方法还有个重载的方法，带有2个参数，第一个参数是一个long类型的数字，第二个参数是时间单位。result.get(1, TimeUnit.MILLISECONDS) 就表示1毫秒，等待这个Future的时间为1毫秒，如果时间1毫秒，那么这个get方法的调用会抛出java.util.concurrent.TimeoutException异常，并且线程内部的执行也会停止。注意，但是如果我们catch这个TimeoutException的话，那么线程里的代码还是会被执行完毕。 try { // catch TimeoutException的话线程里的代码还是会执行下去 System.out.println(result.get(10, TimeUnit.MILLISECONDS)); } catch (TimeoutException e) { e.printStackTrace(); } 3.Future的cancel方法的使用 @Test public void test03() throws Exception { ExecutorService es = Executors.newFixedThreadPool(2, new ThreadFactoryBuilder().setNameFormat(&quot;thread-call-runner-%d&quot;).build()); final LinkedBlockingDeque&lt;String&gt; deque = new LinkedBlockingDeque&lt;String&gt;(); for(int i = 1; i &lt;= 5000; i ++) { deque.add(i + &quot;&quot;); } Future&lt;String&gt; result = es.submit(new Callable&lt;String&gt;() { @Override public String call() throws Exception { while (!deque.isEmpty() &amp;&amp; !Thread.currentThread().isInterrupted()) { System.out.println(deque.poll() + &quot;-&quot; + Thread.currentThread().getName()); } return &quot;done&quot;; } }); try { System.out.println(result.get(10, TimeUnit.MILLISECONDS)); } catch (TimeoutException e) { System.out.println(&quot;cancel result: &quot; + result.cancel(true)); System.out.println(&quot;is cancelled: &quot; + result.isCancelled()); } Thread.sleep(2000l); } 打印： 先打印出几百个 数字-thread-call-runner-0 然后打印出 cancel result: true is cancelled: true cancel方法用来取消线程的继续执行，它有个boolean类型的返回值，表示是否cancel成功。这里我们使用了get方法，10毫秒处理5000条数据，报了TimeoutException异常，catch之后对Future进行了cancel调用。注意，我们在Callable里执行的代码里加上了 !Thread.currentThread().isInterrupted() 如果去掉了这个条件，那么还是会打印出5000条处理数据。cancel方法底层会去interrupted对应的线程，所以才需要加上这个条件的判断。 调度线程池的使用ScheduledExecutorService接口是ExecutorService接口的子类。 看名字也知道，ScheduledExecutorService是基于调度的线程池。 1.ScheduledExecutorService的schedule方法例子： @Test public void test04() throws Exception { ScheduledExecutorService ses = Executors.newScheduledThreadPool(2, new ThreadFactoryBuilder().setNameFormat(&quot;thread-schedule-runner-%d&quot;).build()); Future&lt;String&gt; result = ses.schedule(new Callable&lt;String&gt;() { @Override public String call() throws Exception { System.out.println(&quot;exec task&quot;); return &quot;ok&quot;; } }, 10, TimeUnit.SECONDS); Thread.sleep(15000); } 打印： 执行10秒后打印出 exec task 2.cancel在schedule中的使用： @Test public void test05() throws Exception { ScheduledExecutorService ses = Executors.newScheduledThreadPool(2, new ThreadFactoryBuilder().setNameFormat(&quot;thread-schedule-runner-%d&quot;).build()); Future&lt;String&gt; result = ses.schedule(new Callable&lt;String&gt;() { @Override public String call() throws Exception { System.out.println(&quot;exec task&quot;); try { Thread.sleep(5000l); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(&quot;exec done, take 5 seconds&quot;); return &quot;ok&quot;; } }, 10, TimeUnit.SECONDS); Thread.sleep(11000); result.cancel(true); Thread.sleep(10000); } 打印： 先打印出exec task，然后抛出InterruptedException异常，异常被catch。接着打印出exec done, take 5 seconds 因为Callable线程是10秒后执行的，线程会执行5秒，在11秒的时候会调用Future的cancel方法，会取消线程的时候，由于我们catch了异常，所以线程会执行完毕。 注意一下，cancel方法有个boolean类型的参数mayInterruptIfRunning。上个例子中我们传入了true，所以会打断正在执行的线程，因此抛出了异常。如果我们传入false，线程正在执行，所以不会去打断它，因此会打印出exec task，然后再打印出exec done, take 5 seconds，并且没有异常抛出。 3.scheduleWithFixedDelay方法，定时器，每隔多少时间执行 @Test public void test06() throws Exception { ScheduledExecutorService sec = Executors.newScheduledThreadPool(2, new ThreadFactoryBuilder().setNameFormat(&quot;thread-schedule-runner-%d&quot;).build()); sec.scheduleWithFixedDelay(new Runnable() { @Override public void run() { System.out.println(&quot;exec&quot;); } }, 0, 5, TimeUnit.SECONDS); Thread.sleep(16000l); } 打印： 打印出4次exec。 scheduleWithFixedDelay有4个参数，第一个参数是个Runnable接口的实现类，第二个参数是首次执行线程的延迟时间，第三个参数是每隔多少时间再次执行线程时间，第四个参数是时间的单位。 如果Runnable中执行的代码发生了异常并且没有被catch的话，那么发生异常之后，Runnable里的代码就不会再次执行。 4.scheduleAtFixedRate方法，scheduleAtFixedRate方法跟scheduleWithFixedDelay类似。唯一的区别是scheduleWithFixedDelay是在线程全部执行完毕之后开始计算时间的，而scheduleAtFixedRate是在线程开始执行的时候计算时间的，所以scheduleAtFixedRate有时会产生不是定时执行的感觉。 先看scheduleWithFixedDelay： @Test public void test07() throws Exception { ScheduledExecutorService sec = Executors.newScheduledThreadPool(2, new ThreadFactoryBuilder().setNameFormat(&quot;thread-schedule-runner-%d&quot;).build()); sec.scheduleWithFixedDelay(new Runnable() { @Override public void run() { System.out.println(&quot;exec&quot;); try { Thread.sleep(3000l); } catch (Exception e) { e.printStackTrace(); } } }, 0, 5, TimeUnit.SECONDS); Thread.sleep(17000l); } 打印： 执行3次exec。Runnable每次执行3秒。第一次是在0秒执行，执行了3秒，第二次是在8秒，执行了3秒。第三次是在16秒执行 然后再看scheduleAtFixedRate： @Test public void test09() throws Exception { ScheduledExecutorService sec = Executors.newScheduledThreadPool(2, new ThreadFactoryBuilder().setNameFormat(&quot;thread-schedule-runner-%d&quot;).build()); sec.scheduleAtFixedRate(new Runnable() { @Override public void run() { System.out.println(&quot;exec&quot;); try { Thread.sleep(3000l); } catch (Exception e) { e.printStackTrace(); } } }, 0, 5, TimeUnit.SECONDS); Thread.sleep(16000l); } 打印： 执行了4次exec，第一次在0秒执行，第二次在5秒，第三次是10秒，第四次在15秒执行 线程池的参数解释Executors类的静态方法中大致有这么几种创建不同类型的线程池： newFixedThreadPool newSingleThreadExecutor newCachedThreadPool newScheduledThreadPool newSingleThreadScheduledExecutor 在分析这些方法之前，先解释一下线程池中所使用的一些配置参数。 比如实例化一个单线程的线程池。源码如下： public static ExecutorService newSingleThreadExecutor() { return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;())); } ThreadPoolExecutor是线程池的具体实现类。它的构造函数接受以下这几个重要的参数： corePoolSize：线程池基本数量 workQueue：用于保存等待执行的任务的阻塞队列。阻塞队列的类型可自己选择，阻塞有这几种类型，ArrayBlockingQueue(基于数组的有界阻塞队列，按FIFO进出任务)，LinkedBlockingQueue(基于链表的阻塞队列，FIFO方式)，SynchronousQueue(不存储元素的阻塞队列。每个插入操作必须等到另一个线程调用移除操作，否则插入操作一直处于阻塞状态，吞吐量要高于LinkedBlockingQueue，是一个无限阻塞队列)，PriorityBlockingQueue(具有优先级的无限阻塞队列) maximunPoolSize：线程池最大数量，也就是线程池允许创建的最大线程数。这个参数跟阻塞队列的关系是这样的：如果阻塞队列满了，这个时候又来了一个任务，那么这个时候如果当前线程数小于最大数量，那么就会直接创建新的线程执行任务。当然，如果工作队列是无界的，那么这个参数就没有意义了，因为队列无界，任务都会在队列中存储着 ThreadFactory：创建线程的工厂 RejectedExecutionHandler：饱和策略，也就是当队列和线程数目都满了以后，采取的策略。有AbortPolicy(直接抛出异常)，CallerRunsPolicy(只用调用者所在线程来运行任务)，DiscardOldestPolicy(丢弃队列里最近的一个任务，并执行当前任务)，DiscardPolicy(不处理，直接丢弃)。当然，还可以自定义策略 keeyAliveTime：存活时间。如果当前线程池中的线程数量比基本数量要多，并且是闲置状态的话，这些闲置的线程能存活的最大时间 TimeUnit，跟第6个参数一起使用，表示存活时间的时间单位 这些参数中，有3个参数跟线程池大小有关，分别是基本数量，最大数量和阻塞队列。 线程池的处理流程是这样的： 首先判断线程池的基本大小，如果基本大小还没满，那么直接创建新的线程执行任务，否则进行下一步 判断线程池中的阻塞队列是是否已满，没满的话存到阻塞队列里等待执行，否则执行下一步(所以如果是个无界的阻塞队列，那么这一步永远都成立) 判断线程池最大大小是否已满，没满的话直接创建线程执行任务，否则交给饱和策略处理 以1个例子来说明： ThreadPoolExecutor threadPoolExecutor = new ThreadPoolExecutor(1, 20, 10, TimeUnit.SECONDS,new LinkedBlockingQueue(10)); for(int i = 0; i &lt; 15; i ++) { threadPoolExecutor.submit(new MyThread(i + 1)); } static class MyThread implements Runnable { private int index; public MyThread(int index) { this.index = index; } @Override public void run() { System.out.println(this.index); try { Thread.sleep(3000); } catch (InterruptedException e) { e.printStackTrace(); } } } 输出： 1 12 13 14 15 2 3 4 5 6 7 8 9 10 11 很明显地看到，线程池基本大小只有1个，所以先输出10，接下来阻塞队列有最多有10个线程等待需要执行，但是多出了12，13，14，15这4个线程，由于最大线程数是20个，所以这4个线程就直接被创建开始执行了。 下面就分析Executors的静态方法： newFixedThreadPool方法: public static ExecutorService newFixedThreadPool(int nThreads) { return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;()); } 创建一个基本大小和最大大小都为nThreads，阻塞队列长度为Integer.MAX_VALUE的线程池。 newSingleThreadExecutor方法： public static ExecutorService newSingleThreadExecutor() { return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;())); } 创建一个基本大小和最大大小都为1，阻塞队列长度为Integer.MAX_VALUE的线程池。 newCachedThreadPool方法： public static ExecutorService newCachedThreadPool() { return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;()); } 创建一个基本大小为0，最大大小都为Integer.MAX_VALUE，阻塞队列为无界队列的线程池。 newScheduledThreadPool方法： public static ScheduledExecutorService newScheduledThreadPool(int corePoolSize) { return new ScheduledThreadPoolExecutor(corePoolSize); } 创建一个基本大小为corePoolSize，最大大小都为Integer.MAX_VALUE，阻塞队列为DelayedWorkQueue的线程池。 newSingleThreadScheduledExecutor方法： 创建一个基本大小为corePoolSize，最大大小都为Integer.MAX_VALUE，阻塞队列为DelayedWorkQueue的线程池。 public static ScheduledExecutorService newSingleThreadScheduledExecutor() { return new DelegatedScheduledExecutorService (new ScheduledThreadPoolExecutor(1)); } 创建一个基本大小为1，最大大小都为Integer.MAX_VALUE，阻塞队列为DelayedWorkQueue的线程池。","raw":null,"content":null,"categories":[{"name":"java","slug":"java","permalink":"http://fangjian0423.github.io/categories/java/"}],"tags":[{"name":"java","slug":"java","permalink":"http://fangjian0423.github.io/tags/java/"},{"name":"thread","slug":"thread","permalink":"http://fangjian0423.github.io/tags/thread/"}]},{"title":"通过源码分析Flume HDFSSink 写hdfs文件的过程","slug":"flume-hdfs-sink","date":"2015-07-20T15:32:33.000Z","updated":"2016-05-10T14:59:25.000Z","comments":true,"path":"2015/07/20/flume-hdfs-sink/","link":"","permalink":"http://fangjian0423.github.io/2015/07/20/flume-hdfs-sink/","excerpt":"","text":"Flume有HDFS Sink，可以将Source进来的数据写入到hdfs中。 HDFS Sink具体的逻辑代码是在HDFSEventSink这个类中。 HDFS Sink跟写文件相关的配置如下： hdfs.path -&gt; hdfs目录路径hdfs.filePrefix -&gt; 文件前缀。默认值FlumeDatahdfs.fileSuffix -&gt; 文件后缀hdfs.rollInterval -&gt; 多久时间后close hdfs文件。单位是秒，默认30秒。设置为0的话表示不根据时间close hdfs文件hdfs.rollSize -&gt; 文件大小超过一定值后，close文件。默认值1024，单位是字节。设置为0的话表示不基于文件大小hdfs.rollCount -&gt; 写入了多少个事件后close文件。默认值是10个。设置为0的话表示不基于事件个数hdfs.fileType -&gt; 文件格式， 有3种格式可选择：SequenceFile, DataStream or CompressedStreamhdfs.batchSize -&gt; 批次数，HDFS Sink每次从Channel中拿的事件个数。默认值100hdfs.minBlockReplicas -&gt; HDFS每个块最小的replicas数字，不设置的话会取hadoop中的配置hdfs.maxOpenFiles -&gt; 允许最多打开的文件数，默认是5000。如果超过了这个值，越早的文件会被关闭serializer -&gt; HDFS Sink写文件的时候会进行序列化操作。会调用对应的Serializer借口，可以自定义符合需求的Serializerhdfs.retryInterval -&gt; 关闭HDFS文件失败后重新尝试关闭的延迟数，单位是秒hdfs.callTimeout -&gt; HDFS操作允许的时间，比如hdfs文件的open，write，flush，close操作。单位是毫秒，默认值是10000 HDFSEventSink分析以一个hdfs.path，hdfs.filePrefix和hdfs.fileSuffix分别为/data/%Y/%m/%d/%H，flume, .txt 为例子，分析源码： 直接看HDFSEventSink的process方法： public Status process() throws EventDeliveryException { // 得到Channel Channel channel = getChannel(); Transaction transaction = channel.getTransaction(); // 构造一个BucketWriter集合，BucketWriter就是处理hdfs文件的具体逻辑实现类 List&lt;BucketWriter&gt; writers = Lists.newArrayList(); // Channel的事务启动 transaction.begin(); try { int txnEventCount = 0; // 每次处理batchSize个事件。这里的batchSize就是之前配置的hdfs.batchSize for (txnEventCount = 0; txnEventCount &lt; batchSize; txnEventCount++) { Event event = channel.take(); if (event == null) { break; } // 构造hdfs文件所在的路径 String realPath = BucketPath.escapeString(filePath, event.getHeaders(), timeZone, needRounding, roundUnit, roundValue, useLocalTime); // 构造hdfs文件名, fileName就是之前配置的hdfs.filePrefix，即flume String realName = BucketPath.escapeString(fileName, event.getHeaders(), timeZone, needRounding, roundUnit, roundValue, useLocalTime); // 构造hdfs文件路径，根据之前的path，filePrefix，fileSuffix // 得到这里的lookupPath为 /data/2015/07/20/15/flume String lookupPath = realPath + DIRECTORY_DELIMITER + realName; BucketWriter bucketWriter; HDFSWriter hdfsWriter = null; // 构造一个回调函数 WriterCallback closeCallback = new WriterCallback() { @Override public void run(String bucketPath) { LOG.info(&quot;Writer callback called.&quot;); synchronized (sfWritersLock) { // sfWriters是一个HashMap，最多支持maxOpenFiles个键值对。超过maxOpenFiles的话会关闭越早进来的文件 // 回调函数的作用就是hdfs文件close的时候移除sfWriters中对应的那个文件。防止打开的文件数超过maxOpenFiles // sfWriters这个Map中的key是要写的hdfs路径，value是BucketWriter sfWriters.remove(bucketPath); } } }; synchronized (sfWritersLock) { // 先查看sfWriters是否已经存在key为/data/2015/07/20/15/flume的BucketWriter bucketWriter = sfWriters.get(lookupPath); if (bucketWriter == null) { // 没有的话构造一个BucketWriter // 先根据fileType得到对应的HDFSWriter，fileType默认有3种类型，分别是SequenceFile, DataStream or CompressedStream hdfsWriter = writerFactory.getWriter(fileType); // 构造一个BucketWriter，会将刚刚构造的hdfsWriter当做参数传入，BucketWriter写hdfs文件的时候会使用HDFSWriter bucketWriter = initializeBucketWriter(realPath, realName, lookupPath, hdfsWriter, closeCallback); // 新构造的BucketWriter放入到sfWriters中 sfWriters.put(lookupPath, bucketWriter); } } // 将BucketWriter放入到writers集合中 if (!writers.contains(bucketWriter)) { writers.add(bucketWriter); } // 写hdfs数据 try { bucketWriter.append(event); } catch (BucketClosedException ex) { LOG.info(&quot;Bucket was closed while trying to append, &quot; + &quot;reinitializing bucket and writing event.&quot;); hdfsWriter = writerFactory.getWriter(fileType); bucketWriter = initializeBucketWriter(realPath, realName, lookupPath, hdfsWriter, closeCallback); synchronized (sfWritersLock) { sfWriters.put(lookupPath, bucketWriter); } bucketWriter.append(event); } } if (txnEventCount == 0) { sinkCounter.incrementBatchEmptyCount(); } else if (txnEventCount == batchSize) { sinkCounter.incrementBatchCompleteCount(); } else { sinkCounter.incrementBatchUnderflowCount(); } // 每个批次全部完成后flush所有的hdfs文件 for (BucketWriter bucketWriter : writers) { bucketWriter.flush(); } // 事务提交 transaction.commit(); if (txnEventCount &lt; 1) { return Status.BACKOFF; } else { sinkCounter.addToEventDrainSuccessCount(txnEventCount); return Status.READY; } } catch (IOException eIO) { // 发生异常事务回滚 transaction.rollback(); LOG.warn(&quot;HDFS IO error&quot;, eIO); return Status.BACKOFF; } catch (Throwable th) { // 发生异常事务回滚 transaction.rollback(); LOG.error(&quot;process failed&quot;, th); if (th instanceof Error) { throw (Error) th; } else { throw new EventDeliveryException(th); } } finally { // 关闭事务 transaction.close(); } } BucketWriter分析接下来我们看下BucketWriter的append和flush方法。 append方法： public synchronized void append(final Event event) throws IOException, InterruptedException { checkAndThrowInterruptedException(); // If idleFuture is not null, cancel it before we move forward to avoid a // close call in the middle of the append. if(idleFuture != null) { idleFuture.cancel(false); // There is still a small race condition - if the idleFuture is already // running, interrupting it can cause HDFS close operation to throw - // so we cannot interrupt it while running. If the future could not be // cancelled, it is already running - wait for it to finish before // attempting to write. if(!idleFuture.isDone()) { try { idleFuture.get(callTimeout, TimeUnit.MILLISECONDS); } catch (TimeoutException ex) { LOG.warn(&quot;Timeout while trying to cancel closing of idle file. Idle&quot; + &quot; file close may have failed&quot;, ex); } catch (Exception ex) { LOG.warn(&quot;Error while trying to cancel closing of idle file. &quot;, ex); } } idleFuture = null; } // 如果hdfs文件没有被打开 if (!isOpen) { // hdfs已关闭的话抛出异常 if (closed) { throw new BucketClosedException(&quot;This bucket writer was closed and &quot; + &quot;this handle is thus no longer valid&quot;); } // 打开hdfs文件 open(); } // 查看是否需要创建新文件 if (shouldRotate()) { boolean doRotate = true; if (isUnderReplicated) { if (maxConsecUnderReplRotations &gt; 0 &amp;&amp; consecutiveUnderReplRotateCount &gt;= maxConsecUnderReplRotations) { doRotate = false; if (consecutiveUnderReplRotateCount == maxConsecUnderReplRotations) { LOG.error(&quot;Hit max consecutive under-replication rotations ({}); &quot; + &quot;will not continue rolling files under this path due to &quot; + &quot;under-replication&quot;, maxConsecUnderReplRotations); } } else { LOG.warn(&quot;Block Under-replication detected. Rotating file.&quot;); } consecutiveUnderReplRotateCount++; } else { consecutiveUnderReplRotateCount = 0; } if (doRotate) { // 如果需要创建新文件的时候会关闭文件，然后再打开新的文件。这里的close方法没有参数，表示可以再次打开新的文件 close(); open(); } } // 写event数据 try { sinkCounter.incrementEventDrainAttemptCount(); callWithTimeout(new CallRunner&lt;Void&gt;() { @Override public Void call() throws Exception { // 真正的写数据使用HDFSWriter的append方法 writer.append(event); // could block return null; } }); } catch (IOException e) { LOG.warn(&quot;Caught IOException writing to HDFSWriter ({}). Closing file (&quot; + bucketPath + &quot;) and rethrowing exception.&quot;, e.getMessage()); try { close(true); } catch (IOException e2) { LOG.warn(&quot;Caught IOException while closing file (&quot; + bucketPath + &quot;). Exception follows.&quot;, e2); } throw e; } // 文件大小+起来 processSize += event.getBody().length; // 事件个数+1 eventCounter++; // 批次数+1 batchCounter++; // 批次数达到配置的hdfs.batchSize的话调用flush方法 if (batchCounter == batchSize) { flush(); } } 先看下open方法，打开hdfs文件的方法： private void open() throws IOException, InterruptedException { // hdfs文件路径或HDFSWriter没构造的话抛出异常 if ((filePath == null) || (writer == null)) { throw new IOException(&quot;Invalid file settings&quot;); } final Configuration config = new Configuration(); // disable FileSystem JVM shutdown hook config.setBoolean(&quot;fs.automatic.close&quot;, false); // Hadoop is not thread safe when doing certain RPC operations, // including getFileSystem(), when running under Kerberos. // open() must be called by one thread at a time in the JVM. // NOTE: tried synchronizing on the underlying Kerberos principal previously // which caused deadlocks. See FLUME-1231. synchronized (staticLock) { checkAndThrowInterruptedException(); try { // fileExtensionCounter是一个AtomicLong类型的实例，初始化为当前时间戳的数值 // 由于之前分析的，可能存在先关闭文件，然后再次open新文件的情况。所以在同一个BucketWriter类中open方法得到的文件名时间戳仅仅相差1 // 得到时间戳counter long counter = fileExtensionCounter.incrementAndGet(); // 最终的文件名加上时间戳，这就是为什么flume生成的文件名会带有时间戳的原因 // 这里的fullFileName就是 flume.1437375933234 String fullFileName = fileName + &quot;.&quot; + counter; // 加上后缀名， fullFileName就成了flume.1437375933234.txt if (fileSuffix != null &amp;&amp; fileSuffix.length() &gt; 0) { fullFileName += fileSuffix; } else if (codeC != null) { fullFileName += codeC.getDefaultExtension(); } // 由于没配置inUsePrefix和inUseSuffix。 故这两个属性的值分别为&quot;&quot;和&quot;.tmp&quot; // buckerPath为 /data/2015/07/20/15/flume.1437375933234.txt.tmp bucketPath = filePath + &quot;/&quot; + inUsePrefix + fullFileName + inUseSuffix; // targetPath为 /data/2015/07/20/15/flume.1437375933234.txt targetPath = filePath + &quot;/&quot; + fullFileName; LOG.info(&quot;Creating &quot; + bucketPath); callWithTimeout(new CallRunner&lt;Void&gt;() { @Override public Void call() throws Exception { if (codeC == null) { // Need to get reference to FS using above config before underlying // writer does in order to avoid shutdown hook &amp; // IllegalStateExceptions if(!mockFsInjected) { fileSystem = new Path(bucketPath).getFileSystem( config); } // 使用HDFSWriter打开文件 writer.open(bucketPath); } else { // need to get reference to FS before writer does to // avoid shutdown hook if(!mockFsInjected) { fileSystem = new Path(bucketPath).getFileSystem( config); } // 使用HDFSWriter打开文件 writer.open(bucketPath, codeC, compType); } return null; } }); } catch (Exception ex) { sinkCounter.incrementConnectionFailedCount(); if (ex instanceof IOException) { throw (IOException) ex; } else { throw Throwables.propagate(ex); } } } isClosedMethod = getRefIsClosed(); sinkCounter.incrementConnectionCreatedCount(); // 重置各个计数器 resetCounters(); // 开线程处理hdfs.rollInterval配置的参数，多长时间后调用close方法 if (rollInterval &gt; 0) { Callable&lt;Void&gt; action = new Callable&lt;Void&gt;() { public Void call() throws Exception { LOG.debug(&quot;Rolling file ({}): Roll scheduled after {} sec elapsed.&quot;, bucketPath, rollInterval); try { // Roll the file and remove reference from sfWriters map. close(true); } catch(Throwable t) { LOG.error(&quot;Unexpected error&quot;, t); } return null; } }; // 以秒为单位在这里指定。将这个线程执行的结果赋值给timedRollFuture这个属性 timedRollFuture = timedRollerPool.schedule(action, rollInterval, TimeUnit.SECONDS); } isOpen = true; } flush方法，只会在close和append方法(处理的事件数等于批次数)中被调用： public synchronized void flush() throws IOException, InterruptedException { checkAndThrowInterruptedException(); if (!isBatchComplete()) { //isBatchComplete判断batchCount是否等于0。 所以这里只要batchCount不为0，那么执行下去 doFlush(); // doFlush方法会调用HDFSWriter的sync方法，并且将batchCount设置为0 // idleTimeout没有配置，以下代码不会执行 if(idleTimeout &gt; 0) { // if the future exists and couldn&apos;t be cancelled, that would mean it has already run // or been cancelled if(idleFuture == null || idleFuture.cancel(false)) { Callable&lt;Void&gt; idleAction = new Callable&lt;Void&gt;() { public Void call() throws Exception { LOG.info(&quot;Closing idle bucketWriter {} at {}&quot;, bucketPath, System.currentTimeMillis()); if (isOpen) { close(true); } return null; } }; idleFuture = timedRollerPool.schedule(idleAction, idleTimeout, TimeUnit.SECONDS); } } } } close方法： public synchronized void close(boolean callCloseCallback) throws IOException, InterruptedException { checkAndThrowInterruptedException(); try { // close的时候先执行flush方法，清空batchCount，并调用HDFSWriter的sync方法 flush(); } catch (IOException e) { LOG.warn(&quot;pre-close flush failed&quot;, e); } boolean failedToClose = false; LOG.info(&quot;Closing {}&quot;, bucketPath); // 创建一个关闭线程，这个线程会调用HDFSWriter的close方法 CallRunner&lt;Void&gt; closeCallRunner = createCloseCallRunner(); if (isOpen) { // 如果文件还开着 try { // 执行HDFSWriter的close方法 callWithTimeout(closeCallRunner); sinkCounter.incrementConnectionClosedCount(); } catch (IOException e) { LOG.warn( &quot;failed to close() HDFSWriter for file (&quot; + bucketPath + &quot;). Exception follows.&quot;, e); sinkCounter.incrementConnectionFailedCount(); failedToClose = true; // 关闭文件失败的话起个线程，retryInterval秒后继续执行 final Callable&lt;Void&gt; scheduledClose = createScheduledCloseCallable(closeCallRunner); timedRollerPool.schedule(scheduledClose, retryInterval, TimeUnit.SECONDS); } isOpen = false; } else { LOG.info(&quot;HDFSWriter is already closed: {}&quot;, bucketPath); } // timedRollFuture就是根据hdfs.rollInterval配置生成的一个属性。如果hdfs.rollInterval配置为0，那么不会执行以下代码 // 因为要close文件，所以如果开启了hdfs.rollInterval等待时间到了flush文件，由于文件已经关闭，再次关闭会有问题 // 所以这里取消timedRollFuture线程的执行 if (timedRollFuture != null &amp;&amp; !timedRollFuture.isDone()) { timedRollFuture.cancel(false); // do not cancel myself if running! timedRollFuture = null; } // 没有配置hdfs.idleTimeout， 不会执行 if (idleFuture != null &amp;&amp; !idleFuture.isDone()) { idleFuture.cancel(false); // do not cancel myself if running! idleFuture = null; } // 重命名文件，如果报错了，不会重命名文件 if (bucketPath != null &amp;&amp; fileSystem != null &amp;&amp; !failedToClose) { // 将 /data/2015/07/20/15/flume.1437375933234.txt.tmp 重命名为 /data/2015/07/20/15/flume.1437375933234.txt renameBucket(bucketPath, targetPath, fileSystem); } if (callCloseCallback) { // callCloseCallback是close方法的参数 // 调用关闭文件的回调函数，也就是BucketWriter的onCloseCallback属性 // 这个onCloseCallback属性就是在HDFSEventSink里的回调函数closeCallback。 用来处理sfWriters.remove(bucketPath); // 如果onCloseCallback属性为true，那么说明这个BucketWriter已经不会再次open新的文件了。生命周期已经到了。 // onCloseCallback只有在append方法中调用shouldRotate方法的时候需要close文件的时候才会传入false，其他情况都是true runCloseAction(); closed = true; } } 再回过头来看下append方法里的shouldRotate方法，shouldRotate方法执行下去的话会关闭文件然后再次打开新的文件： private boolean shouldRotate() { boolean doRotate = false; // 调用HDFSWriter的isUnderReplicated方法，用来判断当前hdfs文件是否正在复制。 if (writer.isUnderReplicated()) { this.isUnderReplicated = true; doRotate = true; } else { this.isUnderReplicated = false; } // rollCount就是配置的hdfs.rollCount。 eventCounter事件数达到rollCount之后，会close文件，然后创建新的文件 if ((rollCount &gt; 0) &amp;&amp; (rollCount &lt;= eventCounter)) { LOG.debug(&quot;rolling: rollCount: {}, events: {}&quot;, rollCount, eventCounter); doRotate = true; } // rollSize就是配置的hdfs.rollSize。processSize是每个事件加起来的文件大小。当processSize超过rollSize的时候，会close文件，然后创建新的文件 if ((rollSize &gt; 0) &amp;&amp; (rollSize &lt;= processSize)) { LOG.debug(&quot;rolling: rollSize: {}, bytes: {}&quot;, rollSize, processSize); doRotate = true; } return doRotate; } HDFSWriter分析每个BucketWriter中对应只有一个HDFSWriter。 HDFSWriter是一个接口，有3个具体的实现类，分别是：HDFSDataStream，HDFSSequenceFile和HDFSCompressedDataStream。分别对应fileType为DataStream，SequenceFile和CompressedStream。 我们以HDFSDataStream为例，分析一下在BucketWriter中用到的HDFSWriter的一些方法： append方法，写hdfs文件： @Override public void append(Event e) throws IOException { // 非常简单，直接使用serializer的write方法 // serializer是org.apache.flume.serialization.EventSerializer接口的实现类 // 默认的Serializer是BodyTextEventSerializer serializer.write(e); } open方法： @Override public void open(String filePath) throws IOException { Configuration conf = new Configuration(); // 构造hdfs路径 Path dstPath = new Path(filePath); FileSystem hdfs = getDfs(conf, dstPath); // 调用doOpen方法 doOpen(conf, dstPath, hdfs); } protected void doOpen(Configuration conf, Path dstPath, FileSystem hdfs) throws IOException { if(useRawLocalFileSystem) { if(hdfs instanceof LocalFileSystem) { hdfs = ((LocalFileSystem)hdfs).getRaw(); } else { logger.warn(&quot;useRawLocalFileSystem is set to true but file system &quot; + &quot;is not of type LocalFileSystem: &quot; + hdfs.getClass().getName()); } } boolean appending = false; // 构造FSDataOutputStream，作为属性outStream if (conf.getBoolean(&quot;hdfs.append.support&quot;, false) == true &amp;&amp; hdfs.isFile (dstPath)) { outStream = hdfs.append(dstPath); appending = true; } else { outStream = hdfs.create(dstPath); } // 初始化Serializer serializer = EventSerializerFactory.getInstance( serializerType, serializerContext, outStream); if (appending &amp;&amp; !serializer.supportsReopen()) { outStream.close(); serializer = null; throw new IOException(&quot;serializer (&quot; + serializerType + &quot;) does not support append&quot;); } // must call superclass to check for replication issues registerCurrentStream(outStream, hdfs, dstPath); if (appending) { serializer.afterReopen(); } else { serializer.afterCreate(); } } close方法： @Override public void close() throws IOException { serializer.flush(); serializer.beforeClose(); outStream.flush(); outStream.sync(); outStream.close(); unregisterCurrentStream(); } sync方法： @Override public void sync() throws IOException { serializer.flush(); outStream.flush(); outStream.sync(); } isUnderReplicated方法，在AbstractHDFSWriter中定义： @Override public boolean isUnderReplicated() { try { // 得到目前文件replication后的块数 int numBlocks = getNumCurrentReplicas(); if (numBlocks == -1) { return false; } int desiredBlocks; if (configuredMinReplicas != null) { // 如果配置了hdfs.minBlockReplicas desiredBlocks = configuredMinReplicas; } else { // 没配置hdfs.minBlockReplicas的话直接从hdfs配置中拿 desiredBlocks = getFsDesiredReplication(); } // 如果当前复制的块比期望要复制的块数字要小的话，返回true return numBlocks &lt; desiredBlocks; } catch (IllegalAccessException e) { logger.error(&quot;Unexpected error while checking replication factor&quot;, e); } catch (InvocationTargetException e) { logger.error(&quot;Unexpected error while checking replication factor&quot;, e); } catch (IllegalArgumentException e) { logger.error(&quot;Unexpected error while checking replication factor&quot;, e); } return false; } 总结hdfs.rollInterval，hdfs.rollSize，hdfs.rollCount，hdfs.minBlockReplicas，hdfs.batchSize这5个配置影响着hdfs文件的关闭。 注意，这5个配置影响的是一个hdfs文件，是一个hdfs文件。当hdfs文件关闭的时候，这些配置指标会重新开始计算。因为BucketWriter中的open方法里会调用resetCounters方法，这个方法会重置计数器。而基于hdfs.rollInterval的timedRollFuture线程返回值是在close方法中被销毁的。因此，只要close文件，并且open新文件的时候，这5个属性都会重新开始计算。 hdfs.rollInterval与时间有关，当时间达到hdfs.rollInterval配置的秒数，那么会close文件。 hdfs.rollSize与每个event的字节大小有关，当一个一个event的字节相加起来大于等于hdfs.rollSize的时候，那么会close文件。 hdfs.rollCount与事件的个数有关，当事件个数大于等于hdfs.rollCount的时候，那么会close文件。 hdfs.batchSize表示当事件添加到hdfs.batchSize个的时候，也就是说HDFS Sink每次会拿hdfs.batchSize个事件，而且这些所有的事件都写进了同一个hdfs文件，这才会触发本次条件，并且其他4个配置都未达成条件。然后会close文件。 hdfs.minBlockReplicas表示期望hdfs对文件最小的复制块数。所以有时候我们配置了hdfs.rollInterval，hdfs.rollSize，hdfs.rollCount这3个参数，并且这3个参数都没有符合条件，但是还是生成了多个文件，这就是因为这个参数导致的，而且这个参数的优先级比hdfs.rollSize，hdfs.rollCount要高。","raw":null,"content":null,"categories":[{"name":"flume","slug":"flume","permalink":"http://fangjian0423.github.io/categories/flume/"}],"tags":[{"name":"big data","slug":"big-data","permalink":"http://fangjian0423.github.io/tags/big-data/"},{"name":"flume","slug":"flume","permalink":"http://fangjian0423.github.io/tags/flume/"}]},{"title":"Flume几个比较有用的功能和一些坑(用到新功能后会更新文章)","slug":"flume-notes","date":"2015-07-14T14:23:23.000Z","updated":"2016-05-10T14:59:25.000Z","comments":true,"path":"2015/07/14/flume-notes/","link":"","permalink":"http://fangjian0423.github.io/2015/07/14/flume-notes/","excerpt":"","text":"根据项目的经验，介绍几个flume比较有用的功能。 ChannelSelector功能flume内置的ChannelSelector有两种，分别是Replicating和Multiplexing。 Replicating类型的ChannelSelector会针对每一个Event，拷贝到所有的Channel中，这是默认的ChannelSelector。 replicating类型的ChannelSelector例子如下： a1.sources = r1 a1.channels = c1 c2 # 如果有100个Event，那么c1和c2中都会有这100个事件 a1.channels.c1.type = memory a1.channels.c1.capacity = 1000 a1.channels.c1.transactionCapacity = 100 a1.channels.c2.type = memory a1.channels.c2.capacity = 1000 a1.channels.c2.transactionCapacity = 100 Multiplexing类型的ChannelSelector会根据Event中Header中的某个属性决定分发到哪个Channel。 multiplexing类型的ChannelSelector例子如下： a1.sources = r1 a1.sources.source1.selector.type = multiplexing a1.sources.source1.selector.header = validation # 以header中的validation对应的值作为条件 a1.sources.source1.selector.mapping.SUCCESS = c2 # 如果header中validation的值为SUCCESS，使用c2这个channel a1.sources.source1.selector.mapping.FAIL = c1 # 如果header中validation的值为FAIL，使用c1这个channel a1.sources.source1.selector.default = c1 # 默认使用c1这个channel Sink的SerializerHDFS Sink， HBase Sink，ElasticSearch Sink都支持Serializer功能。 Serializer的作用是sink写入的时候，做一些处理。 HDFS Sink的Serializer在Flume Sink组件分析中一文中，分析过了HDFS写文件的时候使用BucketWriter写数据，BucketWriter内部使用HDFSWriter属性写数据。HDFSWriter是一个处理hdfs文件的接口。 public interface HDFSWriter extends Configurable { public void open(String filePath) throws IOException; public void open(String filePath, CompressionCodec codec, CompressionType cType) throws IOException; public void append(Event e) throws IOException; public void sync() throws IOException; public void close() throws IOException; public boolean isUnderReplicated(); } HDFSWriter的结构如下： hdfs sink的fileType配置如下： HDFSDataStream对应DataStream类型，HDFSCompressedDataStream对应CompressedStream，HDFSSequenceFile对应SequenceFile。 以DataStream为例，HDFSDataStream的append方法如下： @Override public void append(Event e) throws IOException { serializer.write(e); } 这个serializer是HDFSDataStream的属性。是EventSerializer接口类型的属性。HDFSDataStream的append很简单，直接调用serializer的writer方法。 HDFS Sink的Serializer都需要实现EventSerializer接口： public interface EventSerializer { public static String CTX_PREFIX = &quot;serializer.&quot;; public void afterCreate() throws IOException; public void afterReopen() throws IOException; public void write(Event event) throws IOException; public void flush() throws IOException; public void beforeClose() throws IOException; public boolean supportsReopen(); public interface Builder { public EventSerializer build(Context context, OutputStream out); } } HDFS Sink默认的serializer是BodyTextEventSerializer类，不配置的话也是使用这个Serializer。 BodyTextEventSerializer的writer方法： @Override public void write(Event e) throws IOException { out.write(e.getBody()); if (appendNewline) { out.write(&apos;\\n&apos;); } } 这就是为什么hdfs sink写数据的时候写完会自动换行的原因。 当然，我们可以定义自定义的Serializer来满足自身的要求。 HBase Sink的SerializerHBase Sink的Serializer都需要实现HbaseEventSerializer接口。 public interface HbaseEventSerializer extends Configurable, ConfigurableComponent { public void initialize(Event event, byte[] columnFamily); public List&lt;Row&gt; getActions(); public List&lt;Increment&gt; getIncrements(); public void close(); } HBaseSink的process方法关键代码： for (; i &lt; batchSize; i++) { Event event = channel.take(); if (event == null) { if (i == 0) { status = Status.BACKOFF; sinkCounter.incrementBatchEmptyCount(); } else { sinkCounter.incrementBatchUnderflowCount(); } break; } else { serializer.initialize(event, columnFamily); actions.addAll(serializer.getActions()); incs.addAll(serializer.getIncrements()); } } actions和incs都加入了serializer里的actions和increments。之后会commit这里的actions和increments数据。 HBase默认的Serializer是org.apache.flume.sink.hbase.SimpleHbaseEventSerializer。 我们也可以根据需求定义自定义的HbaseEventSerializer，需要注意的是getActions和getIncrements方法。 HBase Sink会加入这2个方法的返回值，并写入到HBase。 Elasticsearch Sink的SerializerElasticsearch Sink的Serializer都需要实现ElasticSearchEventSerializer接口。 public interface ElasticSearchEventSerializer extends Configurable, ConfigurableComponent { public static final Charset charset = Charset.defaultCharset(); abstract BytesStream getContentBuilder(Event event) throws IOException; } 默认的Serializer是org.apache.flume.sink.elasticsearch.ElasticSearchLogStashEventSerializer。 同样，我们也可以根据需求定义自定义的ElasticSearchEventSerialize，就不分析了。 SinkGroup这个功能暂时还没用到，不过以后可能会用到。 Sink Group的作用是把多个Sink合并成一个。这样的话Sink处理器会根据配置的类型来决定如何使用Sink。比如可以使用load balance，failover策略，或者可以使用自定义的策略来处理。 官方文档Sink Group已经写的很清楚了。 其它目前还正在用Flume开发一些功能，后续可能会使用一些新的功能，到时候回头更新这篇文章。","raw":null,"content":null,"categories":[{"name":"flume","slug":"flume","permalink":"http://fangjian0423.github.io/categories/flume/"}],"tags":[{"name":"big data","slug":"big-data","permalink":"http://fangjian0423.github.io/tags/big-data/"},{"name":"flume","slug":"flume","permalink":"http://fangjian0423.github.io/tags/flume/"}]},{"title":"Elasticsearch入门","slug":"elasticsearch-tutorials","date":"2015-07-11T19:06:59.000Z","updated":"2016-05-10T14:59:25.000Z","comments":true,"path":"2015/07/12/elasticsearch-tutorials/","link":"","permalink":"http://fangjian0423.github.io/2015/07/12/elasticsearch-tutorials/","excerpt":"","text":"之前搭建logstash的时候使用过elasticsearch。 刚好最近在公司也用到了es，写篇水文记录一下也当做笔记吧。 Elasticsearch是一个基于Lucene的搜索服务器。它提供了一个分布式多用户能力的全文搜索引擎，使用RESTful web暴露接口。 它有许多特性，比如以下几个属性： 1.实时数据2.实时分析3.分布式设计4.高可用性5.全文搜索6.面向文档 索引索引Index是es中的一个存储数据的地方。相当于关系型数据库中的数据库。 创建一个员工索引的例子如下，创建索引还有很多选项，就不一一说明了： POST $HOST/employee { &quot;mappings&quot;: { &quot;employee&quot;: { &quot;_ttl&quot;: { &quot;enabled&quot;: true, &quot;default&quot;: &quot;5d&quot; }, &quot;_timestamp&quot;: { &quot;enabled&quot;: true, &quot;format&quot;: &quot;yyyy-MM-dd HH:mm:ss&quot; }, &quot;properties&quot;: { &quot;name&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;store&quot;: &quot;no&quot;, &quot;index&quot;: &quot;not_analyzed&quot;, &quot;index_options&quot;: &quot;docs&quot; }, &quot;birth_date&quot;: { &quot;type&quot;: &quot;date&quot;, &quot;store&quot;: &quot;no&quot;, &quot;index&quot;: &quot;not_analyzed&quot;, &quot;index_options&quot;: &quot;docs&quot;, &quot;format&quot;: &quot;yyyy-MM-dd HH:mm:ss&quot; }, &quot;age&quot;: { &quot;type&quot;: &quot;date&quot;, &quot;store&quot;: &quot;no&quot;, &quot;index&quot;: &quot;not_analyzed&quot;, &quot;index_options&quot;: &quot;docs&quot;, &quot;format&quot;: &quot;yyyy-MM-dd HH:mm:ss&quot; } } } } } 索引创建完之后还可以修改(添加一个hobby属性)，需要注意的是，修改mapping不允许修改属性的类型： PUT $HOST/employee/employee/_mapping { &quot;employee&quot;: { &quot;properties&quot;: { &quot;name&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;store&quot;: &quot;no&quot;, &quot;index&quot;: &quot;not_analyzed&quot;, &quot;index_options&quot;: &quot;docs&quot; }, &quot;birth_date&quot;: { &quot;type&quot;: &quot;date&quot;, &quot;store&quot;: &quot;no&quot;, &quot;index&quot;: &quot;not_analyzed&quot;, &quot;index_options&quot;: &quot;docs&quot;, &quot;format&quot;: &quot;yyyy-MM-dd HH:mm:ss&quot; }, &quot;age&quot;: { &quot;type&quot;: &quot;date&quot;, &quot;store&quot;: &quot;no&quot;, &quot;index&quot;: &quot;not_analyzed&quot;, &quot;index_options&quot;: &quot;docs&quot;, &quot;format&quot;: &quot;yyyy-MM-dd HH:mm:ss&quot; }, &quot;hobby&quot; : { &quot;type&quot; : &quot;string&quot;, &quot;index_options&quot;: &quot;docs&quot; } } } } 文档es存储的数据叫做文档，文档存储在索引中。 每个文档都有4个元数据，分别是_id, _type，_index和_version。 _id代表文档的唯一标识符。 _type表示文档代表的对象种类。 _index表示文档存储在哪个索引。 _version表示文档的版本，文档被修改过一次，_version就会+1。 在员工索引中创建文档： POST $HOST/employee/employee { &quot;name&quot;: &quot;format&quot;, &quot;age&quot;: 100, &quot;birth_date&quot;: &quot;1900-01-01 00:00:00&quot; } 返回： { &quot;_index&quot;: &quot;employee&quot;, &quot;_type&quot;: &quot;employee&quot;, &quot;_id&quot;: &quot;AU5-epuwslU6QVfs_UoX&quot;, &quot;_version&quot;: 1, &quot;created&quot;: true } 修改文档： POST $HOST/employee/employee/AU5-epuwslU6QVfs_UoX { &quot;name&quot;: &quot;format&quot;, &quot;age&quot;: 200, &quot;birth_date&quot;: &quot;1900-01-01 00:00:00&quot; } 返回： { &quot;_index&quot;: &quot;employee&quot;, &quot;_type&quot;: &quot;employee&quot;, &quot;_id&quot;: &quot;AU5-epuwslU6QVfs_UoX&quot;, &quot;_version&quot;: 2, &quot;created&quot;: false } 删除文档： DELETE $HOST/employee/employee/AU5-epuwslU6QVfs_UoX 返回： { &quot;found&quot;: true, &quot;_index&quot;: &quot;employee&quot;, &quot;_type&quot;: &quot;employee&quot;, &quot;_id&quot;: &quot;AU5-epuwslU6QVfs_UoX&quot;, &quot;_version&quot;: 3 } 总结写了篇水文记录一下es，es还有很多很强大的功能，比如一些query，filter，aggregations等。官方文档上已经写的非常清楚了。这里就不讲了。 - -||","raw":null,"content":null,"categories":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://fangjian0423.github.io/categories/elasticsearch/"}],"tags":[{"name":"big data","slug":"big-data","permalink":"http://fangjian0423.github.io/tags/big-data/"},{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://fangjian0423.github.io/tags/elasticsearch/"}]},{"title":"Flume运行过程源码分析","slug":"flume-run","date":"2015-07-07T15:23:23.000Z","updated":"2016-05-10T14:59:25.000Z","comments":true,"path":"2015/07/07/flume-run/","link":"","permalink":"http://fangjian0423.github.io/2015/07/07/flume-run/","excerpt":"","text":"Flume运行过程分析入口ApplicationFlume运行的主类是org.apache.flume.node.Application的main方法。 命令行args参数如下： --conf-file YourConfigFile --name AgentName main方法里会先判断参数中是否有z或者zkConnString，如果配置了这两个参数中其中一个的话，那么会使用PollingZooKeeperConfigurationProvider或StaticZooKeeperConfigurationProvider配置。否则使用PollingPropertiesFileConfigurationProvider或PropertiesFileConfigurationProvider配置。 一般我们只传–conf-file和–name，那么就会使用PollingPropertiesFileConfigurationProvider进行配置。 // EventBus，监听者设计模式 EventBus eventBus = new EventBus(agentName + &quot;-event-bus&quot;); // 构造PollingPropertiesFileConfigurationProvider，会处理配置文件相关的信息 PollingPropertiesFileConfigurationProvider configurationProvider = new PollingPropertiesFileConfigurationProvider( agentName, configurationFile, eventBus, 30); components.add(configurationProvider); application = new Application(components); // EventBus注册application eventBus.register(application); ... application.start(); Application内部有个handleConfigurationEvent方法，使用Subscribe注解，EventBus中被使用到： @Subscribe public synchronized void handleConfigurationEvent(MaterializedConfiguration conf) { stopAllComponents(); startAllComponents(conf); } 接下来看Application的start方法： public synchronized void start() { // 使用生命周期组件管理器进行管理，这里其实只有1个组件，那就是PollingPropertiesFileConfigurationProvider for(LifecycleAware component : components) { // 管理器管理各个组件的时候会传入2个参数，分别是管理策略(自动重启策略)和所需状态(START状态) supervisor.supervise(component, new SupervisorPolicy.AlwaysRestartPolicy(), LifecycleState.START); } } LifecycleSupervisor生命周期管理者的supervise方法的主要代码： // Supervisoree是一个带有status和policy这2个属性的封装类。这2个属性分别表示状态和管理策略。其中管理策略只有2种策略，分别是AlwaysRestartPolicy(自动重启策略)和OnceOnlyPolicy(只启动一次策略)；状态表示这个组件状态，状态Status中包括首次发生，最后发生，失败次数，目标状态等属性。 Supervisoree process = new Supervisoree(); process.status = new Status(); process.policy = policy; // 所需状态初始化成START process.status.desiredState = desiredState; process.status.error = false; // 起一个监控线程。 将Supervisoree和组件参数传入 MonitorRunnable monitorRunnable = new MonitorRunnable(); monitorRunnable.lifecycleAware = lifecycleAware; monitorRunnable.supervisoree = process; monitorRunnable.monitorService = monitorService; supervisedProcesses.put(lifecycleAware, process); ScheduledFuture&lt;?&gt; future = monitorService.scheduleWithFixedDelay( monitorRunnable, 0, 3, TimeUnit.SECONDS); monitorFutures.put(lifecycleAware, future); 监控线程MonitorRunnable的run方法主要代码： switch (supervisoree.status.desiredState) { // 所需状态之前已经初始化成START状态，那么会执行组件的start方法。这里的组件是之前分析的PollingPropertiesFileConfigurationProvider case START: try { lifecycleAware.start(); } catch (Throwable e) { logger.error(&quot;Unable to start &quot; + lifecycleAware + &quot; - Exception follows.&quot;, e); if (e instanceof Error) { // This component can never recover, shut it down. supervisoree.status.desiredState = LifecycleState.STOP; try { lifecycleAware.stop(); logger.warn(&quot;Component {} stopped, since it could not be&quot; + &quot;successfully started due to missing dependencies&quot;, lifecycleAware); } catch (Throwable e1) { logger.error(&quot;Unsuccessful attempt to &quot; + &quot;shutdown component: {} due to missing dependencies.&quot; + &quot; Please shutdown the agent&quot; + &quot;or disable this component, or the agent will be&quot; + &quot;in an undefined state.&quot;, e1); supervisoree.status.error = true; if (e1 instanceof Error) { throw (Error) e1; } // Set the state to stop, so that the conf poller can // proceed. } } supervisoree.status.failures++; } break; case STOP: try { lifecycleAware.stop(); } catch (Throwable e) { logger.error(&quot;Unable to stop &quot; + lifecycleAware + &quot; - Exception follows.&quot;, e); if (e instanceof Error) { throw (Error) e; } supervisoree.status.failures++; } break; default: logger.warn(&quot;I refuse to acknowledge {} as a desired state&quot;, supervisoree.status.desiredState); } PollingPropertiesFileConfigurationProvider的start方法： public void start() { LOGGER.info(&quot;Configuration provider starting&quot;); Preconditions.checkState(file != null, &quot;The parameter file must not be null&quot;); // 初始化线程池 executorService = Executors.newSingleThreadScheduledExecutor( new ThreadFactoryBuilder().setNameFormat(&quot;conf-file-poller-%d&quot;) .build()); // 起一个文件观察线程 FileWatcherRunnable fileWatcherRunnable = new FileWatcherRunnable(file, counterGroup); executorService.scheduleWithFixedDelay(fileWatcherRunnable, 0, interval, TimeUnit.SECONDS); // 初始化组件的状态为START lifecycleState = LifecycleState.START; LOGGER.debug(&quot;Configuration provider started&quot;); } 文件观察线程FileWatcherRunnable的run方法： public void run() { LOGGER.debug(&quot;Checking file:{} for changes&quot;, file); counterGroup.incrementAndGet(&quot;file.checks&quot;); // 得到文件的上次修改时间 long lastModified = file.lastModified(); // 如果修改了文件，那么会执行以下代码。首次发生的时候lastChange为0，所以肯定会执行一次。以后只要配置改了才会再次执行 if (lastModified &gt; lastChange) { LOGGER.info(&quot;Reloading configuration file:{}&quot;, file); counterGroup.incrementAndGet(&quot;file.loads&quot;); lastChange = lastModified; try { // eventBus属性之前在Application中分析过，而且它注册了application实例 // Application中有个handleConfigurationEvent方法，eventBus是个观察者设计模式，所以会钓鱼handleConfigurationEvent这个方法 // getConfiguration方法会parse配置文件中的配置信息 eventBus.post(getConfiguration()); } catch (Exception e) { LOGGER.error(&quot;Failed to load configuration data. Exception follows.&quot;, e); } catch (NoClassDefFoundError e) { LOGGER.error(&quot;Failed to start agent because dependencies were not &quot; + &quot;found in classpath. Error follows.&quot;, e); } catch (Throwable t) { // caught because the caller does not handle or log Throwables LOGGER.error(&quot;Unhandled error&quot;, t); } } } getConfiguration方法是在PollingPropertiesFileConfigurationProvider的父类AbstractConfigurationProvider中定义的： public MaterializedConfiguration getConfiguration() { MaterializedConfiguration conf = new SimpleMaterializedConfiguration(); FlumeConfiguration fconfig = getFlumeConfiguration(); AgentConfiguration agentConf = fconfig.getConfigurationFor(getAgentName()); if (agentConf != null) { // 构造Channel Map&lt;String, ChannelComponent&gt; channelComponentMap = Maps.newHashMap(); // 构造Source Map&lt;String, SourceRunner&gt; sourceRunnerMap = Maps.newHashMap(); // 构造Sink Map&lt;String, SinkRunner&gt; sinkRunnerMap = Maps.newHashMap(); try { loadChannels(agentConf, channelComponentMap); loadSources(agentConf, channelComponentMap, sourceRunnerMap); loadSinks(agentConf, channelComponentMap, sinkRunnerMap); Set&lt;String&gt; channelNames = new HashSet&lt;String&gt;(channelComponentMap.keySet()); for(String channelName : channelNames) { ChannelComponent channelComponent = channelComponentMap. get(channelName); if(channelComponent.components.isEmpty()) { LOGGER.warn(String.format(&quot;Channel %s has no components connected&quot; + &quot; and has been removed.&quot;, channelName)); channelComponentMap.remove(channelName); Map&lt;String, Channel&gt; nameChannelMap = channelCache. get(channelComponent.channel.getClass()); if(nameChannelMap != null) { nameChannelMap.remove(channelName); } } else { LOGGER.info(String.format(&quot;Channel %s connected to %s&quot;, channelName, channelComponent.components.toString())); conf.addChannel(channelName, channelComponent.channel); } } for(Map.Entry&lt;String, SourceRunner&gt; entry : sourceRunnerMap.entrySet()) { conf.addSourceRunner(entry.getKey(), entry.getValue()); } for(Map.Entry&lt;String, SinkRunner&gt; entry : sinkRunnerMap.entrySet()) { conf.addSinkRunner(entry.getKey(), entry.getValue()); } } catch (InstantiationException ex) { LOGGER.error(&quot;Failed to instantiate component&quot;, ex); } finally { channelComponentMap.clear(); sourceRunnerMap.clear(); sinkRunnerMap.clear(); } } else { LOGGER.warn(&quot;No configuration found for this host:{}&quot;, getAgentName()); } return conf; } 当所有的组件，Source，Channel，Sink加载完之后。会调用handleConfigurationEvent方法： @Subscribe public synchronized void handleConfigurationEvent(MaterializedConfiguration conf) { // 先关闭之前启动的所有Source，Channel，Sink组件(首次运行的时候并没有任何开启的组件) stopAllComponents(); // 再重新开启这些组件 startAllComponents(conf); } 看下startAllComponents方法： // 启动各个组件的时候同样适用supervisor的supervise方法。然后启动MonitorRunnable线程调用各个组件的start方法。 private void startAllComponents(MaterializedConfiguration materializedConfiguration) { logger.info(&quot;Starting new configuration:{}&quot;, materializedConfiguration); this.materializedConfiguration = materializedConfiguration; for (Entry&lt;String, Channel&gt; entry : materializedConfiguration.getChannels().entrySet()) { try{ logger.info(&quot;Starting Channel &quot; + entry.getKey()); supervisor.supervise(entry.getValue(), new SupervisorPolicy.AlwaysRestartPolicy(), LifecycleState.START); } catch (Exception e){ logger.error(&quot;Error while starting {}&quot;, entry.getValue(), e); } } /* * Wait for all channels to start. */ for(Channel ch: materializedConfiguration.getChannels().values()){ while(ch.getLifecycleState() != LifecycleState.START &amp;&amp; !supervisor.isComponentInErrorState(ch)){ try { logger.info(&quot;Waiting for channel: &quot; + ch.getName() + &quot; to start. Sleeping for 500 ms&quot;); Thread.sleep(500); } catch (InterruptedException e) { logger.error(&quot;Interrupted while waiting for channel to start.&quot;, e); Throwables.propagate(e); } } } for (Entry&lt;String, SinkRunner&gt; entry : materializedConfiguration.getSinkRunners() .entrySet()) { try{ logger.info(&quot;Starting Sink &quot; + entry.getKey()); supervisor.supervise(entry.getValue(), new SupervisorPolicy.AlwaysRestartPolicy(), LifecycleState.START); } catch (Exception e) { logger.error(&quot;Error while starting {}&quot;, entry.getValue(), e); } } for (Entry&lt;String, SourceRunner&gt; entry : materializedConfiguration .getSourceRunners().entrySet()) { try{ logger.info(&quot;Starting Source &quot; + entry.getKey()); supervisor.supervise(entry.getValue(), new SupervisorPolicy.AlwaysRestartPolicy(), LifecycleState.START); } catch (Exception e) { logger.error(&quot;Error while starting {}&quot;, entry.getValue(), e); } } this.loadMonitoring(); } Flume启动过程总结首先通过Application类加载配置文件，加载后调用Application的start方法。 Application的start方法内部会针对PollingPropertiesFileConfigurationProvider组件适用组件管理者管理这个组件。也就是使用LifecycleSupervisor的supervise方法。 supervise方法内部使用MonitorRunnable监控线程。 监控线程内部会调用组件的start方法。 即PollingPropertiesFileConfigurationProvider的start方法。 PollingPropertiesFileConfigurationProvider的start方法会使用FileWatcherRunnable文件查看进程判断配置文件是否已经修改，修改的话重新加载配置文件信息，然后通过EventBus调用Application的handleConfigurationEvent方法关闭目前正在启动的组件，关闭之后重新开启组件。 各个新开启的组件会做类似的工作，使用LifecycleSupervisor的supervise方法，也就是起各个MonitorRunnable监控线程启动各个组件的start方法。 Source组件的构造过程直接看AbstractConfigurationProvider的loadSources方法： private void loadSources(AgentConfiguration agentConf, Map&lt;String, ChannelComponent&gt; channelComponentMap, Map&lt;String, SourceRunner&gt; sourceRunnerMap) throws InstantiationException { Map&lt;String, Context&gt; sourceContexts = agentConf.getSourceContext(); for (String sourceName : sourceNames) { Context context = sourceContexts.get(sourceName); if(context != null){ // 使用sourceFactory构造Source Source source = sourceFactory.create(sourceName, context.getString(BasicConfigurationConstants.CONFIG_TYPE)); try { Configurables.configure(source, context); List&lt;Channel&gt; sourceChannels = new ArrayList&lt;Channel&gt;(); String[] channelNames = context.getString( BasicConfigurationConstants.CONFIG_CHANNELS).split(&quot;\\\\s+&quot;); for (String chName : channelNames) { ChannelComponent channelComponent = channelComponentMap.get(chName); if(channelComponent != null) { sourceChannels.add(channelComponent.channel); } } if(sourceChannels.isEmpty()) { String msg = String.format(&quot;Source %s is not connected to a &quot; + &quot;channel&quot;, sourceName); throw new IllegalStateException(msg); } Map&lt;String, String&gt; selectorConfig = context.getSubProperties( BasicConfigurationConstants.CONFIG_SOURCE_CHANNELSELECTOR_PREFIX); ChannelSelector selector = ChannelSelectorFactory.create( sourceChannels, selectorConfig); ChannelProcessor channelProcessor = new ChannelProcessor(selector); Configurables.configure(channelProcessor, context); source.setChannelProcessor(channelProcessor); sourceRunnerMap.put(sourceName, SourceRunner.forSource(source)); // source关联Channel for(Channel channel : sourceChannels) { ChannelComponent channelComponent = Preconditions. checkNotNull(channelComponentMap.get(channel.getName()), String.format(&quot;Channel %s&quot;, channel.getName())); channelComponent.components.add(sourceName); } } catch (Exception e) { String msg = String.format(&quot;Source %s has been removed due to an &quot; + &quot;error during configuration&quot;, sourceName); LOGGER.error(msg, e); } } } }","raw":null,"content":null,"categories":[{"name":"flume","slug":"flume","permalink":"http://fangjian0423.github.io/categories/flume/"}],"tags":[{"name":"big data","slug":"big-data","permalink":"http://fangjian0423.github.io/tags/big-data/"},{"name":"flume","slug":"flume","permalink":"http://fangjian0423.github.io/tags/flume/"}]},{"title":"Flume Sink组件分析","slug":"flume-sink","date":"2015-06-22T17:23:23.000Z","updated":"2016-05-10T14:59:25.000Z","comments":true,"path":"2015/06/23/flume-sink/","link":"","permalink":"http://fangjian0423.github.io/2015/06/23/flume-sink/","excerpt":"","text":"Source和Channel组件已经分析过，接下来看Sink组件。 Flume内置了很多Sink，比如HDFS Sink, Hive Sink, File Roll Sink, HBase Sink, ElasticSearch Sink等。 Sink接口public interface Sink extends LifecycleAware, NamedComponent { public void setChannel(Channel channel); public Channel getChannel(); public Status process() throws EventDeliveryException; public static enum Status { READY, BACKOFF } } 沒啥好讲的，跟之前的都类似。 Sink实现类AbstractSink： abstract public class AbstractSink implements Sink, LifecycleAware 跟AbstractSource类似。 Flume内置的Sink例子HDFSEventSink1个简单的hdfs sink配置： a1.channels = c1 a1.sinks = k1 a1.sinks.k1.type = hdfs a1.sinks.k1.channel = c1 a1.sinks.k1.hdfs.path = /flume/events/%y-%m-%d/%H%M/%S a1.sinks.k1.hdfs.filePrefix = events- a1.sinks.k1.hdfs.round = true a1.sinks.k1.hdfs.roundValue = 10 a1.sinks.k1.hdfs.roundUnit = minute HDFSEventSink中的process(Sink接口提供的)方法记录着如何写入hdfs文件： public Status process() throws EventDeliveryException { // 得到Channel Channel channel = getChannel(); // 得到Channel里的Transaction，接下来事件的提交，回滚都基于这个Transaction Transaction transaction = channel.getTransaction(); List&lt;BucketWriter&gt; writers = Lists.newArrayList(); // 事务开始，一般的Transaction实现类不会覆盖这个方法，除非有特殊要求，begin方法默认的实现不做任何事 transaction.begin(); try { int txnEventCount = 0; // 每次操作都处理batchSize个事件，batchSize可配置 for (txnEventCount = 0; txnEventCount &lt; batchSize; txnEventCount++) { // channel的take方法内部会使用Transaction的take方法，Transaction回滚后这些take出来的事件全部都会回滚到Channel里 Event event = channel.take(); if (event == null) { break; } // reconstruct the path name by substituting place holders String realPath = BucketPath.escapeString(filePath, event.getHeaders(), timeZone, needRounding, roundUnit, roundValue, useLocalTime); String realName = BucketPath.escapeString(fileName, event.getHeaders(), timeZone, needRounding, roundUnit, roundValue, useLocalTime); String lookupPath = realPath + DIRECTORY_DELIMITER + realName; BucketWriter bucketWriter; HDFSWriter hdfsWriter = null; // Callback to remove the reference to the bucket writer from the // sfWriters map so that all buffers used by the HDFS file // handles are garbage collected. WriterCallback closeCallback = new WriterCallback() { @Override public void run(String bucketPath) { LOG.info(&quot;Writer callback called.&quot;); synchronized (sfWritersLock) { sfWriters.remove(bucketPath); } } }; synchronized (sfWritersLock) { bucketWriter = sfWriters.get(lookupPath); // we haven&apos;t seen this file yet, so open it and cache the handle if (bucketWriter == null) { hdfsWriter = writerFactory.getWriter(fileType); bucketWriter = initializeBucketWriter(realPath, realName, lookupPath, hdfsWriter, closeCallback); sfWriters.put(lookupPath, bucketWriter); } } // track the buckets getting written in this transaction if (!writers.contains(bucketWriter)) { writers.add(bucketWriter); } // Write the data to HDFS try { bucketWriter.append(event); } catch (BucketClosedException ex) { LOG.info(&quot;Bucket was closed while trying to append, &quot; + &quot;reinitializing bucket and writing event.&quot;); hdfsWriter = writerFactory.getWriter(fileType); bucketWriter = initializeBucketWriter(realPath, realName, lookupPath, hdfsWriter, closeCallback); synchronized (sfWritersLock) { sfWriters.put(lookupPath, bucketWriter); } bucketWriter.append(event); } } if (txnEventCount == 0) { sinkCounter.incrementBatchEmptyCount(); } else if (txnEventCount == batchSize) { sinkCounter.incrementBatchCompleteCount(); } else { sinkCounter.incrementBatchUnderflowCount(); } // flush all pending buckets before committing the transaction for (BucketWriter bucketWriter : writers) { bucketWriter.flush(); } // 写完数据无误后，commit，清空Transaction里的数据 transaction.commit(); if (txnEventCount &lt; 1) { return Status.BACKOFF; } else { sinkCounter.addToEventDrainSuccessCount(txnEventCount); return Status.READY; } } catch (IOException eIO) { // 发送异常回滚 transaction.rollback(); LOG.warn(&quot;HDFS IO error&quot;, eIO); return Status.BACKOFF; } catch (Throwable th) { // 发送异常回滚 transaction.rollback(); LOG.error(&quot;process failed&quot;, th); if (th instanceof Error) { throw (Error) th; } else { throw new EventDeliveryException(th); } } finally { // 事务结束，一般的Transaction实现类不会覆盖这个方法，除非有特殊要求，close方法默认的实现不做任何事 transaction.close(); } } 下面我们重点分析一下HDFS Sink如何写入数据到hdfs文件。 分析之前，有几个配置跟是否创建hdfs新文件有关，当达到这些配置的条件后，sink会关闭文件，然后重新创建1个新文件重复执行 hdfs.rollInterval -&gt; 每隔多少秒会关闭文件。默认30秒 hdfs.rollSize -&gt; 文件大小到达一定量后，会关闭文件。单位bytes。默认1024，如果是0的话表示跟文件大小无关 hdfs.batchSize -&gt; 批次数，Sink每次处理都会处理batchSize个事件, 不会创建新文件。默认100个 hdfs.rollCount -&gt; 事件处理了rollCount个后，会关闭文件。默认10个，如果是0的话表示跟事件个数无关 hdfs.minBlockReplicas -&gt; hadoop中的dfs.replication配置属性，表示复制块的个数，默认会根据hadoop的配置 HDFS Sink每个根据batchSize，遍历channel中的事件，针对每个Event的生成地址构造BucketWriter。然后在sfWriters这个Map中以文件的路径为key，BucketWriter为value进行存储。 接下来使用BucketWriter的append方法，当batchSize个事件遍历完成，调用BucketWriter的flush方法。 bucketWriter.append(event); for (BucketWriter bucketWriter : writers) { bucketWriter.flush(); } BucketWriter的append方法部分代码如下： // 打开文件 if (!isOpen) { if (closed) { throw new BucketClosedException(&quot;This bucket writer was closed and &quot; + &quot;this handle is thus no longer valid&quot;); } open(); } // 是否需要创建新文件 if (shouldRotate()) { boolean doRotate = true; if (isUnderReplicated) { if (maxConsecUnderReplRotations &gt; 0 &amp;&amp; consecutiveUnderReplRotateCount &gt;= maxConsecUnderReplRotations) { doRotate = false; if (consecutiveUnderReplRotateCount == maxConsecUnderReplRotations) { LOG.error(&quot;Hit max consecutive under-replication rotations ({}); &quot; + &quot;will not continue rolling files under this path due to &quot; + &quot;under-replication&quot;, maxConsecUnderReplRotations); } } else { LOG.warn(&quot;Block Under-replication detected. Rotating file.&quot;); } consecutiveUnderReplRotateCount++; } else { consecutiveUnderReplRotateCount = 0; } if (doRotate) { close(); open(); } } try { sinkCounter.incrementEventDrainAttemptCount(); // 写hdfs文件 callWithTimeout(new CallRunner&lt;Void&gt;() { @Override public Void call() throws Exception { writer.append(event); // could block return null; } }); } catch (IOException e) { LOG.warn(&quot;Caught IOException writing to HDFSWriter ({}). Closing file (&quot; + bucketPath + &quot;) and rethrowing exception.&quot;, e.getMessage()); try { close(true); } catch (IOException e2) { LOG.warn(&quot;Caught IOException while closing file (&quot; + bucketPath + &quot;). Exception follows.&quot;, e2); } throw e; } // 更新一些属性 processSize += event.getBody().length; eventCounter++; batchCounter++; // 写入的事件达到批次数，flush if (batchCounter == batchSize) { flush(); } shouldRotate方法表示是否需要创建新文件： private boolean shouldRotate() { boolean doRotate = false; // 先判断HDFS是否正在复制块，优先级最高，也就是配置文件的minBlockReplicas if (writer.isUnderReplicated()) { this.isUnderReplicated = true; doRotate = true; } else { this.isUnderReplicated = false; } // 配置文件中的rollCount，也就是事件个数 if ((rollCount &gt; 0) &amp;&amp; (rollCount &lt;= eventCounter)) { LOG.debug(&quot;rolling: rollCount: {}, events: {}&quot;, rollCount, eventCounter); doRotate = true; } // 配置文件中的rollSize，也就文件大小 if ((rollSize &gt; 0) &amp;&amp; (rollSize &lt;= processSize)) { LOG.debug(&quot;rolling: rollSize: {}, bytes: {}&quot;, rollSize, processSize); doRotate = true; } return doRotate; } 有的同学可能因为没有配置minBlockReplicas，而配置了其他属性，所以hdfs还是会生成很多文件，这是因为minBlockReplicas的优先级最高，如果当前正在复制块，其他所有的条件都会被无视。 flush方法： public synchronized void flush() throws IOException, InterruptedException { checkAndThrowInterruptedException(); if (!isBatchComplete()) { doFlush(); if(idleTimeout &gt; 0) { // if the future exists and couldn&apos;t be cancelled, that would mean it has already run // or been cancelled if(idleFuture == null || idleFuture.cancel(false)) { Callable&lt;Void&gt; idleAction = new Callable&lt;Void&gt;() { public Void call() throws Exception { LOG.info(&quot;Closing idle bucketWriter {} at {}&quot;, bucketPath, System.currentTimeMillis()); if (isOpen) { close(true); } return null; } }; idleFuture = timedRollerPool.schedule(idleAction, idleTimeout, TimeUnit.SECONDS); } } } } 总结: HDFSSink里的hdfs.rollSize，hdfs.rollCount，hdfs.minBlockReplicas，hdfs.rollInterval,hdfs.batchSize这些配置都是在BucketWriter中才会使用。 hdfs.rollSize, hdfs.rollCount和hdfs.minBlockReplicas, hdfs.rollInterval这4个属性决定是否创建新文件。 hdfs.batchSize决定是否flush文件数据。 由于HDFS Sink中每次最多只有batchSize个事件，因此BucketWriter会事件个数达到了batchSize后直接flush数据即可。","raw":null,"content":null,"categories":[{"name":"flume","slug":"flume","permalink":"http://fangjian0423.github.io/categories/flume/"}],"tags":[{"name":"big data","slug":"big-data","permalink":"http://fangjian0423.github.io/tags/big-data/"},{"name":"flume","slug":"flume","permalink":"http://fangjian0423.github.io/tags/flume/"}]},{"title":"Flume Channel组件分析","slug":"flume-channel","date":"2015-06-22T15:23:23.000Z","updated":"2016-05-10T14:59:25.000Z","comments":true,"path":"2015/06/22/flume-channel/","link":"","permalink":"http://fangjian0423.github.io/2015/06/22/flume-channel/","excerpt":"","text":"Source组件已经分析过，接下来看Channel组件。 Flume内置了很多Channel，比如Memory Channel, JDBC Channel, Kafka Channel, File Channel等。 Channel接口Channel接口定义： public interface Channel extends LifecycleAware, NamedComponent { public void put(Event event) throws ChannelException; public Event take() throws ChannelException; public Transaction getTransaction(); } LifecycleAware接口和NamedComponent接口之前在分析Source的时候已经说明过。 Channel接口有3个方法，分别是put，take和getTransaction。 Channel是存储Source收集过来的数据的，所以提供put(存储Source的Event)和take(交付给Sink)方法，它也支持事务。 AbstractChannel抽象类是Channel接口的实现类，作用跟AbstractSource是Source的实现类类似。 BasicChannelSemantics是AbstractChannel的子类，是Channel的基础实现类，内部使用ThreadLocal完成事件的处理和事务功能，实现了getTransaction方法，在这个方法内部调用createTransaction得到事务对象，createTransaction也被抽象成了一个抽象方法。 BasicChannelSemantics实现了Channel接口的put和take方法，使用Transaction接口的put和take方法。 @Override public void put(Event event) throws ChannelException { BasicTransactionSemantics transaction = currentTransaction.get(); Preconditions.checkState(transaction != null, &quot;No transaction exists for this thread&quot;); transaction.put(event); } @Override public Event take() throws ChannelException { BasicTransactionSemantics transaction = currentTransaction.get(); Preconditions.checkState(transaction != null, &quot;No transaction exists for this thread&quot;); return transaction.take(); } currentTransaction是个ThreadLocal： private ThreadLocal&lt;BasicTransactionSemantics&gt; currentTransaction = new ThreadLocal&lt;BasicTransactionSemantics&gt;(); currentTransaction的初始化： @Override public Transaction getTransaction() { if (!initialized) { synchronized (this) { if (!initialized) { initialize(); initialized = true; } } } BasicTransactionSemantics transaction = currentTransaction.get(); if (transaction == null || transaction.getState().equals( BasicTransactionSemantics.State.CLOSED)) { transaction = createTransaction(); // 抽象方法，子类实现 currentTransaction.set(transaction); } return transaction; } 这样，Channel的put和take操作就由抽象类createTransaction中得到的Transaction实现。因此，Channel对事件的各个操作由具体的Transaction实现。 一般普通的Channel继承这个BasicChannelSemantics类即可。 Transaction接口由于Channel内部使用了事务特性，因此有必要介绍一个Flume事务相关的结构。 Transaction接口定义： public interface Transaction { public enum TransactionState {Started, Committed, RolledBack, Closed }; public void begin(); public void commit(); public void rollback(); public void close(); } 基本的事务封装类BasicTransactionSemantics，BasicTransactionSemantics实现了Transaction接口的4个方法，并引入了状态的概念，内部也有个state变量表示状态，Transaction接口的4个方法的实现会基于状态： BasicTransactionSemantics构造函数会初始化状态state为State.NEW, 实现Transaction接口的begin方法如下： public void begin() { Preconditions.checkState(Thread.currentThread().getId() == initialThreadId, &quot;begin() called from different thread than getTransaction()!&quot;); Preconditions.checkState(state.equals(State.NEW), &quot;begin() called when transaction is &quot; + state + &quot;!&quot;); try { doBegin(); } catch (InterruptedException e) { Thread.currentThread().interrupt(); throw new ChannelException(e.toString(), e); } state = State.OPEN; } begin方法需要状态为NEW才可执行。同理commit方法需要状态为OPEN，rollback需要状态为OPEN，close方法需要状态为NEW或COMPLETED。BasicTransactionSemantics又提供了put和take方法用来放事件和取事件，所以又抽象出了4个抽象方法和2个默认实现方法： protected void doBegin() throws InterruptedException {} protected abstract void doPut(Event event) throws InterruptedException; protected abstract Event doTake() throws InterruptedException; protected abstract void doCommit() throws InterruptedException; protected abstract void doRollback() throws InterruptedException; protected void doClose() {} Flume内置的ChannelMemoryChannelMemory Channel的配置如下： a1.channels = c1 a1.channels.c1.type = memory a1.channels.c1.capacity = 10000 a1.channels.c1.transactionCapacity = 10000 MemoryChannel继承BasicChannelSemantics。 MemoryChannel的configure读取配置信息，它的内部有个重要的属性如下： private LinkedBlockingDeque&lt;Event&gt; queue; // 阻塞事件队列 MemoryChannel内部有个MemoryTransaction类继承自BasicTransactionSemantics，这个事务对象就是MemoryChannel使用的事务对象。 @Override protected BasicTransactionSemantics createTransaction() { return new MemoryTransaction(transCapacity, channelCounter); } MemoryTransaction的属性如下： // 取走事件列表 private LinkedBlockingDeque&lt;Event&gt; takeList; // 放入事件列表 private LinkedBlockingDeque&lt;Event&gt; putList; private final ChannelCounter channelCounter; private int putByteCounter = 0; private int takeByteCounter = 0; 之前分析过，Channel对事件的各个操作由具体的Transaction实现，也就是由MemoryTransaction实现。 doPut方法，加入事件，doPut方法只针对Source提供过来的Event，跟Sink无关： @Override protected void doPut(Event event) throws InterruptedException { channelCounter.incrementEventPutAttemptCount(); int eventByteSize = (int)Math.ceil(estimateEventSize(event)/byteCapacitySlotSize); if (!putList.offer(event)) { // 事件存储到MemoryTransaction的加入事件列表中 throw new ChannelException( &quot;Put queue for MemoryTransaction of capacity &quot; + putList.size() + &quot; full, consider committing more frequently, &quot; + &quot;increasing capacity or increasing thread count&quot;); } putByteCounter += eventByteSize; } doTake方法，取走事件，doTake方法只针对Sink，负责给Sink传递Event数据： @Override protected Event doTake() throws InterruptedException { channelCounter.incrementEventTakeAttemptCount(); if(takeList.remainingCapacity() == 0) { throw new ChannelException(&quot;Take list for MemoryTransaction, capacity &quot; + takeList.size() + &quot; full, consider committing more frequently, &quot; + &quot;increasing capacity, or increasing thread count&quot;); } if(!queueStored.tryAcquire(keepAlive, TimeUnit.SECONDS)) { return null; } Event event; synchronized(queueLock) { event = queue.poll(); // 取出MemoryChannel中阻塞事件队列中的事件 } Preconditions.checkNotNull(event, &quot;Queue.poll returned NULL despite semaphore &quot; + &quot;signalling existence of entry&quot;); takeList.put(event); // 放到MemoryTransaction的取出事件列表中 int eventByteSize = (int)Math.ceil(estimateEventSize(event)/byteCapacitySlotSize); takeByteCounter += eventByteSize; return event; } doCommit的部分代码，提交处理的事件，doCommit只处理putList，所以提交的是Source过来的数据，并交给MemoryChannel内部的阻塞队列： @Override protected void doCommit() throws InterruptedException { int puts = putList.size(); int takes = takeList.size(); synchronized(queueLock) { if(puts &gt; 0 ) { while(!putList.isEmpty()) { // 加入事件列表中不空的话 if(!queue.offer(putList.removeFirst())) { // 在MemoryChannel里的阻塞事件队列中加入MemoryTransaction的加入事件队列，也就是说在MemoryTransaction中新加入的所有事件全部移植到MemoryChannel中的阻塞队列中 throw new RuntimeException(&quot;Queue add failed, this shouldn&apos;t be able to happen&quot;); } } } putList.clear(); takeList.clear(); } } doRollback方法，回滚处理的事件, 回滚操作只针对Sink，回滚要交给Sink的事件到MemoryChannel的阻塞队列里： @Override protected void doRollback() { int takes = takeList.size(); synchronized(queueLock) { Preconditions.checkState(queue.remainingCapacity() &gt;= takeList.size(), &quot;Not enough space in memory channel &quot; + &quot;queue to rollback takes. This should never happen, please report&quot;); while(!takeList.isEmpty()) { // MemoryTransaction处理的事件全部回滚，回到MemoryChannel的阻塞队列里。因为MemoryTransaction处理的事件数据是从MemoryChannel的阻塞队列里取走的 queue.addFirst(takeList.removeLast()); } putList.clear(); } bytesRemaining.release(putByteCounter); putByteCounter = 0; takeByteCounter = 0; queueStored.release(takes); channelCounter.setChannelSize(queue.size()); } MemoryChannel总结： MemoryChannel是个Channel，Channel的作用是收集Source发来的事件数据(put操作)和发送给Sink由Source发来的事件数据(take操作)。Channel内部对事件的处理使用Transaction完成。 put操作：先放到MemoryTransaction的putList列表中，MemoryTransaction做commit操作的时候写入到MemoryChannel的阻塞队列里。 take操作：先取出MemoryChannel中阻塞队列里的事件数据，然后放入MemoryTransaction里的takeList列表中，当需要回滚的时候会将takeList列表中的事件数据回滚到MemoryChannel中的阻塞队列里。commit操作不需要任何处理，因为事件已经从MemoryChannel中的阻塞队列里取出。 MemoryChannel有2个队列容量配置，分别是capacity和transactionCapacity。capacity是MemoryChannel里的阻塞队列的容量，transactionCapacity是MemoryTransaction里的putList和takeList列表容量。 Channel总结Channel是Flume中的桥梁，连接着Source和Sink,重要性不言而喻。Flume内置也有很多的Channel。同样地，并不是所有的Channel都能符合需求。 比如MemoryChannel在内存中处理，速度很快，但是却不能持久化，当服务器挂了，Channel中的数据会丢失，这时就需要用FileChannel或JDBCChannel，但这2种Channel速度慢。因此可以实现一个自定义的Channel，美团的 http://tech.meituan.com/mt-log-system-optimization.html 地址上就说明了一些Flume可以改进和优化的地方。","raw":null,"content":null,"categories":[{"name":"flume","slug":"flume","permalink":"http://fangjian0423.github.io/categories/flume/"}],"tags":[{"name":"big data","slug":"big-data","permalink":"http://fangjian0423.github.io/tags/big-data/"},{"name":"flume","slug":"flume","permalink":"http://fangjian0423.github.io/tags/flume/"}]},{"title":"Flume Source组件分析","slug":"flume-source","date":"2015-06-21T15:23:23.000Z","updated":"2016-05-10T14:59:25.000Z","comments":true,"path":"2015/06/21/flume-source/","link":"","permalink":"http://fangjian0423.github.io/2015/06/21/flume-source/","excerpt":"","text":"Flume的介绍以及它的架构之前已经分析过。本文分析flume的Source组件。 Flume内置了很多Source，比如Avro Source，Spooling Directory Source，NetCat Source，Kafka Source等。 以源码的角度分析Source组件。 Source接口Source接口的定义： public interface Source extends LifecycleAware, NamedComponent { public void setChannelProcessor(ChannelProcessor channelProcessor); public ChannelProcessor getChannelProcessor(); } Source接口只有2个方法，分别是setChannelProcessor和getChannelProcessor。ChannelProcessor是一个Channel处理器，会暴露一些操作用来将事件存储到channel中。 Source接口继承了LifecycleAware和NamedComponent接口。 LifecycleAware接口表示实现了该接口的类是具有状态的，有生命周期的特性。 public interface LifecycleAware { public void start(); // 启动一个服务或组件 public void stop(); // 关闭一个服务或组件 public LifecycleState getLifecycleState(); // 得到当前组件或服务的状态 } NamedComponent接口是一个可以让组件拥有名字的接口，这样组件可以在配置中被引用。 public interface NamedComponent { public void setName(String name); public String getName(); } Source接口的实现类AbstractSource，基本上所有的Source都会继承这个AbstractSource： AbstractSource属性： private ChannelProcessor channelProcessor; private String name; private LifecycleState lifecycleState; 这3个属性是都是都会在Source接口的方法中被使用到。 @Override public synchronized void start() { Preconditions.checkState(channelProcessor != null, &quot;No channel processor configured&quot;); lifecycleState = LifecycleState.START; } @Override public synchronized void stop() { lifecycleState = LifecycleState.STOP; } @Override public synchronized void setChannelProcessor(ChannelProcessor cp) { channelProcessor = cp; } @Override public synchronized ChannelProcessor getChannelProcessor() { return channelProcessor; } @Override public synchronized LifecycleState getLifecycleState() { return lifecycleState; } @Override public synchronized void setName(String name) { this.name = name; } @Override public synchronized String getName() { return name; } Flume内置的Source例子NetCat Source首先看最简单的NetCat Source。 NetCat Source配置如下： a1.sources.r1.type = netcat a1.sources.r1.bind = localhost a1.sources.r1.port = 44444 NetCat Source对应的Source类是NetcatSource，定义如下： public class NetcatSource extends AbstractSource implements Configurable, EventDrivenSource EventDrivenSource接口表示这个Source不需要外部驱动来收集event，而是自身会实现。这个接口只是一个标识，没有任何方法。 Configurable接口提供了一个public void configure(Context context);方法，会使用以下Context进行一些数据的配置。 NetcatSource覆盖了AbstractSource的start和stop方法，也实现了Configurable中的configure方法。 实现configure方法： 读取配置文件中的信息host和port信息(还有其他一些属性的配置，比如max-line-length和ack-every-event)。 覆盖start方法： 内部会使用ServerSocketChannel监听对应地址和端口发来的数据。 覆盖stop方法： 关闭ServerSocketChannel。 start方法内部会用线程池起一个新的进程来监听数据，其中有个processEvents方法，processEvents方法内部会读取Socket发来的数据，有段代码： Event event = EventBuilder.withBody(body); // process event ChannelException ex = null; try { source.getChannelProcessor().processEvent(event); } catch (ChannelException chEx) { ex = chEx; } 这段代码就会处理socket读取的数据并构造成一个Event，然后放到ChannelProcessor里，这个ChannelProcessor是在AbstractSource中定义的。 Kafka SourceKafka Source也是Flume内置的一个Source之一，配置如下： tier1.sources.source1.type = org.apache.flume.source.kafka.KafkaSource tier1.sources.source1.channels = channel1 tier1.sources.source1.zookeeperConnect = localhost:2181 tier1.sources.source1.topic = test1 tier1.sources.source1.groupId = flume tier1.sources.source1.kafka.consumer.timeout.ms = 100 对应的Source类是KafkaSource： public class KafkaSource extends AbstractSource implements Configurable, PollableSource PollableSource接口表示Source需要自己去查询数据(poll)。 public interface PollableSource extends Source { // process方法的返回值是个Status，有2个状态，分别是READY和BACKOFF public Status process() throws EventDeliveryException; public static enum Status { READY, BACKOFF } } KafkaSource的configure方法同样是读取配置文件信息。KafkaSource有几个特殊的配置，zookeeperConnect，groupId，topic都是kafka本身需要的配置。batchSize和batchDurationMillis是批处理的配置。batchSize表示批次数，每batchSize个event需要写入到Channel中，默认值是1000。batchDurationMillis是处理Kafka对i读取数据的时间，默认是1000毫秒，比如批次个数没有到1000，但是batchDurationMillis到了的话还是会丢到channel里。 start方法构造Kafka的ConsumerConnector，start方法还会调用父类AbstractSource的start方法，也就是初始化lifecycleState属性，说明这个Source是有生命周期的。 stop方法关闭Kafka的ConsumerConnector，同理stop也会调用父类的stop方法。 process方法是PollableSource接口的方法，KafkaSource需要我们自己去查询数据。 process部分源码如下： long batchStartTime = System.currentTimeMillis(); long batchEndTime = System.currentTimeMillis() + timeUpperLimit; try { boolean iterStatus = false; // 批次个数和处理时间全部符合才会进行处理 while (eventList.size() &lt; batchUpperLimit &amp;&amp; System.currentTimeMillis() &lt; batchEndTime) { iterStatus = hasNext(); if (iterStatus) { // kafka队列存在数据的话，提取数据，构造Event，放到eventList里，eventList是KafkaSource里的一个属性 MessageAndMetadata&lt;byte[], byte[]&gt; messageAndMetadata = it.next(); kafkaMessage = messageAndMetadata.message(); kafkaKey = messageAndMetadata.key(); headers = new HashMap&lt;String, String&gt;(); headers.put(KafkaSourceConstants.TIMESTAMP, String.valueOf(System.currentTimeMillis())); headers.put(KafkaSourceConstants.TOPIC, topic); headers.put(KafkaSourceConstants.KEY, new String(kafkaKey)); event = EventBuilder.withBody(kafkaMessage, headers); eventList.add(event); } } if (eventList.size() &gt; 0) { // eventList列表里有数据的话，将数据丢到channel里，清空eventList。代码执行到这里说明要么读取了1000条数据，要么处理时间到了 getChannelProcessor().processEventBatch(eventList); eventList.clear(); if (!kafkaAutoCommitEnabled) { consumer.commitOffsets(); } } // kafka队列没有数据的话返回BACKOFF状态 if (!iterStatus) { return Status.BACKOFF; } // kafka队列还有数据的话返回READY状态 return Status.READY; } catch (Exception e) { log.error(&quot;KafkaSource EXCEPTION, {}&quot;, e); return Status.BACKOFF; } 编写自定义的SourceFlume尽管已经提供了不少Source，但始终无法满足所有的需求。 比如数据库中的数据想使用Flume写入了其他渠道。 编写SQLSource只需要继承AbstractSource，且实现Configurable和PollableSource接口，SQLSource跟KafkaSource一样都需要自身去查询数据，所以都得实现PollableSource接口。 SQLSource在github上已经有人实现过了: https://github.com/keedio/flume-ng-sql-source","raw":null,"content":null,"categories":[{"name":"flume","slug":"flume","permalink":"http://fangjian0423.github.io/categories/flume/"}],"tags":[{"name":"big data","slug":"big-data","permalink":"http://fangjian0423.github.io/tags/big-data/"},{"name":"flume","slug":"flume","permalink":"http://fangjian0423.github.io/tags/flume/"}]},{"title":"Flume介绍","slug":"flume-introduction","date":"2015-06-21T13:23:34.000Z","updated":"2016-05-10T14:59:25.000Z","comments":true,"path":"2015/06/21/flume-introduction/","link":"","permalink":"http://fangjian0423.github.io/2015/06/21/flume-introduction/","excerpt":"","text":"Flume是一个分布式的，效率高的用来收集日志数据的开源框架。它的架构是基于流式数据，有3个重要的组件，分别是Source，Channel和Sink。 Flume架构和特点 Flume架构图如上，非常简单。 一个Flume的事件(event)表示数据流中的一个单位，它会带有字节数据和可选的字符串属性。一个Flume的agent是一个JVM进程，agent持有3个组件，这3个组件分别是Source，Channel和Sink。 Source组件会消费来自外部的一些事件源数据，这个外部事件源比如是一个web服务器。外部事件源会将事件以某种格式发送给Flume的Source，当Source接收到事件之后，会存储到一个或多个Channel中。 Channel是一个被动型的存储容器，它会一直保留事件直到事件被Sink消费。 Sink会消费Channel里的事件，然后会将事件放到外部仓库里，比如hdfs；或者Sink会转发到下一步Flume agent里做重复的事情。 Source和Sink在agent里异步执行处理channel里的事件。 Flume内部提供了一些常用的Source，Channel和Sink。 举个例子： Source使用Spooling Directory Source，这个Source会读取文件系统中的文件数据(文件系统中的外部数据相当于之前说的事件源)，读取数据之后会放到Channel中，比如使用Memory Channel,将Source接收到的事件存储到内存中，最后使用HDFS这个Sink将Memory Channel中的事件数据写入到hdfs中。 Spooling Directory Source， Memory Channel, HDFS Sink都是Flume内部提供的组件。 Flume非常可靠。每个agent的事件从Source进入到Channel之后，会存储在Channel中。这些事件只有在存储到下一步agent的Channel中或者外部存储仓库(比如hdfs)后，才会在Channel中被移除。 Flume还会使用事务来保证事件处理过程。 Flume还具有很高的可恢复性。事件是存储在channel中的，当使用File Channel的时候，当服务器挂了之后这些文件都还在，但是如果使用的是Memory Channel，就不具备容灾性。 一个简单的Flume配置如下： # agent名字是a1，有1个source:r1, 1个sink:k1, 1个channel:c1 a1.sources = r1 a1.sinks = k1 a1.channels = c1 # 使用netcat source a1.sources.r1.type = netcat a1.sources.r1.bind = localhost a1.sources.r1.port = 44444 # 使用 logger sink a1.sinks.k1.type = logger # 使用memory channel，容器为1000，事务容量为100 a1.channels.c1.type = memory a1.channels.c1.capacity = 1000 a1.channels.c1.transactionCapacity = 100 # 将channel讲source和sink关联 a1.sources.r1.channels = c1 a1.sinks.k1.channel = c1 之后至少还会有3篇Flume相关的文章，分别是讲解Source，Channel和Sink源码相关的文章。","raw":null,"content":null,"categories":[{"name":"flume","slug":"flume","permalink":"http://fangjian0423.github.io/categories/flume/"}],"tags":[{"name":"big data","slug":"big-data","permalink":"http://fangjian0423.github.io/tags/big-data/"},{"name":"flume","slug":"flume","permalink":"http://fangjian0423.github.io/tags/flume/"}]},{"title":"HHKB装上了彩色键帽","slug":"hhkb-desc","date":"2015-06-21T07:23:34.000Z","updated":"2016-05-10T14:59:25.000Z","comments":true,"path":"2015/06/21/hhkb-desc/","link":"","permalink":"http://fangjian0423.github.io/2015/06/21/hhkb-desc/","excerpt":"","text":"hhkb已经入手半年多了，早已习惯它的键位。 上个月已经给hhkb装上了彩色键帽，彩色键帽已经绝版了，只能在豆瓣上买其他人不要的键帽了。买了橙色和绿色的，红色和蓝色是之前在日亚买的。 coollllllll! 装上了键帽之后，逼格瞬间提高许多，心情也好了很多 &gt;….&lt;!! 唯一的缺点是第二排的红色键帽其实是第三排的control，因为大小刚好，所以就放下去了，虽然高度低了点。 http://elitekeyboards.com/products.php?sub=access,toprekeys 上的其他键帽也还不错，有机会还会继续入手~~ nice!!","raw":null,"content":null,"categories":[{"name":"hhkb","slug":"hhkb","permalink":"http://fangjian0423.github.io/categories/hhkb/"}],"tags":[{"name":"hhkb","slug":"hhkb","permalink":"http://fangjian0423.github.io/tags/hhkb/"},{"name":"life","slug":"life","permalink":"http://fangjian0423.github.io/tags/life/"},{"name":"keyboard","slug":"keyboard","permalink":"http://fangjian0423.github.io/tags/keyboard/"}]},{"title":"Scala集合相关知识","slug":"scala-collection","date":"2015-06-19T17:59:18.000Z","updated":"2016-05-10T14:59:25.000Z","comments":true,"path":"2015/06/20/scala-collection/","link":"","permalink":"http://fangjian0423.github.io/2015/06/20/scala-collection/","excerpt":"","text":"Scala各个集合类都会有两种类型，分别是可变的(mutable)和不可变的(immutable)。 比如列表集合List，不可变的类型是List，可变的类型为ListBuffer： // 不可变集合，集合添加元素后会产生一个新的集合 val immutableList = scala.collection.immutable.List(1, 2, 3) immutableList(1) = 3 // =号底层会调用update方法，immutable.List没有这个update方法 val mutableList = scala.collection.mutable.ListBuffer(1, 2, 3) mutableList(1) = 3 // ListBuffer(1, 3, 3), ListBuffer有update方法 Scala集合中3个主要的集合类的继承结构如下： Traversable是所有集合类的父类，它会提供一些很基础有用的方法，如下： 1.size: 返回集合元素的个数 List(1,2,3).size // 3 2.++方法：合并两个集合 List(1, 2, 3) ++ List(4, 5, 6) // List(1, 2, 3, 4, 5, 6) 3.map方法：使用函数作用于集合中的各个元素，生成新的集合 List(1,2,3).map { _ + 1 } // List(2, 3, 4) 4.flatMap方法：跟map方法类似，只不过flatMap整合的是集合，会生成新的集合 List(1, 2, 3).flatMap { x =&gt; List(x + 1, x + 2) } List(2, 3, 3, 4, 4, 5) 5.filter方法：使用函数过滤出符合条件的元素，生成新的集合 List(1, 2, 3).filter { _ &lt; 2 } // List(1) 6.find方法：找出第一个符合条件的符合，并得到这个元素。返回值的类型是个Option List(1, 2, 3).find { _ &lt; 3 } // Option[Int] = Some(1) List(1, 2, 3).find { _ &gt; 3 } // Option[Int] = None 7./:方法，/:即foldLeft。foleLeft方法类似reduce方法，只不过多了一个初始值，reduce是从左边开始进行的 List(1, 2, 3).foldLeft(10) { _ + _ } // 16 (10 /: List(1, 2, 3)) { _ + _ } // 16 8.:\\方法，:\\即foldRight。跟foldLeft相反，reduce是从右边开始进行的 List(1, 2, 3).foldRight(10) { _ + _ } // 16 (List(1, 2, 3) :\\ 10) { _ + _ } // 16 9.head方法：集合的第一个元素 List(1, 2, 3).head // 1 10.tail方法：除了集合的第一个元素之外的其他元素的集合 List(1, 2, 3).tail // List(2, 3) 11.mkString方法：使用字符串连接集合中的各个元素 List(1, 2, 3).mkString(&quot;=&quot;) // 1=2=3 List(1, 2, 3).mkString(&quot;===&quot;) // 1===2===3 Set集合Set也分两种集合，可变的Set和不可变的Set。 不可变的Set的全名是scala.collection.immutable.Set可变的Set全名为scala.collection.mutable.Set Set常用的一些方法如下： 1.contains方法，查看元素是否存在 Set(1,2,3).contains(2) // true Set(1,2,3).contains(4) // false // 可以直接调用set对象，相当于是contains方法 Set(1,2,3)(4) // false， 相当于 val s = Set(1,2,3), a(4) false 2.++方法，合并2个集合 Set(1,2,3) ++ Set(3,4,5) // Set(1,2,3,4,5) 3.&amp;方法，交集 Set(1,2,3) &amp; Set(2,3,4) // Set(2,3) 4.|方法，并集 Set(1,2,3) | Set(2,3,4) // Set(1,2,3,4) 5.&amp;~方法，差集 Set(1,2,3) &amp;~ Set(2,3,4) // Set(1) 6.+=方法，集合内部添加元素，只能在mutable.Set中使用 val set1 = scala.collection.immutable.Set(1) set1 += 2 // 报错 val set2 = scala.collection.mutable.Set(1) set2 += 2 // set2: Set(1,2) 7.update方法，添加，删除元素，只能在mutable.Set中使用。 用法： set.update(value, true | false) val set = scala.collection.mutable.Set(1,2,3) set.update(4, true) // set: Set(1,2,3,4) set.update(10, true) // set: Set(1,2,3,10,4) set.update(10, false) // set: Set(1,2,3,4) 8.clear方法，只能在mutable.Set中使用，清除集合中的所有元素 SortedSet: 跟Set一样，唯一的区别是SortedSet会记录插入的顺序。 Map和TupleScala中Map中的每一项都是一个scala.Tuple2，也就是1个带有2个变量的元组。 Map可以这样定义： val map1 = Map(1 -&gt; &quot;one&quot;, 2 -&gt; &quot;two&quot;) val map2 = Map((1, &quot;one&quot;), (2, &quot;two&quot;)) 获得对应键的值： map1(1) // &quot;one&quot; map1(3) // 报错。没有对应的值的话会报错 使用Map的get方法可以避免这个问题，get方法返回的是Option，如果有值，返回Some，否则返回None： map1.get(1) // Some map1.get(3) // None map1.get(1).getOrElse(999) // &quot;one&quot; map1.get(3).getOrElse(999) // 999 Map常用的方法如下： 1.getOrElse(k, d)，获得键值为k的对应的值，不存在的话使用默认值d map1.getOrElse(3, &quot;None&quot;) // &quot;None&quot; 2.+方法， map + (k -&gt; v)，给map添加一个新的项，返回一个新的map map1 + (3 -&gt; &quot;three&quot;) // 返回一个数3个项的新的map 3.++方法，合并2个map Map(1 -&gt; &quot;one&quot;) ++ Map(2 -&gt; &quot;two&quot;) 4.filterByKey，根据key进行过滤 map1.filterByKey { key =&gt; key == 1 } 5.mapValues，遍历所有的value值 map1.mapValues { value =&gt; println(value) } 6.update方法，修改map中的数据。只适用于mutable.Map map1.update(1, &quot;oneone&quot;) 7.clear方法，清空map数据。只适用于mutable.Map Map中的每一个都是一个Tuple2，Tuple2取第一个元素可以使用_1获得，第二个元素使用_2获得。 map1.foreach { item =&gt; println(item._1 + &quot;,&quot; + item._2) }","raw":null,"content":null,"categories":[{"name":"scala","slug":"scala","permalink":"http://fangjian0423.github.io/categories/scala/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"http://fangjian0423.github.io/tags/jvm/"},{"name":"scala","slug":"scala","permalink":"http://fangjian0423.github.io/tags/scala/"}]},{"title":"Scala偏函数(Partial Function)和部分应用(Partial application)","slug":"scala-partial","date":"2015-06-14T14:37:38.000Z","updated":"2016-05-10T14:59:25.000Z","comments":true,"path":"2015/06/14/scala-partial/","link":"","permalink":"http://fangjian0423.github.io/2015/06/14/scala-partial/","excerpt":"","text":"偏函数(Partial Function)的定义普通函数的定义中，有指定的输入类型，所以可以接受任意该类型下的值。比如一个(Int) =&gt; String的函数可以接收任意Int值，并返回一个字符串。 偏函数的定义中，也有指定的输入类型，但是偏函数不接受所有该类型下的值。比诶如(Int) =&gt; String的偏函数不能接收所有Int值为输入。 Scala的偏函数支持2个泛型类型，分别是输入类型和输出类型。 Scala中的偏函数数据类型为PartialFunction。 偏函数例子输入类型为Int，输出类型为String的偏函数例子: def one:PartialFunction[Int, String] = { case 1 =&gt; &quot;one&quot; } one(1) // &quot;one&quot; one(2) // 报错 def two: PartialFunction[Int, String] = { case 2 =&gt; &quot;two&quot; } two(1) // 报错 two(2) // &quot;two&quot; def wildcard: PartialFunction[Int, String] = { case _ =&gt; &quot;something else&quot; } wildcard(1) // &quot;something else&quot; wildcard(2) // &quot;something else&quot; 由于偏函数只会接收部分参数，所以可以使用 “orElse” 方法进行组合： val partial = one orElse two orElse wildcard partial(1) // &quot;one&quot; partial(2) // &quot;two&quot; partial(3) // &quot;something else&quot; 偏函数原理PartialFunction是一个trait，继承了scala.Function1这个trait。 orElse就是PartialFunction内部的1个方法，PartialFunction还有其他一些方法，比如isDefinedAt， andThen，applyOrElse等方法。 isDefinedAt：判断偏函数是否对参数中的参数有效 one.isDefinedAt(1) // true one.isDefinedAt(2) // false andThen：相当于方法的连续调用，比如g(f(x)) orElse: 本文已经分析过 applyOrElse：接收2个参数，第一个是调用的参数，第二个是个回调函数。如果第一个调用的参数匹配，返回匹配的值，否则调用回调函数 one.applyOrElse(2, { num:Int =&gt; &quot;two&quot; }) // &quot;two&quot; 当我们使用PartialFunction类型定义一个偏函数的时候，scala会被自动转换： def int2Char: PartialFunction[Int, Char] = { case 1 =&gt; &apos;a&apos; case 3 =&gt; &apos;c&apos; } 会被scala转换为： val int2Char = new PartialFunction[Int, Char] { def apply(i: Int) = { case 1 =&gt; &apos;a&apos; case 3 =&gt; &apos;c&apos; } def isDefinedAt(i: Int): Boolean = i match { case 1 =&gt; true case 3 =&gt; true case _ =&gt; false } } 部分应用(Partial application)部分应用表示一个函数的调用并不需要全部的参数，可以使用 “_” 这个特殊符号代表一个参数，使用部分应用之后函数的返回值是函数，而不是具体的值。 def add(x: Int, y: Int) = x + y val addOne = add(1, _: Int) addOne(4) // 5 addOne(6) // 7 部分应用的参数可以应用到参数中的任意参数。","raw":null,"content":null,"categories":[{"name":"scala","slug":"scala","permalink":"http://fangjian0423.github.io/categories/scala/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"http://fangjian0423.github.io/tags/jvm/"},{"name":"scala","slug":"scala","permalink":"http://fangjian0423.github.io/tags/scala/"}]},{"title":"Scala函数式编程","slug":"scala-func-program","date":"2015-06-11T17:14:55.000Z","updated":"2016-05-10T14:59:25.000Z","comments":true,"path":"2015/06/12/scala-func-program/","link":"","permalink":"http://fangjian0423.github.io/2015/06/12/scala-func-program/","excerpt":"","text":"函数式编程简介函数式编程(functional programming)，Lisp作为最古老的函数式编程语言，已重获新生。新的函数式编程语言也层出不穷，比如Erlang，clojure，Scala等。 一个简单的Scala函数式编程例子如下： List(1, 2, 3).foreach(x =&gt; println(x)) Scala函数式编程介绍函数是一等公民函数是第一等公民，指的是函数与其他数据类型一样，处于平等地位，可以赋值给其他变量，也可以作为参数，传入另一个函数，或者作为别的函数的返回值。 以下就是将一个匿名函数赋值给++变量的例子： val ++ = (num: Int) =&gt; num + 1 ++(2) // 3 ++(6) // 7 // ++ 变量赋值给tmp变量 val tmp = ++ tmp(7) // 8 tmp(9) // 10 函数作为参数例子，一个最典型的例子就是一些connection的处理，connection处理完毕之后都需要close，可以使用函数作为参数，这样就不用每次都close数据了： import scala.reflect.io.File import java.util.Scanner def scanFile(f: File, f: Scanner =&gt; Unit) = { val scanner = new Scanner(f.bufferedReader) try { f(scanner) } finally { scanner.close() } } scanFile(File(&quot;/tmp/test.txt&quot;), scanner =&gt; println(scanner.next())) 一些常用的higher-order函数higher-order函数指的是以函数做参数或者返回值是一个函数的函数。 List中的map，flatMap，foreach函数都是higher-order函数。 map例子，map函数的作用是使用一个函数调用集合中的各个元素，得到一个新的集合： List(1, 2, 3).map({x =&gt; x * 2}) // List(2, 4, 6) 可以简化为： List(1, 2, 3).map(x =&gt; x * 2) // List(2, 4, 6) 还可以简化为: List(1, 2, 3).map { x =&gt; x * 2 } // List(2, 4, 6) currying函数的应用 还可以简化为： List(1, 2, 3).map { _ * 2 } // List(2, 4, 6) flatMap例子，flatMap函数的作用跟map函数类似，把多个集合合并成一个集合，注意，flatMap针对的是集合： List(1, 2, 3).flatMap { x =&gt; List(x * 2) } // List(2, 4, 6) foreach例子： List(1, 2, 3).foreach { println(_) } 函数和方法的区别方法：方法指的是定义在类中的方法 class UseResource { def use(r: Resource): Boolean = { ... } } 上面UseResource类中的use方法就是一个方法，it is a method. 函数：函数在scala中代表1个类型和一个对象，方法却不会，方法只会出现在类中。 val succ = (x: Int) =&gt; x + 1 scala允许将方法转换成函数，可以在方法后面添加 “_” 即可。比如： val use_func: Resource =&gt; Boolean = new UserResource().use _ 函数柯里化(function currying)函数柯里化是一种把函数中多个参数改造成只有一个参数的技术。 def add(x: Int, y: Int) = x + y add函数柯里化之后： def add(x: Int) = (y: Int) =&gt; x + y 简化为: def add(x: Int)(y: Int) = x + y 调用柯里化后的add函数： add(2)(3) // 5 add(2) { 5 } // 7 scala支持把一个非柯里化的函数转换成一个柯里化函数，使用函数变量的curried方法： def add(x: Int, y: Int) = x + y val addVar = add _ // 转换成函数变量 val curryingFunc = addVar.curried // 使用curried方法转换成柯里化函数 curryingFunc(1) { 3 } // 4 // 或者定义add函数的时候直接使用函数变量 val add = (x: Int, y: Int) =&gt; x + y val curryingFunc = addVar.curried curryingFunc(1) { 3 } // 4 不过一般我们定义函数的时候都会定义成柯里化函数，而不会去转换： def add(x: Int)(y: Int) = x + y add(2) { 7 } // 9 // 实现一个map函数 def map[A, B](xs: List[A])(func: A =&gt; B) = xs.map { func(_) } // List[String] = List(11, 21, 31) map(List(1, 2, 3)) { x =&gt; x + &quot;1&quot; } 鸭子类型鸭子类型是动态类型的一种风格，使用鸭子类型就不可以不使用继承这种不够灵活的特性。 def withClose(closeAble: { def close(): Unit }, op: { def close(): Unit } =&gt; Unit) { try { op(closeAble) } finally { closeAble.close() } } class Connection { def close = println(&quot;close Connection&quot;) } val conn = new Connection() withClose(conn, conn =&gt; println(&quot;do something with Connection&quot;))","raw":null,"content":null,"categories":[{"name":"scala","slug":"scala","permalink":"http://fangjian0423.github.io/categories/scala/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"http://fangjian0423.github.io/tags/jvm/"},{"name":"scala","slug":"scala","permalink":"http://fangjian0423.github.io/tags/scala/"}]},{"title":"Scala特质(trait)","slug":"scala-trait","date":"2015-06-07T12:47:38.000Z","updated":"2016-05-10T14:59:25.000Z","comments":true,"path":"2015/06/07/scala-trait/","link":"","permalink":"http://fangjian0423.github.io/2015/06/07/scala-trait/","excerpt":"","text":"scala提供了一种叫做trait的特性。 Trait就像是有函数体的Interface，跟抽象类类似，trait不像抽象类，它没有构造函数，使用with关键字来混入trait。 比如想给java.util.ArrayList添加一个foreach方法，这个方法接受一个函数作为参数，对ArrayList重的每个元素执行这个函数。 如果是java的话，只能继承ArrayList，然后添加foreach方法，或者修改ArrayList的源码。 在scala中，可以使用trait完成这个功能。 trait ForeachAble[A] { def iterator: java.util.Iterator[A] def foreach(f: A =&gt; Unit) = { def iter = iterator while(iter.hasNext) f(iter.next) } } val list = new java.util.ArrayList[Int]() with ForeachAble[Int] list.add(1); list.add(2) list.foreach({ x =&gt; println(x) }) // println 1 and 2 with关键字可以混入多个trait。 trait Jsonable { def toJson() = scala.util.parsing.json.JSONFormat.defaultFormatter(this) } val list = new java.util.ArrayList[Int]() with ForeachAble[Int] with Jsonable list.add(1); list.add(2) list.foreach({ x =&gt; println(x) }) // println 1 and 2 println(list.toJson) // [1, 2] 再来个 + 方法： trait Addable[A] { def add(a: A): Boolean def +(a: A) = add(a) } val list = new java.util.ArrayList[Int]() with ForeachAble[Int] with Jsonable with Addable[Int] list + 2; list + 3 list.foreach({ x =&gt; println(x) }) // println 2 and 3 println(list.toJson) // [2, 3]","raw":null,"content":null,"categories":[{"name":"scala","slug":"scala","permalink":"http://fangjian0423.github.io/categories/scala/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"http://fangjian0423.github.io/tags/jvm/"},{"name":"scala","slug":"scala","permalink":"http://fangjian0423.github.io/tags/scala/"}]},{"title":"Scala泛型","slug":"scala-generic","date":"2015-06-07T10:17:21.000Z","updated":"2016-05-10T14:59:25.000Z","comments":true,"path":"2015/06/07/scala-generic/","link":"","permalink":"http://fangjian0423.github.io/2015/06/07/scala-generic/","excerpt":"","text":"简单回顾泛型java中可使用泛型进行编程，一个简单的泛型例子如下： List&lt;String&gt; strList = new ArrayList&lt;String&gt;(); strList.add(&quot;one&quot;); strList.add(&quot;two&quot;); strList.add(&quot;three&quot;); String one = strList.get(0); // 泛型拿数据不必进行类型转换，不使用泛型的话需要对类型进行转换 scala的泛型scala中的泛型称为类型参数化(type parameterlization)。语法跟java不一样，使用”[]”表示类型。 一个使用类型参数化的函数： def position[A](xs: List[A], value: A): Int = { xs.indexOf(value) } position(List(1,2,3), 1) // 0 position(List(&quot;one&quot;, &quot;two&quot;, &quot;three&quot;), &quot;two&quot;) // 1 稍微复杂点的类型参数化，实现一个map函数，需要一个List和一个函数作为参数： 普通的map方法： List(1,2,3) map { _ * 2 } // List[Int] = List(2,4,6) List(1,2,3) map { _ + &quot;2&quot; } // List[String] = List(12, 22, 32) 使用泛型实现的map方法： def map[A,B](list:List[A], func: A =&gt; B) = list.map(func) map(List(1,2,3), { num: Int =&gt; num + &quot;2&quot; }) // List[String] = List(12, 22, 32) map(List(1,2,3), { num: Int =&gt; num * 2 }) // List[Int] = List(2, 4, 6) 上限和下限scala跟java一样，也提供了上限(upper bounds)和下限(lower bounds)功能。 上限(upper bounds)java中上限的使用如下： &lt;T extends Object&gt; 通配符形式 &lt;? extends Object&gt; scala写法： [T &lt;: AnyRef] 通配符形式 [_ &lt;: AnyRef] 上限的一些例子： public void upperBound(List&lt;? extends Number&gt; list) { Object obj = list.get(0); // Number是Object的子类，使用Object可以代替Number。 Number num = list.get(0); Integet i = list.get(0); // compile error list.add(new Integer(1)); // compile error } 上限做参数，set的话不能确定具体的类型，所以会报编译错误。get的话得到的结果的类型的下限为参数的上限。相当于使用了上限参数的话，该参数就变成了只读参数，类似生产者，只提供数据。 scala版本： def upperBound[A &lt;: Animal](list: ListBuffer[A]): Unit = { list += new Animal(&quot;123&quot;) // compile error val obj: AnyRef = list(0) // ok val a: Animal = list(0) // ok val a: Cat = list(0) // compile error } 这里使用ListBuffer作为集合，ListBuffer的+=方法会在列表内部添加数据，不会产生一个新的List。如果使用List的话，:+操作符会在新生成的List中自动得到符合所有元素的类型。 List[Animal](new Cat()) :+ 1 // List[Any] = List(Cat@3f23a076, 1) 生成新的List会自动根据上下文得到新的泛型类型List[AnyRef]。 下限(lower bounds)&lt;T super MyClass&gt; 通配符形式 &lt;? super MyClass&gt; scala写法： [T &gt;: MyClass] 通配符形式 [_ &gt;: MyClass] 下限的一些例子： public static void lowerBound(List&lt;? super Number&gt; l) { l.add(new Integer(1)); l.add(new Float(2)); Object obj = l.get(0); Number num = l.get(0); // compile error } 下限做参数，get方法只能用最宽泛的类型来获取数据，相当于get只提供了数据最小级别的访问权限。类似消费者，主要用来消费数据。 scala版本： def lowerBound[A &gt;: Animal](list: ListBuffer[A]): Unit = { list += new Animal() // ok list += new Cat() // ok val obj: Any = list(0) // ok val obj: Animal = list(0) // compile error } 协变和逆变协变(covariance)：对于一个带类型参数的类型，比如List[T]，如果对A及其子类型B，满足List[B]也符合List[A]的子类型，那么就称为协变，用加号表示。比如 MyType[+A] 逆变(contravariance)：如果List[A]是List[B]的子类型，用减号表示。比如MyType[+B] 如果一个类型支持协变或逆变，则称这个类型为可变的(variance)。否则称为不可变的(invariant)。 在java里，泛型类型都是不可变的，比如List&lt;String&gt;并不是List&lt;Object&gt;的子类。 trait A[T] class C[T] extends A[T] class Parent; class Child extends Parent val c:C[Parent] = new C[Parent] // ok val c:C[Parent] = new C[Child]; // Child &lt;: Parent, but class C is invariant in type T. 协变上面的例子提示已经很明确了。类C是不可变的，改成协变即可。 trait A[+T] class C[+T] extends A[T] class Parent; class Child extends Parent val c: C[Parent] = new C[Parent] // ok val c: C[Parent] = new C[Child] // ok scala中List就是一个协变类。 val list:List[Parent] = List[Child](new Child()) 逆变逆变概念与协变相反。 trait A[-T] class C[-T] extends A[T] class Parent; class Child extends Parent val c: C[Parent] = new C[Parent] // ok val c: C[Child] = new C[Parent] // ok 协变逆变注意点逆变协变并不会被继承，父类声明为逆变或协变，子类如果想要保持，任需要声明： trait A[+T] class C[T] extends A[T] // C是不可变的，因为它不是逆变或协变。 class D[+T] extends A[T] // D是可变的，是协变 class E[-T] extends A[T] // E是可变的，是逆变","raw":null,"content":null,"categories":[{"name":"scala","slug":"scala","permalink":"http://fangjian0423.github.io/categories/scala/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"http://fangjian0423.github.io/tags/jvm/"},{"name":"scala","slug":"scala","permalink":"http://fangjian0423.github.io/tags/scala/"}]},{"title":"Scala模式匹配","slug":"scala-pattern-match","date":"2015-06-03T17:03:27.000Z","updated":"2016-05-10T14:59:25.000Z","comments":true,"path":"2015/06/04/scala-pattern-match/","link":"","permalink":"http://fangjian0423.github.io/2015/06/04/scala-pattern-match/","excerpt":"","text":"模式匹配介绍模式匹配是一种scala中的一种函数式编程概念，跟java中的switch case类似。 java中的switch case比较局限，只支持基本类型和枚举类型；而且switch case如果case不到相应的值，除非在有写default，否则就不会执行。 模式匹配简单例子： 1 match { case 1 =&gt; println(&quot;yeah, it is 1&quot;) } 模式匹配的应用模式匹配可以匹配字符串，复杂值，类型，变量，常量，构造函数，元组。 变量匹配本文介绍的简单例子就可说明。 字符串匹配val name = &quot;format&quot; name match { case &quot;format&quot; =&gt; println(&quot;my name is format&quot;) } 类型匹配def printType(obj: AnyRef) = { case s:String =&gt; println(&quot;this is String&quot;) case l:List[_] =&gt; println(&quot;this is List&quot;) case a:Array[_] =&gt; println(&quot;this is array&quot;) case d:java.util.Date =&gt; println(&quot;this is Date&quot;) } printType(&quot;1&quot;) // this is String printType(List(1)) // this is List printType(Array(1)) // this is Array printType(new java.util.Date()) // this is Date printType(Map(1 -&gt; 2)) // 报错。scala.MatchError: Map(1 -&gt; 2) (of class scala.collection.immutable.Map$Map1) 构造函数匹配之前介绍scala的对象的unapply方法的时候提到了这点。 构造函数的模式匹配基于对象的unapply方法。 case class Person(name: String, age: Int) val p = new Person(&quot;format&quot;, 99) // 打印 format,99 p match { case Person(name, age) =&gt; println(name + &quot;,&quot; + age) } 元组匹配// 打印1,2 (1,2) match { case (a,b) =&gt; println(a+&quot;,&quot;+b) } // 打印2 (1,2) match { case (1, b) =&gt; println(b) } // 打印found (1,2) match { case (_, 2) =&gt; println(&quot;found&quot;) } 特殊的模式匹配中缀操作符匹配(infix operation pattern)直接看例子： // 得到List(1,2) List(1,2,3,4) match { case a :: b :: other =&gt; List(a, b) case _ =&gt; List() } a和b会被解析成1和2，other会被解析成List(3,4)。 :: 操作符是List的添加操作符。 带有逻辑判断的模式匹配case里面可以添加部分逻辑代码： 1 match { case i if i &lt; 10 =&gt; println(&quot;less than 10&quot;) case i if i &gt;= 10 =&gt; println(&quot;more than 10&quot;) } 模式匹配要注意的几个地方1.没有匹配到的模式匹配会报错 // 报错， 2没有匹配1 2 match { case 1 =&gt; println(&quot;get one&quot;) } 2.使用 “_” 代替没匹配到的情况 跟java里的switch case里的default语法一样。 // error get num 2 match { case 1 =&gt; println(&quot;get one&quot;) case _ =&gt; println(&quot;error get num&quot;) }","raw":null,"content":null,"categories":[{"name":"scala","slug":"scala","permalink":"http://fangjian0423.github.io/categories/scala/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"http://fangjian0423.github.io/tags/jvm/"},{"name":"scala","slug":"scala","permalink":"http://fangjian0423.github.io/tags/scala/"}]},{"title":"Scala类和对象","slug":"scala-class-object","date":"2015-05-31T14:02:14.000Z","updated":"2016-05-10T14:59:25.000Z","comments":true,"path":"2015/05/31/scala-class-object/","link":"","permalink":"http://fangjian0423.github.io/2015/05/31/scala-class-object/","excerpt":"","text":"类的定义class Person(val name:String, val age: Int) 构造函数中的参数修饰符用val修饰，就是类的定义中的那种方式使用val修饰，属性会有对应的getter方法。 val p = new Person(&quot;format&quot;, 99) p.name // format p.name = &quot;new name&quot; // 报错，用val修饰的变量不可变 用var修饰使用var修饰，属性会有对应的getter方法。同时还会有setter方法。 class Person(var name:String, var age: Int) val p = new Person(&quot;format&quot;, 99) p.name // format p.name = &quot;new name&quot; p.name // new name p.age = 100 不用任何修饰符不适用任何修饰符代表这个这些属性被当做私有的实例值(private instance values)，在类的外部不能被访问。 class Person(name:String, age: Int) val p = new Person(&quot;format&quot;, 99) p.name // 报错，error: value name is not a member of Person p.name = &quot;new name&quot; // 报错，error: value name is not a member of Person 使用private修饰符private修饰的属性不会有对应的生成器(accessors)，外部无法访问。 class Person(private var name:String, private var age: Int) val p = new Person(&quot;format&quot;, 99) p.name // error: variable name in class Person cannot be accessed in Person 手动添加getter和setter生成器class Person(name:String, private var _age: Int) { def age = _age // getter生成器 def age_=(newAge: Int) = age = newAge // setter生成器 } val p = new Person(&quot;format&quot;, 99) p.age // 99 p.age = 1 p.age // 1 p.age = 3会被scala解析成p.age_=(3) 构造函数的重载class Person(var name:String, var age: Int) { def this() = this(&quot;format&quot;, 11) } val p = new Person() p.name // format p.age // 11 单例对象的定义单例对象scala不提供static关键字，也就是说没有静态变量，静态类，静态方法等概念了，取而代之的是使用object关键字，也就是单例对象。 object Switch { def open: Boolean = true } Switch.open // true apply方法对象的apply方法比较特殊。 object Person { def apply(name: String, age: Int) = new Person(name, age) } val p = Person(&quot;format&quot;, 99) p.name // format p.age // 99 apply方法的作用也就是可以让对象被直接调用，这里Person的apply方法返回的是一个Person类的实例，Person类也就是之前定义的一个类。 unapply方法对象的unapply方法用于模式匹配(pattern match)。模式匹配的内容会在之后的文章中说明。 object Person { def unapply(p: Person): Option[(String, Int)] = Some((p.name, p.age)) } val p = new Person(&quot;format&quot;, 99) p match { case Person(name, age) =&gt; println(name + &quot;,&quot; + age) } 样本类 (case class)样本类的定义: case class Person(name: String, age: Int) 样本类会自定生成一些样本代码： 会给所有的参数带上val前缀(可以手动改为var前缀，case class Person(var name: String, age: Int)) equals和hashCode方法会根据参数自动生成 toString方法也会自动生成，会带上类名和参数 所有的case类都会有有一个copy方法 companion object会自动生成，且还会带有apply方法，apply方法的参数跟类的参数一样(companion object也就是对应类的单例对象，比如本文提到了Person类和Person对象) companion object还会有unapply方法 会有一个默认的序列化实现(a default implementation is provided for serialization) 以之前定义的Person样本类为例，验证一些样本代码： val p = Person(&quot;format&quot;, 99) // case Person会生成一个带有apply的Person对象 p.name // format p.age // 99 p // Person(format, 99) p == Person(&quot;format&quot;, 99) // true p == Person(&quot;format&quot;, 9) // false val p2 = p.copy() // p2: Person(format, 99) p == p2 // true p.copy(name=&quot;newName&quot;) // Person(newName, 99) p.hashCode // 187596955 p2.hashCode //187596955 // 模式匹配，unapply方法 p match { case Person(name, age) =&gt; println(name + &quot;,&quot; + age) } val p = new Person(&quot;name&quot;, 99) // 不用单例对象，用new构造实例","raw":null,"content":null,"categories":[{"name":"scala","slug":"scala","permalink":"http://fangjian0423.github.io/categories/scala/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"http://fangjian0423.github.io/tags/jvm/"},{"name":"scala","slug":"scala","permalink":"http://fangjian0423.github.io/tags/scala/"}]},{"title":"Scala基础","slug":"scala-introduction","date":"2015-05-31T01:49:50.000Z","updated":"2016-05-10T14:59:25.000Z","comments":true,"path":"2015/05/31/scala-introduction/","link":"","permalink":"http://fangjian0423.github.io/2015/05/31/scala-introduction/","excerpt":"","text":"Scala简介Scala是一门可以运行的jvm上的动态语言，其他可以运行在jvm上的动态语言还有groovy,clojure等。 Scala是一种面向对象编程语言，同时又无缝结合了命令式和函数式的编程风格。 变量的定义以及赋值使用var或val定义变量。 var表示这个变量可变，val则表示这个变量不可变(类似java中的final关键字)。 var a = 1; a = 2 val a = 1; a = 1 // 报错，a不可变 基础类型scala有8种基础类型，分别是Byte, Short, Int, Long, Float, Double, Boolean, Char。 var a: Int = 1 // 指定变量a的类型为Int，不指定的话scala会自动根据上下文得出变量的类型 var str = &quot;a test string&quot; var ten = &quot;10&quot;.toInt // string类型有toInt方法可以转换成Int类型 函数的定义函数的定义语法如下： def 函数名(参数名: 参数类型): 函数返回类型 = 函数体 1个简单的函数: def scalaFunc(name: String): Unit = println(&quot;welcome to use scala function: &quot; + name) // 函数调用 scalaFunc(&quot;format&quot;) // 打印出 welcome to use scala cuntion: format if语法if语法跟其他语言类似 if(some condition) value1 else value2 for语法基本使用： val range = 1 to 10 // 打印出1-10数字 for(index &lt;- num) { println(index) } 在for循环中使用yield关键字可返回循环中的各个值并组成一个新的集合 val range = 1 to 10 for(index &lt;- range) yield index // 得到一个1-10的新的集合 for循环中可以嵌入其他语法： 打印出在1-10中所有的偶数: for(index &lt;- 1 to 10) { if（index % 2 == 0) println(index) } 由于scala可以在for循环中嵌入其他语法，所以可以简化： for(index &lt;- 1 to 10; if(index % 2 == 0)) println(index) List和Array的简单使用List和Array都支持类型参数化，也就是泛型。 Array的使用： val array = new Array[String](3) array(0) = &quot;format01&quot; array(1) = &quot;format02&quot; List的使用： val list = List[Int](1, 2, 3) list ++ List(4, 5, 6) // List(1, 2, 3, 4, 5, 6) 1 :: list // list前方加入元素。 List(1, 1, 2, 3) list :+ 1 // list后方加入元素。 List(1, 2, 3, 1)","raw":null,"content":null,"categories":[{"name":"scala","slug":"scala","permalink":"http://fangjian0423.github.io/categories/scala/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"http://fangjian0423.github.io/tags/jvm/"},{"name":"scala","slug":"scala","permalink":"http://fangjian0423.github.io/tags/scala/"}]},{"title":"playframework简介","slug":"playframework-introduce","date":"2015-05-17T15:56:38.000Z","updated":"2016-05-10T14:59:25.000Z","comments":true,"path":"2015/05/17/playframework-introduce/","link":"","permalink":"http://fangjian0423.github.io/2015/05/17/playframework-introduce/","excerpt":"","text":"好久没写博了，因为工作忙成狗了 →_→ ， 还有一个原因就在最近还在看scala in action这本书。 in action系列的就不用多介绍了，像我这种英文渣渣都能看的懂。 →_→。 几个web框架说明一下： grails：工作上使用的框架，grails虽然开发也很快，但是是用groovy语言写的，不是很喜欢groovy。grails老是遇到一些莫名其妙的问题，之前还碰到过domain有个enum类型的属性，使用ordinal映射，但是数据库中字段的类型。domain做query查询，debug居然提示Variable没有找到。后来在try catch块内找到了相对正确一点的error message。而且groovy语法感觉设计地很糟糕，完全不能跟scala对比。SpringMVC：不用多说了，还看过源码，还是不错的。当年写java的时候的第一选择。虽然现在还在写 →_→。Struts2：不想说了，不好用，不知道现在用的人还多不。Playframework：java，scala都可以使用。跟grails一样，开发速度也很快。 Playframework简介记录一下play常用的功能，做个笔记。 从MVC角度分别说明一下。 Controllerplay的controller需要继承Controller，controller中的每个方法都是一个Action。一个Action本质上就是一个 (play.api.mvc.Request =&gt; play.api.mvc.Result) 函数，这个函数处理请求并生成对应的结果。 1最简单的Controller： package controllers import play.api._ import play.api.mvc._ object Application extends Controller { def index = Action { Ok(&quot;This is index page&quot;) } } index方法就是一个Action。Ok代表一个返回值为200的Response。 Action可以带request参数： def index = Action { request =&gt; Ok(&quot;request: &quot; + request) } request可以加上implicit，这样其他的api也可以使用： def index = Action { implicit request =&gt; Ok(&quot;request: &quot; + request) } 响应json： val list = Action { implicit request =&gt; val items = Item.findAll render { case Accepts.Html() =&gt; Ok(views.html.list(items)) case Accepts.Json() =&gt; Ok(Json.toJson(items)) } } 除了Ok返回，还有一些其他封装好的方法(这些方法都是在play.api.mvc.Results中定义的)： NotFound BadRequest InternalServerError Status TODO // def index(name: String) = TODO Redirect 还可以自定义header和response： def index = Action { Result( header = ResponseHeader(200, Map(CONTENT_TYPE -&gt; &quot;text/plain&quot;)), body = Enumerator(&quot;Hello World!&quot;.getBytes()) ) } controller中定义的方法如果想被http访问，需要在routes中定义路由规则和对应的controller中的方法。 这点是我不大爽的地方，项目一大，routes中的配置内容也会变多。 GET /clients/:id controllers.Clients.show(id: Long) GET /items/$id&lt;[0-9]+&gt; controllers.Items.show(id: Long) // 默认值 GET /clients controllers.Clients.list(page: Int ?= 1) // Option类型的参数不需要一定传递 GET /api/list-all controllers.Api.list(version: Option[String]) Viewplay的模板引擎中可以写一些scala代码。 play的模板引擎会根据对应的模板文件，生成一个类，这个类有个apply方法。 比如views目录下有个index.scala.html文件，play会生成一个views.html.index类。 也可以使用views.html.index方法得到对应的html文本： val content = views.html.index(c, o) 一段简单的模板代码： @(customer: Customer, orders: List[Order]) &lt;h1&gt;Welcome @customer.name!&lt;/h1&gt; &lt;ul&gt; @for(order &lt;- orders) { &lt;li&gt;@order.title&lt;/li&gt; } &lt;/ul&gt; 一般模板文件第一行会定义参数。然后接下来就是具体的逻辑。 play的模板引擎使用@来表示scala代码。之前例子的for循环就是使用了@。 还有if： @if(items.isEmpty) { &lt;h1&gt;Nothing&lt;/h1&gt; } else { &lt;h1&gt;&lt;@items.size items/h1&gt; } 模板中还可以定义函数： @display(product: Product) = { @product.name ($@product.price) } &lt;ul&gt; @for(product &lt;- products) { @display(product) } &lt;/ul 模板的继承： @(title: String)(content: Html) &lt;!DOCTYPE html&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;@title&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;section class=&quot;content&quot;&gt;@content&lt;/section&gt; &lt;/body&gt; &lt;/html&gt; @main(title = &quot;Home&quot;) { &lt;h1&gt;Home page&lt;/h1&gt; } 2个子模板： @(title: String)(sidebar: Html)(content: Html) &lt;!DOCTYPE html&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;@title&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;section class=&quot;sidebar&quot;&gt;@sidebar&lt;/section&gt; &lt;section class=&quot;content&quot;&gt;@content&lt;/section&gt; &lt;/body&gt; &lt;/html&gt; @main(&quot;Home&quot;) { &lt;h1&gt;Sidebar&lt;/h1&gt; } { &lt;h1&gt;Home page&lt;/h1&gt; } 或者： @sidebar = { &lt;h1&gt;Sidebar&lt;/h1&gt; } @main(&quot;Home&quot;)(sidebar) { &lt;h1&gt;Home page&lt;/h1&gt; } Modelplay的Model使用的是Ebean这个ORM框架","raw":null,"content":null,"categories":[{"name":"scala","slug":"scala","permalink":"http://fangjian0423.github.io/categories/scala/"}],"tags":[{"name":"scala","slug":"scala","permalink":"http://fangjian0423.github.io/tags/scala/"},{"name":"playframework","slug":"playframework","permalink":"http://fangjian0423.github.io/tags/playframework/"}]},{"title":"macos中jdk版本切换","slug":"mac-jdk","date":"2015-03-29T16:51:12.000Z","updated":"2016-05-10T14:59:25.000Z","comments":true,"path":"2015/03/30/mac-jdk/","link":"","permalink":"http://fangjian0423.github.io/2015/03/30/mac-jdk/","excerpt":"","text":"简单记录一下macos中的jdk版本切换问题。 macos中安装jdk就有点麻烦… 不像ubuntu一样，ubuntu解压一下放到某个文件夹下，path指定一下就ok了，macos却比较搞。 我在macos上安装了jEnv。 jEnv is a command line tool to help you forget how to set the JAVA_HOME environment variable jEnv是一个命令行工具，用来帮助你忘记怎么设置JAVA_HOME环境变量的设置。 具体的install和jdk版本切换可以上官网看下，很easy的。 使用jEnv设置好jdk版本之后，比如要使用maven命令，可能会出现如下错误： Error: JAVA_HOME is not defined correctly. We cannot execute /usr/libexec/java_home/bin/java FAIL 这个时候可以使用 jenv enable-plugin maven 这样的话就可以使用maven命令了。 同理，disable的话： jenv disable-plugin maven 同理，还有其他插件，比如： jenv enable-plugin grails 我的maven命令之所以出现错误是这样的： 一开始安装了jdk8。 后来换成了jdk7。 但是一开始装了jdk8，macos下/usr/libexec/java_home默认指向了jdk8，装了jdk7之后，用jEnv设置默认的jdk为1.7。但是/usr/libexec/java_home还是指向jdk1.8。 然后就会出现以上情况。 PS： 如果不安装jEnv的话，在环境变量中加入JAVA_HOME可能会解决问题。 比如还是有2个jdk版本，1.7和1.8。 使用jdk1.7版本，JAVA_HOME设置如下： /usr/libexec/java_home -v 1.7 使用jdk1.8版本，JAVA_HOME设置如下： /usr/libexec/java_home -v 1.8 我设置了貌似不行 → → 可能装了jEnv有影响。 暂时先不研究了 → →","raw":null,"content":null,"categories":[{"name":"java","slug":"java","permalink":"http://fangjian0423.github.io/categories/java/"}],"tags":[{"name":"java","slug":"java","permalink":"http://fangjian0423.github.io/tags/java/"},{"name":"mac","slug":"mac","permalink":"http://fangjian0423.github.io/tags/mac/"}]},{"title":"bootstrap学习系列之表格布局原理","slug":"bootstrap-grid-theory","date":"2015-03-08T14:07:24.000Z","updated":"2016-05-10T14:59:25.000Z","comments":true,"path":"2015/03/08/bootstrap-grid-theory/","link":"","permalink":"http://fangjian0423.github.io/2015/03/08/bootstrap-grid-theory/","excerpt":"","text":"分析一下bootstrap中表格布局的原理。 自定义css: .border { margin-top: 40px; margin-ledt: 40px; border: 1px solid red; width: 300px; } ##表格布局基础## 页面： &lt;div class=&quot;col-sm-12 border&quot;&gt; &lt;div class=&quot;col-sm-6&quot;&gt; 123 &lt;/div&gt; &lt;div class=&quot;col-sm-6&quot;&gt; 456 &lt;/div&gt; &lt;/div&gt; 效果： 表格布局的基础已经分析过。 重点来看下一些样式的定义。 col-sm-**这些样式都有一些默认的值，分别有： width: 100%; (因为是col-sm-12，除了width在表格布局中不一样，下面的值一样) float: left; position: relative; min-height: 1px; padding-right: 15px; padding-left: 15px; 最外层的width本来是100%(col-sm-12)， 在自定义的样式border中覆盖了，改成了width: 300px; 外部红色边框的div的padding-left和padding-right，15个像素。 外部红色边框div的Content。 内部div内容为123的padding-left和padding-right，15个像素。 内部div内容为123的Content。 外部红色边框的div的盒子模型。margin-top和margin-left都是40个像素，这是自定义的样式border指定的。border为1个像素的红色，也是自定义样式border指定的。padding-right和padding-left是bootstrap自带的样式col-sm-**定义的，都是15个像素。padding-top和padding-bottom没有任何地方指定，为0。内容Content的高度没有指定，以自字体高度为准，20个像素。内容的宽度为268个像素。 这个268像素是这样来的： 300(自定义样式border定义的宽度) - 1 * 2(左右边距) - 15 * 2(左右内边距) = 268 内部两个样式为col-sm-6的div的盒子模型。 内容104个像素的宽度是这样算的： 150 - 1 - 15(150边距外部div宽度的一半，也就是col-sm-6的width: 50%算的，1是外部div的左边框，15是外部div的左内边框) - 15 * 2(左右内边距) = 104 ##col-sm-offset样式## &lt;div class=&quot;col-sm-12 border&quot;&gt; &lt;div class=&quot;col-sm-6 col-sm-offset-6&quot;&gt; 123 &lt;/div&gt; &lt;/div 效果： 内部div的盒子模型。 由于使用了col-sm-offset-6。 这个样式属性如下： margin-left: 50%; 所以盒子模型的margin-left为134。 300 - 1 * 2 - 15 * 2 (外层红色边框div的Content) * 50% = 134 ##相对布局样式## &lt;div class=&quot;col-sm-12 border&quot;&gt; &lt;div class=&quot;col-sm-6&quot; style=&quot;left: 100px;&quot;&gt; 123 &lt;/div&gt; &lt;/div&gt; 效果： 盒子模型。由于col-sm-**使用的是相对布局，left，right，top，bottom这些属性都是有效的。所以left: 100px生效了。内部div在原先的位置上向右偏移了100个像素。","raw":null,"content":null,"categories":[{"name":"bootstrap","slug":"bootstrap","permalink":"http://fangjian0423.github.io/categories/bootstrap/"}],"tags":[{"name":"bootstrap","slug":"bootstrap","permalink":"http://fangjian0423.github.io/tags/bootstrap/"},{"name":"css","slug":"css","permalink":"http://fangjian0423.github.io/tags/css/"}]},{"title":"css 盒子模型","slug":"css_boxmodel","date":"2015-03-07T12:51:12.000Z","updated":"2016-05-10T14:59:25.000Z","comments":true,"path":"2015/03/07/css_boxmodel/","link":"","permalink":"http://fangjian0423.github.io/2015/03/07/css_boxmodel/","excerpt":"","text":"废话不多讲，直接进入主题。 所有的HTML元素都具有盒子模型的特性。 盒子模型由margin, border, padding和元素内部的内容(Content)组成。 margin指的是 border边框外的一块区域， 俗称外边距padding指的是 包裹着content的一块区域， 俗称内边距border指的是 包裹着padding的一块区域，俗称边框Content指的是 盒子中的内容，文本和图片出现的地方，俗称元素内容 一个盒子模型从外到里一次是 Margin -&gt; Border -&gt; Padding -&gt; Content 以下就是一个box model的样子。 接下来看一个div的css： div { width: 300px; padding: 25px; border: 25px solid navy; margin: 25px; } 这样一个div在页面上是这样的： 通过chrome的console看到它的盒子模型： 从外到里分析一下： margin占了25个像素， border也是25个像素，padding还是25个像素，content内容占了300个像素。 这样的话整个box盒子的宽度占了： 300(Content宽度) + 25 * 2(两个padding宽度，padding-left，padding-right) + 25 * 2(两个border宽度，border-left, border-right) + 25 * 2(两个margin宽度,margin-left, margin-right) = 450个像素 总结一下： css盒子模型由4部分组成，分别是margin(外边距)， border(边框)，padding(内边距)和Content(内容)。 盒子的宽度不仅仅是内容的宽度，还包括其他3部分的宽度。 盒子的高度不仅仅是内容的高度，还包括其他3部分的高度。 盒子宽度 = 元素宽度 + 左内边距 + 右内边距 + 左边框 + 右边框 + 左外边距 + 右外边距 盒子高度 = 元素高度 + 上内边距 + 下内边距 + 上边框 + 下边框 + 上外边距 + 下外边距","raw":null,"content":null,"categories":[{"name":"css","slug":"css","permalink":"http://fangjian0423.github.io/categories/css/"}],"tags":[{"name":"css","slug":"css","permalink":"http://fangjian0423.github.io/tags/css/"},{"name":"boxmodel","slug":"boxmodel","permalink":"http://fangjian0423.github.io/tags/boxmodel/"}]},{"title":"css position属性记录","slug":"css_position","date":"2015-03-01T11:23:54.000Z","updated":"2016-05-10T14:59:25.000Z","comments":true,"path":"2015/03/01/css_position/","link":"","permalink":"http://fangjian0423.github.io/2015/03/01/css_position/","excerpt":"","text":"前言前几天在公司看到一个关于div disable的功能。 html中一些input会有disable属性，但是div块这种元素是没有disable属性的，可以自己实现一个。 公司里要实现的div disable功能基于css的布局原理。好奇心使用研究了它的原理。它的原理是使用position属性完成的， 在这里记录一下。 css position属性html文档中是基于流式布局的，可以使用position属性修改元素的布局方式。 position属性规定元素的定位类型。所有的元素都有position这个属性。 下面是它的一些值的定义以及解释： static: 默认值。没有定位，出现在正常的流中，会忽略top，bottom，left，right或者z-index声明。 absolute: 绝对定位。元素的位置通过top, bottom, left, right规定。 相对于static定位以外的第一个父元素进行定位 relative: 相对定位。元素的位置通过top, bottom, left, right规定。相对其正常位置进行定位 fixed：固定定位。元素的位置通过top, bottom, left, right规定。相对浏览器窗口进行定位 各个position属性验证一些css代码如下： .box { border: 1px solid; width: 100px; height: 100px; } static 流式布局 默认值position为static表示遵从流式布局。 &lt;div class=&quot;box&quot;&gt;盒子1&lt;/div&gt; &lt;div class=&quot;box&quot;&gt;盒子2&lt;/div&gt; 两个div盒子块。不设置postion属性，遵从流式布局，div的display是block，是个块，所以会换行。 因此两个盒子一上一下换行。 relative 相对布局相对布局相对的是其正常位置。 也就是说相对布局在它原先的流式布局的位置上使用top, bottom, left, right进行定位。 &lt;div class=&quot;box&quot;&gt;盒子1&lt;/div&gt; &lt;div class=&quot;box&quot; style=&quot;position: relative; left: 20px; bottom: 20px;&quot;&gt;盒子2&lt;/div&gt; &lt;div class=&quot;box&quot;&gt;盒子3&lt;/div&gt; 绿色部分表示left属性的生效，有20个像素的距离，间隔原先位置左边20个像素。 红色表示bottom属性的生效，有20个像素的距离，间隔原先位置下边20个像素。 盒子2不使用相对布局的话，效果如下： &lt;div class=&quot;box&quot;&gt;盒子1&lt;/div&gt; &lt;div class=&quot;box&quot; style=&quot;position: static; left: 20px; bottom: 20px;&quot;&gt;盒子2&lt;/div&gt; &lt;div class=&quot;box&quot;&gt;盒子3&lt;/div&gt; static布局，top，bottom，left，right属性不生效。3个盒子都在流式布局上。 相对布局另外一个例子： &lt;div class=&quot;box&quot;&gt; &lt;div style=&quot;border: 1px solid; width: 30px; height: 30px; position: relative; left: 20px; top: 20px;&quot;&gt; &lt;/div&gt; &lt;/div&gt; 里面小的盒子使用了相对布局，在原先的位置上进行了便宜(原先位置在左上角跟外面那个盒子相交)。 absolute 绝对布局绝对定位。相对其static定位以外的第一个父元素进行定位。 &lt;div class=&quot;box&quot; style=&quot;width: 200px; height: 200px; background-color: yellow;&quot;&gt; &lt;div class=&quot;box&quot; style=&quot;width: 150px; height: 150px; background-color: green; position: relative; left: 20px; top: 20px;&quot;&gt; &lt;div class=&quot;box&quot; style=&quot;background-color: blue; position: absolute; top: 20px; left: 20px;&quot;&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; 蓝色盒子是绝对布局，默认是在绿色盒子的左上角相交的，但是是绝对布局，在它的非static父元素(绿色盒子)上想左上各便宜20个像素。 绝对布局另外一个例子： &lt;div class=&quot;box&quot; style=&quot;width: 200px; height: 200px; background-color: yellow; position: relative;&quot;&gt; &lt;div class=&quot;box&quot; style=&quot;width: 150px; height: 150px; background-color: green; position: static; margin-left: 20px; margin-top: 20px;&quot;&gt; &lt;div class=&quot;box&quot; style=&quot;background-color: blue; position: absolute; top: 20px; left: 20px;&quot;&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; 蓝色盒子是绝对布局，但是蓝色盒子的父元素绿色盒子是static布局，所以被无视了。但是黄色盒子是非stati布局(相对布局)所以以黄色盒子为标准，左上各便宜20个元素，绿色盒子是static布局，这里使用了margin-top和margin-left属性。所以蓝色盒子相交了。 fixed 固定布局固定布局相对的是浏览器窗口。 比较简单，就不写了。 relative 和 absolute 的区别 &lt;div class=&quot;box&quot; style=&quot;width: 400px; height: 400px; background-color: yellow; position: relative;&quot;&gt; &lt;div class=&quot;box&quot; style=&quot;position: relative; top: 20px; left: 20px;&quot;&gt; &lt;/div&gt; &lt;div class=&quot;box&quot;&gt; &lt;/div&gt; &lt;div class=&quot;box&quot; style=&quot;position: relative; top: 0; left: 5px;&quot;&gt; &lt;/div&gt; &lt;/div&gt; relative布局虽然偏离了它原先的流式布局位置。但是那块原本属于它的地盘别人无法侵占。 &lt;div class=&quot;box&quot; style=&quot;width: 400px; height: 400px; background-color: yellow; position: relative;&quot;&gt; &lt;div class=&quot;box&quot; style=&quot;position: relative; top: 20px; left: 20px;&quot;&gt; &lt;/div&gt; &lt;div class=&quot;box&quot; style=&quot;position: absolute; background-color: red; top: 50px;&quot;&gt; &lt;/div&gt; &lt;div class=&quot;box&quot; style=&quot;position: relative;&quot;&gt; &lt;/div&gt; &lt;/div&gt; absolute布局不会占用它原先的流式布局位置。 总结position属性值总结： static，默认值，流式布局。top, bottom, left, right属性值无效。 relative，相对布局，相对的是原先的流式布局位置。会占用原先的流式布局位置。top, bottom, left, right属性值有效。 absolute，绝对布局，相对的是非static父元素。不会占用原先的流式布局位置top, bottom, left, right属性值有效。 fixed，固定布局，相对的是浏览器窗口。top, bottom, left, right属性值有效。","raw":null,"content":null,"categories":[{"name":"css","slug":"css","permalink":"http://fangjian0423.github.io/categories/css/"}],"tags":[{"name":"css","slug":"css","permalink":"http://fangjian0423.github.io/tags/css/"}]},{"title":"groovy基本概念学习笔记","slug":"groovy_note","date":"2015-02-06T15:43:57.000Z","updated":"2016-05-10T14:59:25.000Z","comments":true,"path":"2015/02/06/groovy_note/","link":"","permalink":"http://fangjian0423.github.io/2015/02/06/groovy_note/","excerpt":"","text":"groovy是一种基于JVM的动态语言，能够与java代码很好地结合，可以使用Java语言编写的库。 这里记录一下groovy的一些学习笔记。 1.groovy可以使用def定义一个变量，使用def其实就是表示这个对象的类型是Object： def num = 1 def str = &quot;format&quot; def today = new Date() 而在java中，由于java是静态语言，因此java定义变量的时候必须指明变量的类型。 2.groovy中 “==” 符号相当于java中的 “equals” 方法， groovy中的 “is” 方法相当于java中的 “==” new Date() == new Date() // true new Date().is(new Date()) // false 3.groovy中可以使用 “[]” 表示数组或链表， “[:]” 表示map。 [1,2,3] // 表示包含一个ArrayList实例 [&quot;1&quot;:&quot;a&quot;, &quot;2&quot;:&quot;b&quot;] // 表示一个map 4.可以使用dump方法查看当前实例的具体信息 [&quot;1&quot;:&quot;a&quot;, &quot;2&quot;:&quot;b&quot;] // &lt;java.util.LinkedHashMap@a0 head=1=a tail=2=b accessOrder=false table=[null, 1=a, 2=b, null] entrySet=[1=a, 2=b] size=2 modCount=2 threshold=3 loadFactor=0.75 keySet=null values=null&gt; 5.groovy会默认import一些包。 java.io.* java.lang.* java.math.BigDecimal java.math.BigInteger java.net.* java.util.* groovy.lang.* groovy.util.* 而java只会默认import java.lang.* 6.java空对象调用方法会报错，因此需要加上逻辑判断一些非null对象，而groovy可以使用?.符号： instance?.method() 而在java中需要这么写： if(instance != null) instance.method() 7.groovy提供了一种叫做GString的概念。 &quot;${name}, nice to meet you!&quot; 上面这段字符串就是个GString，字符串中的${name}会被自动解析成name变量所表示的值。 8.闭包 闭包应该是最大的区别了。 动态语言都有闭包这个概念，而静态语言没有。 java遍历map的时候需要得到map的keySet，然后使用for遍历： Set&lt;String&gt; keySet = map.keySet(); for(String str : keySet) { ... } groovy中的闭包： map.each { key, value -&gt; .... } list也可以使用闭包： list.eachWithIndex { value, index -&gt; ... } 如果闭包内只有1个属性，那么这个属性可以不写，默认为it： [5,6,7].each { println it } 等于： [5,6,7].each { num -&gt; println num } 9.范围range groovy中可以使用1..3这个range，表示[1,2,3]。 1..3也可以写成 1..&lt;4 10.list实用方法 list的findAll方法，find方法，groupBy方法。 找出30岁的人，返回值也是一个List list.each { it.age == 30 // 注意是==。 如果用=的话不会报错，但是有时候会出现莫名其妙的错误，很难找。 } 根据年龄分组，返回值是一个Map，key是age，也就是年龄，value是一个list list.groupBy { it.age } 11.日期实用方法 昨天和明天的获取： def now = new Date() def tomorrow = now + 1 def yesterday = now - 1 根据时分秒设置时间： def now = new Date() now.set([hourOfDay: 1, minute: 30, second: 45]) 12.metaClass的使用 groovy不像javascript，javascript中定义一个对象，然后可以给这个对象的实例动态地插入任何属性和方法。 groovy定义好了一个类，如何给这个类的实例动态地添加属性和方法呢， 那就是使用metaClass。 class Person { String name int age } Person p = new Person() p.hobby = &apos;music&apos; // 会报错，因为Person类没有hobby属性 p.metaClass.hobby = &apos;music&apos; // 不会报错，动态添加属性","raw":null,"content":null,"categories":[{"name":"groovy","slug":"groovy","permalink":"http://fangjian0423.github.io/categories/groovy/"}],"tags":[{"name":"groovy","slug":"groovy","permalink":"http://fangjian0423.github.io/tags/groovy/"},{"name":"jvm","slug":"jvm","permalink":"http://fangjian0423.github.io/tags/jvm/"}]},{"title":"bootstrap学习系列之表格布局","slug":"bootstrap-grid","date":"2015-01-10T16:07:24.000Z","updated":"2016-05-10T14:59:25.000Z","comments":true,"path":"2015/01/11/bootstrap-grid/","link":"","permalink":"http://fangjian0423.github.io/2015/01/11/bootstrap-grid/","excerpt":"","text":"bootstrap框架不用多说了，本文来讲讲bootstrap中的grid相关的知识。 grid，也就是表格，可以用来展示一些表单的数据。 废话不多说，直接进入主题。 grid样式基本知识bootstrap中grid样式有几个基本的知识，我们先来看一下这几个基本的知识。 grid样式有12种比例设置，分别为1-12. 1代表宽度为1/12，2代表宽度为2/12，以此类推。 grid样式可以在多种设备中生效，且针对各个设备会有不同的样式。 小于768像素的， 比如手机。 样式以.col-xs-为前缀 大于等于768像素的，比如平板。 样式以.col-sm为前缀 大于等于992像素的，比如笔记本。 样式以.col-md-为前缀 大于等于1200像素的，比如大屏显示器。 样式以.col-lg-为前缀 grid样式的position都是relative，即相对布局。 具体的一些细节方便的知识请参考bootstrap官网介绍。 例子由于使用markdown编写，无法进行展示，具体的效果读者可以自行尝试，当然也可以查看我的学习笔记。下面的说明一般都使用col-md-这种设备，这都是在我的PC上测试过的。 例子1：这行占3列，每列占宽度的1/3。由于使用的是 md样式 这个样式只针对笔记本这个设备，该例子在笔记本上显示3列正常，但是在手机上看的话这里会有3行，因为手机端无法识别md这个样式。因此如果不区分设备，所有的设备都显示同样的效果，那应该使用col-xs-为前缀的样式。 &lt;div class=&quot;row&quot;&gt; &lt;div class=&quot;col-md-4&quot;&gt;.col-md-4&lt;/div&gt; &lt;div class=&quot;col-md-4&quot;&gt;.col-md-4&lt;/div&gt; &lt;div class=&quot;col-md-4&quot;&gt;.col-md-4&lt;/div&gt; &lt;/div 例子2：这行有2列，笔记本上这2列各占一半，但是在手机上这2列各占1/3。 之前已经分析过，col-md-为前缀的代表笔记本设备，col-xs-为前缀的代表手机设备。 &lt;div class=&quot;row&quot;&gt; &lt;div class=&quot;col-md-6 col-xs-4&quot;&gt;.col-md-6 .col-xs-4&lt;/div&gt; &lt;div class=&quot;col-md-6 col-xs-4&quot;&gt;.col-md-6 .col-xs-4&lt;/div&gt; &lt;/div&gt; 例子3：布局的嵌套, 嵌套内部也是以12为总和进行计算的。 &lt;div class=&quot;row&quot;&gt; &lt;div class=&quot;col-md-6&quot;&gt; .col-md-6 &lt;div class=&quot;row&quot;&gt; &lt;div class=&quot;col-md-6&quot;&gt;.col-md-6&lt;/div&gt; &lt;div class=&quot;col-md-6&quot;&gt;.col-md-6&lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;col-md-6&quot;&gt;.col-md-6&lt;/div&gt; &lt;/div&gt; 例子4：位置的偏移。 如果想在列与列之间有间隔，可以使用col-md-offset这个样式，这个样式也是分1-12种，分别代为1/12, 2/12…的margin-left样式。 有了这个样式，间隔的代码就不需要使用一个空的div隔开了。 &lt;div class=&quot;row&quot;&gt; &lt;div class=&quot;col-md-5&quot;&gt;.col-md-5&lt;/div&gt; &lt;div class=&quot;col-md-5 col-md-offset-2&quot;&gt;.col-md-5 .col-md-offset-2&lt;/div&gt; &lt;/div&gt; col-md-offset-2的样式的margin-left， 还有另外两个类似的样式，分别是 col-md-push-2和col-md-pull-2。 分别代表left和right百分比。 由于col-md-5等样式使用的是相对布局，因此这些right，left样式会生效。","raw":null,"content":null,"categories":[{"name":"bootstrap","slug":"bootstrap","permalink":"http://fangjian0423.github.io/categories/bootstrap/"}],"tags":[{"name":"bootstrap","slug":"bootstrap","permalink":"http://fangjian0423.github.io/tags/bootstrap/"},{"name":"css","slug":"css","permalink":"http://fangjian0423.github.io/tags/css/"}]},{"title":"mybatis-helper介绍","slug":"mybatis_helper","date":"2015-01-05T16:32:24.000Z","updated":"2016-05-10T14:59:25.000Z","comments":true,"path":"2015/01/06/mybatis_helper/","link":"","permalink":"http://fangjian0423.github.io/2015/01/06/mybatis_helper/","excerpt":"","text":"最近没事写了一个mybatis-helper这么个小东西。 主要的功能就是扩展一下mybatis，提供了一个BaseDao，这个dao有query，count，getAll，getById，insert，update，delete这些最基础的方法。 有了这个BaseDao，这样的话继承这个dao就会有这些基础方法。只有一个条件，那就是对应实体的xml配置文件中需要有一个对应实体的resultMap，这个resultMap的id必须为”resultMap”。 然后这个helper还提供了一个分页和排序的接口，dto继承默认的接口实现类即可，在查询的时候使用这个dto作为查询条件将会默认带上分页和排序的功能。 这个helper基本的原理： BaseDao中的所有方法都使用Annotation的方式处理，然后对应的SqlProvider生成sql，其中表名用TABLE表示，其他都根据对应的参数处理。 接下来使用拦截器拦截StatementHandler的时候使用反射得到MappedStatement中的BoundSql，然后根据全局的resultMap处理sql，把TABLE字符串替换成真正的表名。 分页和排序的功能也是在这个拦截器中实现的。 很简单的一个helper，不想做的太复杂，感觉没什么必要。","raw":null,"content":null,"categories":[{"name":"mybatis","slug":"mybatis","permalink":"http://fangjian0423.github.io/categories/mybatis/"}],"tags":[{"name":"mybatis","slug":"mybatis","permalink":"http://fangjian0423.github.io/tags/mybatis/"}]},{"title":"2014总结","slug":"2014_end","date":"2014-12-31T15:43:05.000Z","updated":"2016-05-10T14:59:25.000Z","comments":true,"path":"2014/12/31/2014_end/","link":"","permalink":"http://fangjian0423.github.io/2014/12/31/2014_end/","excerpt":"","text":"2014年的总结2014年，经历了很多事，总结一下今年的经历以及一些收获。 2013年的时候并没有写，那时候刚毕业半年 = =， 好多不懂，毕业后进了一个坑，每天每夜都在加班，真的累，而且都是重复性的工作，说多了都是泪呀 - -|| 。 不过都已经过去了，2014年，对自己做个总结。 工作2014年过年的时候就从毕业后的第一家单位辞职了，真的是个坑，之后面试都不怎么理想，跌跌撞撞进了一家电子商务互联网公司。待了两个星期就跑路了，呵呵，原谅我… 我也不想的，感觉团队真的太差了；一点都没有互联网公司的感觉；配个电脑都是二手的，超级卡。 o(╯□╰)o 接下来面了两家公司，由于简历亮点太少再加上学校太渣，所以才面了两家.. 真的是醉了。一家公司面试过程表现地相当不错，结果居然把我拒了… 当时真的觉得进这家公司基本上没什么问题。 另外一家公司面的一般，但是最后却表示觉得我挺适合的，让我等个几天时间；过了几天之后以”找到更合适的人”为理由把我拒了.. 那天晚上失眠了.. 真的好悲剧。 之后在同学的介绍下进了他在的公司，但是待的不开心.. 我所在的部门不是一个核心部门，都是做一些边边角角的事情，对技术基本没有提升，然后待了4个多月就辞职滚蛋了。 然后进了现在的这家O2O互联网公司，总体感觉挺不错的。在这里应该能稳定下来了。 总体来说，虽然待了3家公司，跳槽频率相当高，但是最终找到了自己满意的公司，并且也待了下来。 技术2014年，对技术做以下总结： 开始写起了博客，算是吸取了今年找工作遇到的各种问题的教训吧，有了博客，我觉得找工作的时候面试官看了博客会对这个人有了大致的了解，而不仅仅通过面试了解。当然，也不仅仅只是这个原因，写博客也算是对知识的一种总结。14年一共写了34篇博客。其实有几篇文章真的写的挺不错的，但是居然只有一篇上了博客园的首页，算是小小的遗憾吧 看了SpringMVC的源码，并写了一个系列博文分析SpringMVC的源码 看了MyBatis的源码，包括xml文件的解析，拦截器以及缓存的实现原理 vim的深入学习，作为一只coder，感觉一定要深入一下vim，不然感觉对不起coder这个群体 github搞了几个自己的repository，但是质量太一般了 →_→ github pages上用hexo搭了个新的博客，这篇文章也是放在这里的 Python的学习，今年学习了一下Python，其实上大学的时候已经学过了，只是好久没用已经忘得差不多了，算是复习了一下吧 →_→ solr的学习，根据公司的一些需求，搞了一些solr的知识，包括分词器的使用，空间搜索的内容，空间搜索的内容算是自己折腾了半天，这东西资料太少，只能看官方的wiki，以及一些书，比如《solr in action》 grails的使用，在公司有一些小功能，我用grails写了一些， 其实想用Python的，但是要装一些lib，leader不想破坏环境 →→， 只能用grails搞了 →→ logstash + elasticsearch + kibana搭建了一个实时分析日志的系统， 其实就是现成的，需要配置这些东西， 自己搭建好了，但是在公司一直没用上 →_→ redis的了解。 公司搭建好了redis的环境，但是貌似一直没用上 →_→ nginx的配置了解了一些，比如负载均衡，反向代理 了解了mysql的执行计划，sql调优以及binlog日志 大致上就这些内容吧， 现在看看， 真的太少了。。 明年要加油，要学习更多的东西 生活今年还是老样子，一直宅，宅成狗。不过今年有个突破，那就是游泳。 7月份的时候开始游泳一直到10月份，基本上1星期都会去一次。 希望2015年更好一点，比如学习一下guitar， 让生活变得更有意义，而不是一味的coding。 2015年计划 今年真的没看书。 有的书看了一点点就没看了，java并发的书看了100多页就没下文了。。 要养成看书的习惯。 技术方面的计划：redisdockergithub贡献开源框架nio并发包MinaNettyPython深入搜索相关的知识看源码的时候多想想作者的思路以及架构方面，不用特别在意细节 继续写博客 做让生活变得更有趣的事，比如guitar 先挖个坑，2015结束的时候再来验证这些计划是否实现 →_→","raw":null,"content":null,"categories":[{"name":"总结","slug":"总结","permalink":"http://fangjian0423.github.io/categories/总结/"}],"tags":[{"name":"总结","slug":"总结","permalink":"http://fangjian0423.github.io/tags/总结/"}]},{"title":"2014-12-21 小记","slug":"2014-12-21","date":"2014-12-21T14:32:35.000Z","updated":"2016-05-10T14:59:25.000Z","comments":true,"path":"2014/12/21/2014-12-21/","link":"","permalink":"http://fangjian0423.github.io/2014/12/21/2014-12-21/","excerpt":"","text":"今天晚上我陪大学好基友玩了几个小时的炉石，玩的有点high，有点累。 到了小区无忧已经4个多月了，从一开始的激情满满，到现在的沉寂，变化的有点快。 感觉自己有点不知道该做什么。 简单的事情不想做，难点的事情又做不了。 有点醉。 下周开始要使用python写自动化测试了，一开始其实挺不愿意写自动化测试的，觉得这是测试的事，一个开发写什么自动化测试。 后来发现其实还是要了解的，以后自己独挡一面的时候这些东西都逃不了。 最近看了python的django和flask。 一直不愿意尝试着去写点什么东西，觉得没什么意义，其实简单的事情好写，但是难点的又不会写。 哎， o(╯□╰)o。 然后也在intellij中加入了vim的插件，自己应该也要深入地学习一下vim，毕竟做了一个coder，我觉得不会vim或者emacs是不及格的程序员。 加油吧，干把爹。","raw":null,"content":null,"categories":[{"name":"杂事","slug":"杂事","permalink":"http://fangjian0423.github.io/categories/杂事/"}],"tags":[{"name":"杂事","slug":"杂事","permalink":"http://fangjian0423.github.io/tags/杂事/"}]},{"title":"MyBatis拦截器原理探究","slug":"mybatis_interceptor","date":"2014-12-15T14:43:05.000Z","updated":"2016-05-10T14:59:25.000Z","comments":true,"path":"2014/12/15/mybatis_interceptor/","link":"","permalink":"http://fangjian0423.github.io/2014/12/15/mybatis_interceptor/","excerpt":"","text":"MyBatis拦截器介绍MyBatis提供了一种插件(plugin)的功能，虽然叫做插件，但其实这是拦截器功能。那么拦截器拦截MyBatis中的哪些内容呢？ 我们进入官网看一看： MyBatis 允许你在已映射语句执行过程中的某一点进行拦截调用。默认情况下，MyBatis 允许使用插件来拦截的方法调用包括： Executor (update, query, flushStatements, commit, rollback, getTransaction, close, isClosed) ParameterHandler (getParameterObject, setParameters) ResultSetHandler (handleResultSets, handleOutputParameters) StatementHandler (prepare, parameterize, batch, update, query) 我们看到了可以拦截Executor接口的部分方法，比如update，query，commit，rollback等方法，还有其他接口的一些方法等。 总体概括为： 拦截执行器的方法 拦截参数的处理 拦截结果集的处理 拦截Sql语法构建的处理 拦截器的使用拦截器介绍及配置首先我们看下MyBatis拦截器的接口定义： public interface Interceptor { Object intercept(Invocation invocation) throws Throwable; Object plugin(Object target); void setProperties(Properties properties); } 比较简单，只有3个方法。 MyBatis默认没有一个拦截器接口的实现类，开发者们可以实现符合自己需求的拦截器。 下面的MyBatis官网的一个拦截器实例： @Intercepts({@Signature( type= Executor.class, method = &quot;update&quot;, args = {MappedStatement.class,Object.class})}) public class ExamplePlugin implements Interceptor { public Object intercept(Invocation invocation) throws Throwable { return invocation.proceed(); } public Object plugin(Object target) { return Plugin.wrap(target, this); } public void setProperties(Properties properties) { } } 全局xml配置： &lt;plugins&gt; &lt;plugin interceptor=&quot;org.format.mybatis.cache.interceptor.ExamplePlugin&quot;&gt;&lt;/plugin&gt; &lt;/plugins&gt; 这个拦截器拦截Executor接口的update方法（其实也就是SqlSession的新增，删除，修改操作），所有执行executor的update方法都会被该拦截器拦截到。 源码分析下面我们分析一下这段代码背后的源码。 首先从源头-&gt;配置文件开始分析： XMLConfigBuilder解析MyBatis全局配置文件的pluginElement私有方法： private void pluginElement(XNode parent) throws Exception { if (parent != null) { for (XNode child : parent.getChildren()) { String interceptor = child.getStringAttribute(&quot;interceptor&quot;); Properties properties = child.getChildrenAsProperties(); Interceptor interceptorInstance = (Interceptor) resolveClass(interceptor).newInstance(); interceptorInstance.setProperties(properties); configuration.addInterceptor(interceptorInstance); } } } 具体的解析代码其实比较简单，就不贴了，主要就是通过反射实例化plugin节点中的interceptor属性表示的类。然后调用全局配置类Configuration的addInterceptor方法。 public void addInterceptor(Interceptor interceptor) { interceptorChain.addInterceptor(interceptor); } 这个interceptorChain是Configuration的内部属性，类型为InterceptorChain，也就是一个拦截器链，我们来看下它的定义： public class InterceptorChain { private final List&lt;Interceptor&gt; interceptors = new ArrayList&lt;Interceptor&gt;(); public Object pluginAll(Object target) { for (Interceptor interceptor : interceptors) { target = interceptor.plugin(target); } return target; } public void addInterceptor(Interceptor interceptor) { interceptors.add(interceptor); } public List&lt;Interceptor&gt; getInterceptors() { return Collections.unmodifiableList(interceptors); } } 现在我们理解了拦截器配置的解析以及拦截器的归属，现在我们回过头看下为何拦截器会拦截这些方法（Executor，ParameterHandler，ResultSetHandler，StatementHandler的部分方法）： public ParameterHandler newParameterHandler(MappedStatement mappedStatement, Object parameterObject, BoundSql boundSql) { ParameterHandler parameterHandler = mappedStatement.getLang().createParameterHandler(mappedStatement, parameterObject, boundSql); parameterHandler = (ParameterHandler) interceptorChain.pluginAll(parameterHandler); return parameterHandler; } public ResultSetHandler newResultSetHandler(Executor executor, MappedStatement mappedStatement, RowBounds rowBounds, ParameterHandler parameterHandler, ResultHandler resultHandler, BoundSql boundSql) { ResultSetHandler resultSetHandler = new DefaultResultSetHandler(executor, mappedStatement, parameterHandler, resultHandler, boundSql, rowBounds); resultSetHandler = (ResultSetHandler) interceptorChain.pluginAll(resultSetHandler); return resultSetHandler; } public StatementHandler newStatementHandler(Executor executor, MappedStatement mappedStatement, Object parameterObject, RowBounds rowBounds, ResultHandler resultHandler, BoundSql boundSql) { StatementHandler statementHandler = new RoutingStatementHandler(executor, mappedStatement, parameterObject, rowBounds, resultHandler, boundSql); statementHandler = (StatementHandler) interceptorChain.pluginAll(statementHandler); return statementHandler; } public Executor newExecutor(Transaction transaction, ExecutorType executorType, boolean autoCommit) { executorType = executorType == null ? defaultExecutorType : executorType; executorType = executorType == null ? ExecutorType.SIMPLE : executorType; Executor executor; if (ExecutorType.BATCH == executorType) { executor = new BatchExecutor(this, transaction); } else if (ExecutorType.REUSE == executorType) { executor = new ReuseExecutor(this, transaction); } else { executor = new SimpleExecutor(this, transaction); } if (cacheEnabled) { executor = new CachingExecutor(executor, autoCommit); } executor = (Executor) interceptorChain.pluginAll(executor); return executor; } 以上4个方法都是Configuration的方法。这些方法在MyBatis的一个操作(新增，删除，修改，查询)中都会被执行到，执行的先后顺序是Executor，ParameterHandler，ResultSetHandler，StatementHandler(其中ParameterHandler和ResultSetHandler的创建是在创建StatementHandler[3个可用的实现类CallableStatementHandler,PreparedStatementHandler,SimpleStatementHandler]的时候，其构造函数调用的[这3个实现类的构造函数其实都调用了父类BaseStatementHandler的构造函数])。 这4个方法实例化了对应的对象之后，都会调用interceptorChain的pluginAll方法，InterceptorChain的pluginAll刚才已经介绍过了，就是遍历所有的拦截器，然后调用各个拦截器的plugin方法。注意：拦截器的plugin方法的返回值会直接被赋值给原先的对象 由于可以拦截StatementHandler，这个接口主要处理sql语法的构建，因此比如分页的功能，可以用拦截器实现，只需要在拦截器的plugin方法中处理StatementHandler接口实现类中的sql即可，可使用反射实现。 MyBatis还提供了@Intercepts和@Signature关于拦截器的注解。官网的例子就是使用了这2个注解，还包括了Plugin类的使用： @Override public Object plugin(Object target) { return Plugin.wrap(target, this); } 下面我们就分析这3个 “新组合” 的源码，首先先看Plugin类的wrap方法： public static Object wrap(Object target, Interceptor interceptor) { Map&lt;Class&lt;?&gt;, Set&lt;Method&gt;&gt; signatureMap = getSignatureMap(interceptor); Class&lt;?&gt; type = target.getClass(); Class&lt;?&gt;[] interfaces = getAllInterfaces(type, signatureMap); if (interfaces.length &gt; 0) { return Proxy.newProxyInstance( type.getClassLoader(), interfaces, new Plugin(target, interceptor, signatureMap)); } return target; } Plugin类实现了InvocationHandler接口，很明显，我们看到这里返回了一个JDK自身提供的动态代理类。我们解剖一下这个方法调用的其他方法： getSignatureMap方法： private static Map&lt;Class&lt;?&gt;, Set&lt;Method&gt;&gt; getSignatureMap(Interceptor interceptor) { Intercepts interceptsAnnotation = interceptor.getClass().getAnnotation(Intercepts.class); if (interceptsAnnotation == null) { // issue #251 throw new PluginException(&quot;No @Intercepts annotation was found in interceptor &quot; + interceptor.getClass().getName()); } Signature[] sigs = interceptsAnnotation.value(); Map&lt;Class&lt;?&gt;, Set&lt;Method&gt;&gt; signatureMap = new HashMap&lt;Class&lt;?&gt;, Set&lt;Method&gt;&gt;(); for (Signature sig : sigs) { Set&lt;Method&gt; methods = signatureMap.get(sig.type()); if (methods == null) { methods = new HashSet&lt;Method&gt;(); signatureMap.put(sig.type(), methods); } try { Method method = sig.type().getMethod(sig.method(), sig.args()); methods.add(method); } catch (NoSuchMethodException e) { throw new PluginException(&quot;Could not find method on &quot; + sig.type() + &quot; named &quot; + sig.method() + &quot;. Cause: &quot; + e, e); } } return signatureMap; } getSignatureMap方法解释：首先会拿到拦截器这个类的@Interceptors注解，然后拿到这个注解的属性@Signature注解集合，然后遍历这个集合，遍历的时候拿出@Signature注解的type属性(Class类型)，然后根据这个type得到带有method属性和args属性的Method。由于@Interceptors注解的@Signature属性是一个属性，所以最终会返回一个以type为key，value为Set&lt;Method&gt;的Map。 @Intercepts({@Signature( type= Executor.class, method = &quot;update&quot;, args = {MappedStatement.class,Object.class})}) 比如这个@Interceptors注解会返回一个key为Executor，value为集合(这个集合只有一个元素，也就是Method实例，这个Method实例就是Executor接口的update方法，且这个方法带有MappedStatement和Object类型的参数)。这个Method实例是根据@Signature的method和args属性得到的。如果args参数跟type类型的method方法对应不上，那么将会抛出异常。 getAllInterfaces方法： private static Class&lt;?&gt;[] getAllInterfaces(Class&lt;?&gt; type, Map&lt;Class&lt;?&gt;, Set&lt;Method&gt;&gt; signatureMap) { Set&lt;Class&lt;?&gt;&gt; interfaces = new HashSet&lt;Class&lt;?&gt;&gt;(); while (type != null) { for (Class&lt;?&gt; c : type.getInterfaces()) { if (signatureMap.containsKey(c)) { interfaces.add(c); } } type = type.getSuperclass(); } return interfaces.toArray(new Class&lt;?&gt;[interfaces.size()]); } getAllInterfaces方法解释：根据目标实例target(这个target就是之前所说的MyBatis拦截器可以拦截的类，Executor,ParameterHandler,ResultSetHandler,StatementHandler)和它的父类们，返回signatureMap中含有target实现的接口数组。 所以Plugin这个类的作用就是根据@Interceptors注解，得到这个注解的属性@Signature数组，然后根据每个@Signature注解的type，method，args属性使用反射找到对应的Method。最终根据调用的target对象实现的接口决定是否返回一个代理对象替代原先的target对象。 比如MyBatis官网的例子，当Configuration调用newExecutor方法的时候，由于Executor接口的update(MappedStatement ms, Object parameter)方法被拦截器被截获。因此最终返回的是一个代理类Plugin，而不是Executor。这样调用方法的时候，如果是个代理类，那么会执行： public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { try { Set&lt;Method&gt; methods = signatureMap.get(method.getDeclaringClass()); if (methods != null &amp;&amp; methods.contains(method)) { return interceptor.intercept(new Invocation(target, method, args)); } return method.invoke(target, args); } catch (Exception e) { throw ExceptionUtil.unwrapThrowable(e); } } 没错，如果找到对应的方法被代理之后，那么会执行Interceptor接口的interceptor方法。 这个Invocation类如下： public class Invocation { private Object target; private Method method; private Object[] args; public Invocation(Object target, Method method, Object[] args) { this.target = target; this.method = method; this.args = args; } public Object getTarget() { return target; } public Method getMethod() { return method; } public Object[] getArgs() { return args; } public Object proceed() throws InvocationTargetException, IllegalAccessException { return method.invoke(target, args); } } 它的proceed方法也就是调用原先方法(不走代理)。 总结MyBatis拦截器接口提供的3个方法中，plugin方法用于某些处理器(Handler)的构建过程。interceptor方法用于处理代理类的执行。setProperties方法用于拦截器属性的设置。 其实MyBatis官网提供的使用@Interceptors和@Signature注解以及Plugin类这样处理拦截器的方法，我们不一定要直接这样使用。我们也可以抛弃这3个类，直接在plugin方法内部根据target实例的类型做相应的操作。 总体来说MyBatis拦截器还是很简单的，拦截器本身不需要太多的知识点，但是学习拦截器需要对MyBatis中的各个接口很熟悉，因为拦截器涉及到了各个接口的知识点。","raw":null,"content":null,"categories":[{"name":"mybatis","slug":"mybatis","permalink":"http://fangjian0423.github.io/categories/mybatis/"}],"tags":[{"name":"mybatis","slug":"mybatis","permalink":"http://fangjian0423.github.io/tags/mybatis/"}]},{"title":"通过源码分析MyBatis的缓存","slug":"mybatis_cache","date":"2014-12-10T06:34:05.000Z","updated":"2016-05-10T14:59:25.000Z","comments":true,"path":"2014/12/10/mybatis_cache/","link":"","permalink":"http://fangjian0423.github.io/2014/12/10/mybatis_cache/","excerpt":"","text":"前方高能！ 本文内容有点多，通过实际测试例子+源码分析的方式解剖MyBatis缓存的概念，对这方面有兴趣的小伙伴请继续看下去~ MyBatis缓存介绍首先看一段wiki上关于MyBatis缓存的介绍： MyBatis支持声明式数据缓存（declarative data caching）。当一条SQL语句被标记为“可缓存”后，首次执行它时从数据库获取的所有数据会被存储在一段高速缓存中，今后执行这条语句时就会从高速缓存中读取结果，而不是再次命中数据库。MyBatis提供了默认下基于Java HashMap的缓存实现，以及用于与OSCache、Ehcache、Hazelcast和Memcached连接的默认连接器。MyBatis还提供API供其他缓存实现使用。 重点的那句话就是：MyBatis执行SQL语句之后，这条语句就是被缓存，以后再执行这条语句的时候，会直接从缓存中拿结果，而不是再次执行SQL 这也就是大家常说的MyBatis一级缓存，一级缓存的作用域scope是SqlSession。 MyBatis同时还提供了一种全局作用域global scope的缓存，这也叫做二级缓存，也称作全局缓存。 一级缓存测试同个session进行两次相同查询： @Test public void test() { SqlSession sqlSession = sqlSessionFactory.openSession(); try { User user = (User)sqlSession.selectOne(&quot;org.format.mybatis.cache.UserMapper.getById&quot;, 1); log.debug(user); User user2 = (User)sqlSession.selectOne(&quot;org.format.mybatis.cache.UserMapper.getById&quot;, 1); log.debug(user2); } finally { sqlSession.close(); } } MyBatis只进行1次数据库查询： ==&gt; Preparing: select * from USERS WHERE ID = ? ==&gt; Parameters: 1(Integer) &lt;== Total: 1 User{id=1, name=&apos;format&apos;, age=23, birthday=Sun Oct 12 23:20:13 CST 2014} User{id=1, name=&apos;format&apos;, age=23, birthday=Sun Oct 12 23:20:13 CST 2014} 同个session进行两次不同的查询： @Test public void test() { SqlSession sqlSession = sqlSessionFactory.openSession(); try { User user = (User)sqlSession.selectOne(&quot;org.format.mybatis.cache.UserMapper.getById&quot;, 1); log.debug(user); User user2 = (User)sqlSession.selectOne(&quot;org.format.mybatis.cache.UserMapper.getById&quot;, 2); log.debug(user2); } finally { sqlSession.close(); } } MyBatis进行两次数据库查询： ==&gt; Preparing: select * from USERS WHERE ID = ? ==&gt; Parameters: 1(Integer) &lt;== Total: 1 User{id=1, name=&apos;format&apos;, age=23, birthday=Sun Oct 12 23:20:13 CST 2014} ==&gt; Preparing: select * from USERS WHERE ID = ? ==&gt; Parameters: 2(Integer) &lt;== Total: 1 User{id=2, name=&apos;FFF&apos;, age=50, birthday=Sat Dec 06 17:12:01 CST 2014} 不同session，进行相同查询： @Test public void test() { SqlSession sqlSession = sqlSessionFactory.openSession(); SqlSession sqlSession2 = sqlSessionFactory.openSession(); try { User user = (User)sqlSession.selectOne(&quot;org.format.mybatis.cache.UserMapper.getById&quot;, 1); log.debug(user); User user2 = (User)sqlSession2.selectOne(&quot;org.format.mybatis.cache.UserMapper.getById&quot;, 1); log.debug(user2); } finally { sqlSession.close(); sqlSession2.close(); } } MyBatis进行了两次数据库查询： ==&gt; Preparing: select * from USERS WHERE ID = ? ==&gt; Parameters: 1(Integer) &lt;== Total: 1 User{id=1, name=&apos;format&apos;, age=23, birthday=Sun Oct 12 23:20:13 CST 2014} ==&gt; Preparing: select * from USERS WHERE ID = ? ==&gt; Parameters: 1(Integer) &lt;== Total: 1 User{id=1, name=&apos;format&apos;, age=23, birthday=Sun Oct 12 23:20:13 CST 2014} 同个session,查询之后更新数据，再次查询相同的语句： @Test public void test() { SqlSession sqlSession = sqlSessionFactory.openSession(); try { User user = (User)sqlSession.selectOne(&quot;org.format.mybatis.cache.UserMapper.getById&quot;, 1); log.debug(user); user.setAge(100); sqlSession.update(&quot;org.format.mybatis.cache.UserMapper.update&quot;, user); User user2 = (User)sqlSession.selectOne(&quot;org.format.mybatis.cache.UserMapper.getById&quot;, 1); log.debug(user2); sqlSession.commit(); } finally { sqlSession.close(); } } 更新操作之后缓存会被清除： ==&gt; Preparing: select * from USERS WHERE ID = ? ==&gt; Parameters: 1(Integer) &lt;== Total: 1 User{id=1, name=&apos;format&apos;, age=23, birthday=Sun Oct 12 23:20:13 CST 2014} ==&gt; Preparing: update USERS SET NAME = ? , AGE = ? , BIRTHDAY = ? where ID = ? ==&gt; Parameters: format(String), 23(Integer), 2014-10-12 23:20:13.0(Timestamp), 1(Integer) &lt;== Updates: 1 ==&gt; Preparing: select * from USERS WHERE ID = ? ==&gt; Parameters: 1(Integer) &lt;== Total: 1 User{id=1, name=&apos;format&apos;, age=23, birthday=Sun Oct 12 23:20:13 CST 2014} 很明显，结果验证了一级缓存的概念，在同个SqlSession中，查询语句相同的sql会被缓存，但是一旦执行新增或更新或删除操作，缓存就会被清除 源码分析在分析MyBatis的一级缓存之前，我们先简单看下MyBatis中几个重要的类和接口： org.apache.ibatis.session.Configuration类：MyBatis全局配置信息类 org.apache.ibatis.session.SqlSessionFactory接口：操作SqlSession的工厂接口，具体的实现类是DefaultSqlSessionFactory org.apache.ibatis.session.SqlSession接口：执行sql，管理事务的接口，具体的实现类是DefaultSqlSession org.apache.ibatis.executor.Executor接口：sql执行器，SqlSession执行sql最终是通过该接口实现的，常用的实现类有SimpleExecutor和CachingExecutor,这些实现类都使用了装饰者设计模式 一级缓存的作用域是SqlSession，那么我们就先看一下SqlSession的select过程： 这是DefaultSqlSession（SqlSession接口实现类，MyBatis默认使用这个类）的selectList源码（我们例子上使用的是selectOne方法，调用selectOne方法最终会执行selectList方法）： public &lt;E&gt; List&lt;E&gt; selectList(String statement, Object parameter, RowBounds rowBounds) { try { MappedStatement ms = configuration.getMappedStatement(statement); List&lt;E&gt; result = executor.query(ms, wrapCollection(parameter), rowBounds, Executor.NO_RESULT_HANDLER); return result; } catch (Exception e) { throw ExceptionFactory.wrapException(&quot;Error querying database. Cause: &quot; + e, e); } finally { ErrorContext.instance().reset(); } } 我们看到SqlSession最终会调用Executor接口的方法。 接下来我们看下DefaultSqlSession中的executor接口属性具体是哪个实现类。 DefaultSqlSession的构造过程（DefaultSqlSessionFactory内部）： private SqlSession openSessionFromDataSource(ExecutorType execType, TransactionIsolationLevel level, boolean autoCommit) { Transaction tx = null; try { final Environment environment = configuration.getEnvironment(); final TransactionFactory transactionFactory = getTransactionFactoryFromEnvironment(environment); tx = transactionFactory.newTransaction(environment.getDataSource(), level, autoCommit); final Executor executor = configuration.newExecutor(tx, execType, autoCommit); return new DefaultSqlSession(configuration, executor); } catch (Exception e) { closeTransaction(tx); // may have fetched a connection so lets call close() throw ExceptionFactory.wrapException(&quot;Error opening session. Cause: &quot; + e, e); } finally { ErrorContext.instance().reset(); } } 我们看到DefaultSqlSessionFactory构造DefaultSqlSession的时候，Executor接口的实现类是由Configuration构造的： public Executor newExecutor(Transaction transaction, ExecutorType executorType, boolean autoCommit) { executorType = executorType == null ? defaultExecutorType : executorType; executorType = executorType == null ? ExecutorType.SIMPLE : executorType; Executor executor; if (ExecutorType.BATCH == executorType) { executor = new BatchExecutor(this, transaction); } else if (ExecutorType.REUSE == executorType) { executor = new ReuseExecutor(this, transaction); } else { executor = new SimpleExecutor(this, transaction); } if (cacheEnabled) { executor = new CachingExecutor(executor, autoCommit); } executor = (Executor) interceptorChain.pluginAll(executor); return executor; } Executor根据ExecutorType的不同而创建，最常用的是SimpleExecutor，本文的例子也是创建这个实现类。 最后我们发现如果cacheEnabled这个属性为true的话，那么executor会被包一层装饰器，这个装饰器是CachingExecutor。其中cacheEnabled这个属性是mybatis总配置文件中settings节点中cacheEnabled子节点的值，默认就是true，也就是说我们在mybatis总配置文件中不配cacheEnabled的话，它也是默认为打开的。 现在，问题就剩下一个了，CachingExecutor执行sql的时候到底做了什么？ 带着这个问题，我们继续走下去（CachingExecutor的query方法）： public &lt;E&gt; List&lt;E&gt; query(MappedStatement ms, Object parameterObject, RowBounds rowBounds, ResultHandler resultHandler, CacheKey key, BoundSql boundSql) throws SQLException { Cache cache = ms.getCache(); if (cache != null) { flushCacheIfRequired(ms); if (ms.isUseCache() &amp;&amp; resultHandler == null) { ensureNoOutParams(ms, parameterObject, boundSql); if (!dirty) { cache.getReadWriteLock().readLock().lock(); try { @SuppressWarnings(&quot;unchecked&quot;) List&lt;E&gt; cachedList = (List&lt;E&gt;) cache.getObject(key); if (cachedList != null) return cachedList; } finally { cache.getReadWriteLock().readLock().unlock(); } } List&lt;E&gt; list = delegate.&lt;E&gt; query(ms, parameterObject, rowBounds, resultHandler, key, boundSql); tcm.putObject(cache, key, list); // issue #578. Query must be not synchronized to prevent deadlocks return list; } } return delegate.&lt;E&gt;query(ms, parameterObject, rowBounds, resultHandler, key, boundSql); } 其中Cache cache = ms.getCache();这句代码中，这个cache实际上就是个二级缓存，由于我们没有开启二级缓存(二级缓存的内容下面会分析)，因此这里执行了最后一句话。这里的delegate也就是SimpleExecutor,SimpleExecutor没有Override父类的query方法，因此最终执行了SimpleExecutor的父类BaseExecutor的query方法。 所以一级缓存最重要的代码就是BaseExecutor的query方法! BaseExecutor的属性localCache是个PerpetualCache类型的实例，PerpetualCache类是实现了MyBatis的Cache缓存接口的实现类之一，内部有个Map类型的属性用来存储缓存数据。 这个localCache的类型在BaseExecutor内部是写死的。 这个localCache就是一级缓存！ 接下来我们看下为何执行新增或更新或删除操作，一级缓存就会被清除这个问题。 首先MyBatis处理新增或删除的时候，最终都是调用update方法，也就是说新增或者删除操作在MyBatis眼里都是一个更新操作。 我们看下DefaultSqlSession的update方法： public int update(String statement, Object parameter) { try { dirty = true; MappedStatement ms = configuration.getMappedStatement(statement); return executor.update(ms, wrapCollection(parameter)); } catch (Exception e) { throw ExceptionFactory.wrapException(&quot;Error updating database. Cause: &quot; + e, e); } finally { ErrorContext.instance().reset(); } } 很明显，这里调用了CachingExecutor的update方法： public int update(MappedStatement ms, Object parameterObject) throws SQLException { flushCacheIfRequired(ms); return delegate.update(ms, parameterObject); } 这里的flushCacheIfRequired方法清除的是二级缓存，我们之后会分析。 CachingExecutor委托给了(之前已经分析过)SimpleExecutor的update方法，SimpleExecutor没有Override父类BaseExecutor的update方法，因此我们看BaseExecutor的update方法： public int update(MappedStatement ms, Object parameter) throws SQLException { ErrorContext.instance().resource(ms.getResource()).activity(&quot;executing an update&quot;).object(ms.getId()); if (closed) throw new ExecutorException(&quot;Executor was closed.&quot;); clearLocalCache(); return doUpdate(ms, parameter); } 我们看到了关键的一句代码： clearLocalCache(); 进去看看： public void clearLocalCache() { if (!closed) { localCache.clear(); localOutputParameterCache.clear(); } } 没错，就是这条，sqlsession没有关闭的话，进行新增、删除、修改操作的话就是清除一级缓存，也就是SqlSession的缓存。 二级缓存二级缓存的作用域是全局，换句话说，二级缓存已经脱离SqlSession的控制了。 在测试二级缓存之前，我先把结论说一下： 二级缓存的作用域是全局的，二级缓存在SqlSession关闭或提交之后才会生效。 在分析MyBatis的二级缓存之前，我们先简单看下MyBatis中一个关于二级缓存的类(其他相关的类和接口之前已经分析过)： org.apache.ibatis.mapping.MappedStatement： MappedStatement类在Mybatis框架中用于表示XML文件中一个sql语句节点，即一个&lt;select /&gt;、&lt;update /&gt;或者&lt;insert /&gt;标签。Mybatis框架在初始化阶段会对XML配置文件进行读取，将其中的sql语句节点对象化为一个个MappedStatement对象。 配置二级缓存跟一级缓存不同，一级缓存不需要配置任何东西，且默认打开。 二级缓存就需要配置一些东西。 本文就说下最简单的配置，在mapper文件上加上这句配置即可： &lt;cache/&gt; 其实二级缓存跟3个配置有关： mybatis全局配置文件中的setting中的cacheEnabled需要为true(默认为true，不设置也行) mapper配置文件中需要加入&lt;cache&gt;节点 mapper配置文件中的select节点需要加上属性useCache需要为true(默认为true，不设置也行) 测试不同SqlSession，查询相同语句，第一次查询之后commit SqlSession： @Test public void testCache2() { SqlSession sqlSession = sqlSessionFactory.openSession(); SqlSession sqlSession2 = sqlSessionFactory.openSession(); try { String sql = &quot;org.format.mybatis.cache.UserMapper.getById&quot;; User user = (User)sqlSession.selectOne(sql, 1); log.debug(user); // 注意，这里一定要提交。 不提交还是会查询两次数据库 sqlSession.commit(); User user2 = (User)sqlSession2.selectOne(sql, 1); log.debug(user2); } finally { sqlSession.close(); sqlSession2.close(); } } MyBatis仅进行了一次数据库查询： ==&gt; Preparing: select * from USERS WHERE ID = ? ==&gt; Parameters: 1(Integer) &lt;== Total: 1 User{id=1, name=&apos;format&apos;, age=23, birthday=Sun Oct 12 23:20:13 CST 2014} User{id=1, name=&apos;format&apos;, age=23, birthday=Sun Oct 12 23:20:13 CST 2014} 不同SqlSession，查询相同语句，第一次查询之后close SqlSession： @Test public void testCache2() { SqlSession sqlSession = sqlSessionFactory.openSession(); SqlSession sqlSession2 = sqlSessionFactory.openSession(); try { String sql = &quot;org.format.mybatis.cache.UserMapper.getById&quot;; User user = (User)sqlSession.selectOne(sql, 1); log.debug(user); sqlSession.close(); User user2 = (User)sqlSession2.selectOne(sql, 1); log.debug(user2); } finally { sqlSession2.close(); } } MyBatis仅进行了一次数据库查询： ==&gt; Preparing: select * from USERS WHERE ID = ? ==&gt; Parameters: 1(Integer) &lt;== Total: 1 User{id=1, name=&apos;format&apos;, age=23, birthday=Sun Oct 12 23:20:13 CST 2014} User{id=1, name=&apos;format&apos;, age=23, birthday=Sun Oct 12 23:20:13 CST 2014} 不同SqlSesson，查询相同语句。 第一次查询之后SqlSession不提交： @Test public void testCache2() { SqlSession sqlSession = sqlSessionFactory.openSession(); SqlSession sqlSession2 = sqlSessionFactory.openSession(); try { String sql = &quot;org.format.mybatis.cache.UserMapper.getById&quot;; User user = (User)sqlSession.selectOne(sql, 1); log.debug(user); User user2 = (User)sqlSession2.selectOne(sql, 1); log.debug(user2); } finally { sqlSession.close(); sqlSession2.close(); } } MyBatis执行了两次数据库查询： ==&gt; Preparing: select * from USERS WHERE ID = ? ==&gt; Parameters: 1(Integer) &lt;== Total: 1 User{id=1, name=&apos;format&apos;, age=23, birthday=Sun Oct 12 23:20:13 CST 2014} ==&gt; Preparing: select * from USERS WHERE ID = ? ==&gt; Parameters: 1(Integer) &lt;== Total: 1 User{id=1, name=&apos;format&apos;, age=23, birthday=Sun Oct 12 23:20:13 CST 2014} 源码分析我们从在mapper文件中加入的&lt;cache/&gt;中开始分析源码，关于MyBatis的SQL解析请参考另外一篇博客Mybatis解析动态sql原理分析。接下来我们看下这个cache的解析： XMLMappedBuilder（解析每个mapper配置文件的解析类，每一个mapper配置都会实例化一个XMLMapperBuilder类）的解析方法： private void configurationElement(XNode context) { try { String namespace = context.getStringAttribute(&quot;namespace&quot;); if (namespace.equals(&quot;&quot;)) { throw new BuilderException(&quot;Mapper&apos;s namespace cannot be empty&quot;); } builderAssistant.setCurrentNamespace(namespace); cacheRefElement(context.evalNode(&quot;cache-ref&quot;)); cacheElement(context.evalNode(&quot;cache&quot;)); parameterMapElement(context.evalNodes(&quot;/mapper/parameterMap&quot;)); resultMapElements(context.evalNodes(&quot;/mapper/resultMap&quot;)); sqlElement(context.evalNodes(&quot;/mapper/sql&quot;)); buildStatementFromContext(context.evalNodes(&quot;select|insert|update|delete&quot;)); } catch (Exception e) { throw new BuilderException(&quot;Error parsing Mapper XML. Cause: &quot; + e, e); } } 我们看到了解析cache的那段代码： private void cacheElement(XNode context) throws Exception { if (context != null) { String type = context.getStringAttribute(&quot;type&quot;, &quot;PERPETUAL&quot;); Class&lt;? extends Cache&gt; typeClass = typeAliasRegistry.resolveAlias(type); String eviction = context.getStringAttribute(&quot;eviction&quot;, &quot;LRU&quot;); Class&lt;? extends Cache&gt; evictionClass = typeAliasRegistry.resolveAlias(eviction); Long flushInterval = context.getLongAttribute(&quot;flushInterval&quot;); Integer size = context.getIntAttribute(&quot;size&quot;); boolean readWrite = !context.getBooleanAttribute(&quot;readOnly&quot;, false); Properties props = context.getChildrenAsProperties(); builderAssistant.useNewCache(typeClass, evictionClass, flushInterval, size, readWrite, props); } } 解析完cache标签之后会使用builderAssistant的userNewCache方法，这里的builderAssistant是一个MapperBuilderAssistant类型的帮助类，每个XMLMappedBuilder构造的时候都会实例化这个属性，MapperBuilderAssistant类内部有个Cache类型的currentCache属性，这个属性也就是mapper配置文件中cache节点所代表的值： public Cache useNewCache(Class&lt;? extends Cache&gt; typeClass, Class&lt;? extends Cache&gt; evictionClass, Long flushInterval, Integer size, boolean readWrite, Properties props) { typeClass = valueOrDefault(typeClass, PerpetualCache.class); evictionClass = valueOrDefault(evictionClass, LruCache.class); Cache cache = new CacheBuilder(currentNamespace) .implementation(typeClass) .addDecorator(evictionClass) .clearInterval(flushInterval) .size(size) .readWrite(readWrite) .properties(props) .build(); configuration.addCache(cache); currentCache = cache; return cache; } ok，现在mapper配置文件中的cache节点被解析到了XMLMapperBuilder实例中的builderAssistant属性中的currentCache值里。 接下来XMLMapperBuilder会解析select节点，解析select节点的时候使用XMLStatementBuilder进行解析(也包括其他insert，update，delete节点)： public void parseStatementNode() { String id = context.getStringAttribute(&quot;id&quot;); String databaseId = context.getStringAttribute(&quot;databaseId&quot;); if (!databaseIdMatchesCurrent(id, databaseId, this.requiredDatabaseId)) return; Integer fetchSize = context.getIntAttribute(&quot;fetchSize&quot;); Integer timeout = context.getIntAttribute(&quot;timeout&quot;); String parameterMap = context.getStringAttribute(&quot;parameterMap&quot;); String parameterType = context.getStringAttribute(&quot;parameterType&quot;); Class&lt;?&gt; parameterTypeClass = resolveClass(parameterType); String resultMap = context.getStringAttribute(&quot;resultMap&quot;); String resultType = context.getStringAttribute(&quot;resultType&quot;); String lang = context.getStringAttribute(&quot;lang&quot;); LanguageDriver langDriver = getLanguageDriver(lang); Class&lt;?&gt; resultTypeClass = resolveClass(resultType); String resultSetType = context.getStringAttribute(&quot;resultSetType&quot;); StatementType statementType = StatementType.valueOf(context.getStringAttribute(&quot;statementType&quot;, StatementType.PREPARED.toString())); ResultSetType resultSetTypeEnum = resolveResultSetType(resultSetType); String nodeName = context.getNode().getNodeName(); SqlCommandType sqlCommandType = SqlCommandType.valueOf(nodeName.toUpperCase(Locale.ENGLISH)); boolean isSelect = sqlCommandType == SqlCommandType.SELECT; boolean flushCache = context.getBooleanAttribute(&quot;flushCache&quot;, !isSelect); boolean useCache = context.getBooleanAttribute(&quot;useCache&quot;, isSelect); boolean resultOrdered = context.getBooleanAttribute(&quot;resultOrdered&quot;, false); // Include Fragments before parsing XMLIncludeTransformer includeParser = new XMLIncludeTransformer(configuration, builderAssistant); includeParser.applyIncludes(context.getNode()); // Parse selectKey after includes and remove them. processSelectKeyNodes(id, parameterTypeClass, langDriver); // Parse the SQL (pre: &lt;selectKey&gt; and &lt;include&gt; were parsed and removed) SqlSource sqlSource = langDriver.createSqlSource(configuration, context, parameterTypeClass); String resultSets = context.getStringAttribute(&quot;resultSets&quot;); String keyProperty = context.getStringAttribute(&quot;keyProperty&quot;); String keyColumn = context.getStringAttribute(&quot;keyColumn&quot;); KeyGenerator keyGenerator; String keyStatementId = id + SelectKeyGenerator.SELECT_KEY_SUFFIX; keyStatementId = builderAssistant.applyCurrentNamespace(keyStatementId, true); if (configuration.hasKeyGenerator(keyStatementId)) { keyGenerator = configuration.getKeyGenerator(keyStatementId); } else { keyGenerator = context.getBooleanAttribute(&quot;useGeneratedKeys&quot;, configuration.isUseGeneratedKeys() &amp;&amp; SqlCommandType.INSERT.equals(sqlCommandType)) ? new Jdbc3KeyGenerator() : new NoKeyGenerator(); } builderAssistant.addMappedStatement(id, sqlSource, statementType, sqlCommandType, fetchSize, timeout, parameterMap, parameterTypeClass, resultMap, resultTypeClass, resultSetTypeEnum, flushCache, useCache, resultOrdered, keyGenerator, keyProperty, keyColumn, databaseId, langDriver, resultSets); } 这段代码前面都是解析一些标签的属性，我们看到了最后一行使用builderAssistant添加MappedStatement，其中builderAssistant属性是构造XMLStatementBuilder的时候通过XMLMappedBuilder传入的，我们继续看builderAssistant的addMappedStatement方法： 进入setStatementCache： private void setStatementCache( boolean isSelect, boolean flushCache, boolean useCache, Cache cache, MappedStatement.Builder statementBuilder) { flushCache = valueOrDefault(flushCache, !isSelect); useCache = valueOrDefault(useCache, isSelect); statementBuilder.flushCacheRequired(flushCache); statementBuilder.useCache(useCache); statementBuilder.cache(cache); } 最终mapper配置文件中的&lt;cache/&gt;被设置到了XMLMapperBuilder的builderAssistant属性中，XMLMapperBuilder中使用XMLStatementBuilder遍历CRUD节点，遍历CRUD节点的时候将这个cache节点设置到这些CRUD节点中，这个cache就是所谓的二级缓存！ 接下来我们回过头来看查询的源码，CachingExecutor的query方法： 进入TransactionalCacheManager的putObject方法： public void putObject(Cache cache, CacheKey key, Object value) { getTransactionalCache(cache).putObject(key, value); } private TransactionalCache getTransactionalCache(Cache cache) { TransactionalCache txCache = transactionalCaches.get(cache); if (txCache == null) { txCache = new TransactionalCache(cache); transactionalCaches.put(cache, txCache); } return txCache; } TransactionalCache的putObject方法： public void putObject(Object key, Object object) { entriesToRemoveOnCommit.remove(key); entriesToAddOnCommit.put(key, new AddEntry(delegate, key, object)); } 我们看到，数据被加入到了entriesToAddOnCommit中，这个entriesToAddOnCommit是什么东西呢，它是TransactionalCache的一个Map属性： private Map&lt;Object, AddEntry&gt; entriesToAddOnCommit; AddEntry是TransactionalCache内部的一个类： private static class AddEntry { private Cache cache; private Object key; private Object value; public AddEntry(Cache cache, Object key, Object value) { this.cache = cache; this.key = key; this.value = value; } public void commit() { cache.putObject(key, value); } } 好了，现在我们发现使用二级缓存之后：查询数据的话，先从二级缓存中拿数据，如果没有的话，去一级缓存中拿，一级缓存也没有的话再查询数据库。有了数据之后在丢到TransactionalCache这个对象的entriesToAddOnCommit属性中。 接下来我们来验证为什么SqlSession commit或close之后，二级缓存才会生效这个问题。 DefaultSqlSession的commit方法： public void commit(boolean force) { try { executor.commit(isCommitOrRollbackRequired(force)); dirty = false; } catch (Exception e) { throw ExceptionFactory.wrapException(&quot;Error committing transaction. Cause: &quot; + e, e); } finally { ErrorContext.instance().reset(); } } CachingExecutor的commit方法： public void commit(boolean required) throws SQLException { delegate.commit(required); tcm.commit(); dirty = false; } tcm.commit即 TransactionalCacheManager的commit方法： public void commit() { for (TransactionalCache txCache : transactionalCaches.values()) { txCache.commit(); } } TransactionalCache的commit方法： public void commit() { delegate.getReadWriteLock().writeLock().lock(); try { if (clearOnCommit) { delegate.clear(); } else { for (RemoveEntry entry : entriesToRemoveOnCommit.values()) { entry.commit(); } } for (AddEntry entry : entriesToAddOnCommit.values()) { entry.commit(); } reset(); } finally { delegate.getReadWriteLock().writeLock().unlock(); } } 发现调用了AddEntry的commit方法： public void commit() { cache.putObject(key, value); } 发现了！ AddEntry的commit方法会把数据丢到cache中，也就是丢到二级缓存中！ 关于为何调用close方法后，二级缓存才会生效，因为close方法内部会调用commit方法。本文就不具体说了。 读者有兴趣的话看一看源码就知道为什么了。 其他Cache接口简介org.apache.ibatis.cache.Cache是MyBatis的缓存接口，想要实现自定义的缓存需要实现这个接口。 MyBatis中关于Cache接口的实现类也使用了装饰者设计模式。 我们看下它的一些实现类： 简单说明： LRU – 最近最少使用的:移除最长时间不被使用的对象。 FIFO – 先进先出:按对象进入缓存的顺序来移除它们。 SOFT – 软引用:移除基于垃圾回收器状态和软引用规则的对象。 WEAK – 弱引用:更积极地移除基于垃圾收集器状态和弱引用规则的对象。 &lt;cache eviction=&quot;FIFO&quot; flushInterval=&quot;60000&quot; size=&quot;512&quot; readOnly=&quot;true&quot;/&gt; 可以通过cache节点的eviction属性设置，也可以设置其他的属性。 cache-ref节点mapper配置文件中还可以加入cache-ref节点，它有个属性namespace。 如果每个mapper文件都是用cache-ref，且namespace都一样，那么就代表着真正意义上的全局缓存。 如果只用了cache节点，那仅代表这个这个mapper内部的查询被缓存了，其他mapper文件的不起作用，这并不是所谓的全局缓存。 总结总体来说，MyBatis的源码看起来还是比较轻松的，本文从实践和源码方面深入分析了MyBatis的缓存原理，希望对读者有帮助。","raw":null,"content":null,"categories":[{"name":"mybatis","slug":"mybatis","permalink":"http://fangjian0423.github.io/categories/mybatis/"}],"tags":[{"name":"mybatis","slug":"mybatis","permalink":"http://fangjian0423.github.io/tags/mybatis/"},{"name":"cache","slug":"cache","permalink":"http://fangjian0423.github.io/tags/cache/"}]},{"title":"ThreadLocal原理及其实际应用","slug":"java_threadlocal","date":"2014-11-22T12:35:21.000Z","updated":"2016-05-10T14:59:25.000Z","comments":true,"path":"2014/11/22/java_threadlocal/","link":"","permalink":"http://fangjian0423.github.io/2014/11/22/java_threadlocal/","excerpt":"","text":"前言java猿在面试中，经常会被问到1个问题：java实现同步有哪几种方式？ 大家一般都会回答使用synchronized， 那么还有其他方式吗？ 答案是肯定的， 另外一种方式也就是本文要说的ThreadLocal。 ThreadLocal介绍ThreadLocal, 看名字也能猜到， “线程本地”, “线程本地变量”。 我们看下官方的一段话： This class provides thread-local variables. These variables differ from their normal counterparts in that each thread that accesses one (via its get or set method) has its own, independently initialized copy of the variable. ThreadLocal instances are typically private static fields in classes that wish to associate state with a thread (e.g., a user ID or Transaction ID). 粗略地翻译一下：ThreadLocal这个类提供线程本地的变量。这些变量与一般正常的变量不同，它们在每个线程中都是独立的。ThreadLocal实例最典型的运用就是在类的私有静态变量中定义，并与线程关联。 什么意思呢？ 下面我们通过1个实例来说明一下： jdk中的SimpleDateFormat类不是一个线程安全的类，在多线程中使用会出现问题，我们会通过线程同步来处理： 使用synchronized private static SimpleDateFormat sdf = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;); public synchronized static String formatDate(Date date) { return sdf.format(date); } 使用ThreadLocal private static final ThreadLocal&lt;SimpleDateFormat&gt; formatter = new ThreadLocal&lt;SimpleDateFormat&gt;(){ @Override protected SimpleDateFormat initialValue() { return new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;); } }; public String formatIt(Date date) { return formatter.get().format(date); } 这两种方式是一样的，只不过一种用了synchronized，另外一种用了ThreadLocal。 synchronized和ThreadLocal的区别使用synchronized的话，表示当前只有1个线程才能访问方法，其他线程都会被阻塞。当访问的线程也阻塞的时候，其他所有访问该方法的线程全部都会阻塞，这个方法相当地 “耗时”。使用ThreadLocal的话，表示每个线程的本地变量中都有SimpleDateFormat这个实例的引用，也就是各个线程之间完全没有关系，也就不存在同步问题了。 综合来说：使用synchronized是一种 “以时间换空间”的概念， 而使用ThreadLocal则是 “以空间换时间”的概念。 ThreadLocal原理分析我们先看下ThreadLocal的类结构： 我们看到ThreadLocal内部有个ThreadLocalMap内部类，ThreadLocalMap内部有个Entry内部类。 先介绍一下ThreadLocalMap和ThreadLocalMap.Entry内部类：ThreadLocalMap其实也就是一个为ThreadLocal服务的自定义的hashmap类。Entry是一个继承WeakReference类的类，也就是ThreadLocalMap这个hash map中的每一项，并且Entry中的key基本上都是ThreadLocal。 再下来我们看下Thread线程类：Thread线程类内部有个ThreadLocal.ThreadLocalMap类型的属性： /* ThreadLocal values pertaining to this thread. This map is maintained * by the ThreadLocal class. */ ThreadLocal.ThreadLocalMap threadLocals = null; 下面重点来看ThreadLocal类的源码： public T get() { // 得到当前线程 Thread t = Thread.currentThread(); // 拿到当前线程的ThreadLocalMap对象 ThreadLocalMap map = getMap(t); if (map != null) { // 找到该ThreadLocal对应的Entry ThreadLocalMap.Entry e = map.getEntry(this); if (e != null) { @SuppressWarnings(&quot;unchecked&quot;) T result = (T)e.value; return result; } } // 当前线程没有ThreadLocalMap对象的话，那么就初始化ThreadLocalMap return setInitialValue(); } private T setInitialValue() { // 初始化ThreadLocalMap，默认返回null，可由子类扩展 T value = initialValue(); Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) // 实例化ThreadLocalMap之后，将初始值丢入到Map中 map.set(this, value); else createMap(t, value); return value; } void createMap(Thread t, T firstValue) { t.threadLocals = new ThreadLocalMap(this, firstValue); } ThreadLocalMap getMap(Thread t) { return t.threadLocals; } public void set(T value) { // set逻辑：找到当前线程的ThreadLocalMap，找到的话，设置对应的值，否则创建ThreadLocalMap Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value); } 注释已经写了，读者有不明白的可以自己看看源码。 ThreadLocal的应用ThreadLocal应用广泛，下面介绍下在SpringMVC中的应用。 RequestContextHolder内部结构RequestContextHolder：该类会暴露与线程绑定的RequestAttributes对象，什么意思呢？ 就是说web请求过来的数据可以跟线程绑定， 用户A，用户B分别请求过来，可以使用RequestContextHolder得到各个请求的数据。 RequestContextHolder数据结构： 具体这两个holder： private static final ThreadLocal&lt;RequestAttributes&gt; requestAttributesHolder = new NamedThreadLocal&lt;RequestAttributes&gt;(&quot;Request attributes&quot;); private static final ThreadLocal&lt;RequestAttributes&gt; inheritableRequestAttributesHolder = new NamedInheritableThreadLocal&lt;RequestAttributes&gt;(&quot;Request context&quot;); 这里的NamedThreadLocal只是1个带name属性的ThreadLocal： public class NamedThreadLocal&lt;T&gt; extends ThreadLocal&lt;T&gt; { private final String name; public NamedThreadLocal(String name) { Assert.hasText(name, &quot;Name must not be empty&quot;); this.name = name; } @Override public String toString() { return this.name; } } 继续看下RequestContextHolder的getRequestAttributes方法，其中接口RequestAttributes是对请求request的封装： public static RequestAttributes getRequestAttributes() { // 直接从ThreadLocalContext拿当前线程的RequestAttributes RequestAttributes attributes = requestAttributesHolder.get(); if (attributes == null) { attributes = inheritableRequestAttributesHolder.get(); } return attributes; } 我们看到，这里直接使用了ThreadLocal的get方法得到了RequestAttributes。当需要得到Request的时候执行： ServletRequestAttributes requestAttributes = (ServletRequestAttributes)RequestContextHolder.getRequestAttributes(); HttpServletRequest request = requestAttributes.getRequest(); RequestContextHolder的初始化以下代码在FrameworkServlet代码中： 总结本文介绍了ThreadLocal的原理以及ThreadLocal在SpringMVC中的应用。个人感觉ThreadLocal应用场景更多是共享一个变量，但是该变量又不是线程安全的，而不是线程同步。比如RequestContextHolder、LocaleContextHolder、SimpleDateFormat等的共享应用。","raw":null,"content":null,"categories":[{"name":"java","slug":"java","permalink":"http://fangjian0423.github.io/categories/java/"}],"tags":[{"name":"java","slug":"java","permalink":"http://fangjian0423.github.io/tags/java/"},{"name":"threadlocal","slug":"threadlocal","permalink":"http://fangjian0423.github.io/tags/threadlocal/"},{"name":"springmvc","slug":"springmvc","permalink":"http://fangjian0423.github.io/tags/springmvc/"}]},{"title":"logstash搭建日志追踪系统","slug":"logstash_build","date":"2014-11-01T14:40:05.000Z","updated":"2016-05-10T14:59:25.000Z","comments":true,"path":"2014/11/01/logstash_build/","link":"","permalink":"http://fangjian0423.github.io/2014/11/01/logstash_build/","excerpt":"","text":"前言开始博客之前，首先看个问题：作为一只程序猿，写的代码的过程需要加入一些日志信息，这些日志信息包括debug调试信息，异常记录日志等。 Java猿一般都是使用log4j，logback等第三方库记录日志。 那么问题来了，挖掘机到底哪家强？…… 扯个淡，那么问题来了，如果我们想看日志信息，怎么办， ssh到服务器上，vim然后查询。每次都这样，是不是很蛋疼 = =； 还有另外一个问题，如果我们想分析、追踪日志，或者找关键字(分词后的关键字)，这样简简单单看日志文件是不可能的。 因此，我们就需要开源力量了！ logstash介绍摘自官网上的一句话：logstash is a tool for managing events and logs. You can use it to collect logs, parse them, and store them for later use (like, for searching)。logstash是一个用来管理事件和日志的工具，它的作用是收集日志，解析日志，存储日志为以后使用。 官网上有tutorials。 本文也就是对tutorials做一个总结。 logstash日志追踪系统搭建过程要搭建logstash日志追踪系统需要以下几个环境： JDK logstash elasticsearch 没有JDK的小伙伴首先先去下载吧。 logstash和elasticsearch都先下载过来吧~。 logstash环境搭建首先先进入logstash的bin目录建立一个logstash.conf配置文件： input { stdin { } } output { stdout { codec =&gt; rubydebug } } 然后执行： ./logstash -f logstash.conf 这时控制台等待输入内容，我们输入hello world，这个时候控制台会打印出： { &quot;message&quot; =&gt; &quot;hello world&quot;, &quot;@version&quot; =&gt; &quot;1&quot;, &quot;@timestamp&quot; =&gt; &quot;2014-11-01T12:38:17.217Z&quot;, &quot;host&quot; =&gt; &quot;format-2.local&quot; } 这个说明我们的logstash本地配置成功了。 配置文件有2个内容组成，input和output，其实还有2个配置：filter和codec。这就是logstash内部的一个叫做事件处理管道(pipeline)的核心概念的3大组成部分：input：生成事件(logstash中的事件是由队列实现的，这个队列由ruby的SizedQueue实现)的数据来源，常见的有file(文件)、syslog(系统日志)、redis(缓存系统)、lumberjack(lumberjack协议)。 filter：修改事件内容，常见的filter有grok(常用，解析文本并结构化地存储下来，用来处理没有结构的文本)、mutate、drop、clone、geoip output：展现结果，常见的有elasticsearch(搜索引擎)、file、graphite、statsd codec：可以作为input或output的一部分，主要用来处理日志过程中产生的消息，常见的codec有json、rubydebug 现在我们回过头看来我们的logstash.conf配置文件，只配置了input和output，其中input由一个stdin组成，这个stdin没有任何参数，output由stdout组成，这个stdout由codec参数，且使用了rebydebug，因此控制台打印出的信息是reby的对象格式。 我们把codec改成json的话，将会打印出以下内容： {&quot;message&quot;:&quot;hello world&quot;,&quot;@version&quot;:&quot;1&quot;,&quot;@timestamp&quot;:&quot;2014-11-01T13:16:38.221Z&quot;,&quot;host&quot;:&quot;format-2.local&quot;} elasticsearch环境搭建elasticsearch的环境搭建比较简单，download elasticsearch之后进入bin目录，执行： ./elasticsearch 之后打开浏览器进入http://localhost:9200/，发现有一串json文本就表示elasticsearch服务器已启。 但是貌似没有发现什么界面，是不是很不友好= =。 elasticsearch支持插件功能，我们使用kibana插件，下载之后修改config.js文件，把elasticsearch对应的地址改成elasticsearch服务器地址，然后把kibana解压出来的所有文件放到$elasticsearchhome/plugins/kibana/_site/目录中。之后打开http://localhost:9200/_plugin/kibana logstash整合elasticsearch配置完logstash和elasticsearch之后，整合一下这两个框架。logstash配置文件(input、output可以配置多个)： input { stdin { } } output { stdout { codec =&gt; json } elasticsearch { host =&gt; localhost } } 然后重新启动logstash，控制台输入hello elasticsearch，刷新kibana页面： logstash日志追踪系统搭建完毕。 logstash的实际应用以log4j为例。 logstash配置： input { stdin { } log4j { mode =&gt; &quot;server&quot; host =&gt; &quot;127.0.0.1&quot; port =&gt; 56789 type =&gt; &quot;log4j&quot; } } output { stdout { codec =&gt; rubydebug } elasticsearch { host =&gt; localhost } } 测试类： public class LogTest { private Logger logger = Logger.getLogger(DebugLogger.class); @Before public void setUp() { PropertyConfigurator.configure(LogTest.class.getClassLoader().getResourceAsStream(&quot;log4j.properties&quot;)); } @Test public void testLog() { logger.debug(&quot;hello logstash, this is a message from log4j&quot;); } @Test public void testException() { logger.error(&quot;error&quot;, new TestException(&quot;sorry, error&quot;)); } } log4j配置： log4j.appender.stdout=org.apache.log4j.ConsoleAppender log4j.appender.stdout.layout=org.apache.log4j.PatternLayout log4j.appender.stdout.layout.ConversionPattern=%d %p %t %c : %m%n log4j.appender.file=org.apache.log4j.RollingFileAppender log4j.appender.file.file=/Users/fangjian/Develop/log_file/test_log.log log4j.appender.file.maxFileSize=1024 log4j.appender.file.layout=org.apache.log4j.PatternLayout log4j.appender.file.layout.ConversionPattern=%d %p %t %c : %m%n # logstash配置 log4j.appender.logstash=org.apache.log4j.net.SocketAppender log4j.appender.logstash.port=56789 log4j.appender.logstash.remoteHost=127.0.0.1 log4j.rootLogger=debug,stdout,file,logstash 2个test方法跑完之后，刷新kibana界面： 总结本文仅仅只是对logstash的搭建做一个总结，包括logstash内部的结构，还有一些配置语言的介绍都没有非常详细的解释，如果读者有兴趣，可以自行查阅相关资料。 参考资料：http://logstash.net/http://blog.yeradis.com/2013/10/logstash-and-apache-log4j-or-how-to.htmlhttp://www.cnblogs.com/buzzlight/p/logstash_elasticsearch_kibana_log.html","raw":null,"content":null,"categories":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://fangjian0423.github.io/categories/elasticsearch/"}],"tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://fangjian0423.github.io/tags/elasticsearch/"},{"name":"logstash","slug":"logstash","permalink":"http://fangjian0423.github.io/tags/logstash/"},{"name":"kibana","slug":"kibana","permalink":"http://fangjian0423.github.io/tags/kibana/"}]},{"title":"Intellij Idea中一个非常牛逼的UML图功能","slug":"intellij_idea_uml","date":"2014-10-31T02:40:05.000Z","updated":"2016-05-10T14:59:25.000Z","comments":true,"path":"2014/10/31/intellij_idea_uml/","link":"","permalink":"http://fangjian0423.github.io/2014/10/31/intellij_idea_uml/","excerpt":"","text":"今天一来公司，旁边leader立马给我炫耀了intellij中一个非常牛逼的功能，先上效果图。 于是立马想到了上篇博客mybatis自己画的uml图： 我靠！。 intellij内部有自动生成uml图的功能，我自己还去画了！ 简直就是弱爆了。！ 于是点开keymap，搜索uml： 这里uml图的生成是由子类找父类的，如果要找出父类的所有子类，需要手动show implementations，然后add要查看的子类。 同时还可以Show Categories -&gt; Field，找出类的所有属性： nice！ 有木有发现逼格瞬间就高了！。以后看源码架构方面的话就方便多了， 噢耶！","raw":null,"content":null,"categories":[],"tags":[{"name":"Intellij","slug":"Intellij","permalink":"http://fangjian0423.github.io/tags/Intellij/"},{"name":"Idea","slug":"Idea","permalink":"http://fangjian0423.github.io/tags/Idea/"},{"name":"UML","slug":"UML","permalink":"http://fangjian0423.github.io/tags/UML/"}]},{"title":"MyBatis动态SQL底层原理分析","slug":"mybatis-dynamic-sql ","date":"2014-09-26T07:49:50.000Z","updated":"2016-05-10T14:59:25.000Z","comments":true,"path":"2014/09/26/mybatis-dynamic-sql /","link":"","permalink":"http://fangjian0423.github.io/2014/09/26/mybatis-dynamic-sql /","excerpt":"","text":"前言废话不多说，直接进入文章。我们在使用mybatis的时候，会在xml中编写sql语句。比如这段动态sql代码： &lt;update id=&quot;update&quot; parameterType=&quot;org.format.dynamicproxy.mybatis.bean.User&quot;&gt; UPDATE users &lt;trim prefix=&quot;SET&quot; prefixOverrides=&quot;,&quot;&gt; &lt;if test=&quot;name != null and name != &apos;&apos;&quot;&gt; name = #{name} &lt;/if&gt; &lt;if test=&quot;age != null and age != &apos;&apos;&quot;&gt; , age = #{age} &lt;/if&gt; &lt;if test=&quot;birthday != null and birthday != &apos;&apos;&quot;&gt; , birthday = #{birthday} &lt;/if&gt; &lt;/trim&gt; where id = ${id} &lt;/update&gt; mybatis底层是如何构造这段sql的？这方面的知识网上资料不多，于是就写了这么一篇文章。下面带着这个疑问，我们一步一步分析。 介绍MyBatis中一些关于动态SQL的接口和类SqlNode接口，简单理解就是xml中的每个标签，比如上述sql的update,trim,if标签： public interface SqlNode { boolean apply(DynamicContext context); } SqlSource Sql源接口，代表从xml文件或注解映射的sql内容，主要就是用于创建BoundSql，有实现类DynamicSqlSource(动态Sql源)，StaticSqlSource(静态Sql源)等： public interface SqlSource { BoundSql getBoundSql(Object parameterObject); } BoundSql类，封装mybatis最终产生sql的类，包括sql语句，参数，参数源数据等参数： XNode，一个Dom API中的Node接口的扩展类。 BaseBuilder接口及其实现类(属性，方法省略了，大家有兴趣的自己看),这些Builder的作用就是用于构造sql: 下面我们简单分析下其中4个Builder： 1 XMLConfigBuilder 解析mybatis中configLocation属性中的全局xml文件，内部会使用XMLMapperBuilder解析各个xml文件。 2 XMLMapperBuilder 遍历mybatis中mapperLocations属性中的xml文件中每个节点的Builder，比如user.xml，内部会使用XMLStatementBuilder处理xml中的每个节点。 3 XMLStatementBuilder 解析xml文件中各个节点，比如select,insert,update,delete节点，内部会使用XMLScriptBuilder处理节点的sql部分，遍历产生的数据会丢到Configuration的mappedStatements中。 4 XMLScriptBuilder 解析xml中各个节点sql部分的Builder。 LanguageDriver接口及其实现类(属性，方法省略了，大家有兴趣的自己看)，该接口主要的作用就是构造sql: 简单分析下XMLLanguageDriver(处理xml中的sql，RawLanguageDriver处理静态sql)： XMLLanguageDriver内部会使用XMLScriptBuilder解析xml中的sql部分。 ok， 大部分比较重要的类我们都已经介绍了，下面源码分析走起。 源码分析走起Spring与Mybatis整合的时候需要配置SqlSessionFactoryBean，该配置会加入数据源和mybatis xml配置文件路径等信息： &lt;bean id=&quot;sqlSessionFactory&quot; class=&quot;org.mybatis.spring.SqlSessionFactoryBean&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;/&gt; &lt;property name=&quot;configLocation&quot; value=&quot;classpath:mybatisConfig.xml&quot;/&gt; &lt;property name=&quot;mapperLocations&quot; value=&quot;classpath*:org/format/dao/*.xml&quot;/&gt; &lt;/bean&gt; 我们就分析这一段配置背后的细节： SqlSessionFactoryBean实现了Spring的InitializingBean接口，InitializingBean接口的afterPropertiesSet方法中会调用buildSqlSessionFactory方法 buildSqlSessionFactory方法内部会使用XMLConfigBuilder解析属性configLocation中配置的路径，还会使用XMLMapperBuilder属性解析mapperLocations属性中的各个xml文件。 部分源码如下： 由于XMLConfigBuilder内部也是使用XMLMapperBuilder，我们就看看XMLMapperBuilder的解析细节。 我们关注一下，增删改查节点的解析。 XMLStatementBuilder的解析： 默认会使用XMLLanguageDriver创建SqlSource（Configuration构造函数中设置）。 XMLLanguageDriver创建SqlSource： XMLScriptBuilder解析sql： 得到SqlSource之后，会放到Configuration中，有了SqlSource，就能拿BoundSql了，BoundSql可以得到最终的sql。 实例分析我以以下xml的解析大概说下parseDynamicTags的解析过程： &lt;update id=&quot;update&quot; parameterType=&quot;org.format.dynamicproxy.mybatis.bean.User&quot;&gt; UPDATE users &lt;trim prefix=&quot;SET&quot; prefixOverrides=&quot;,&quot;&gt; &lt;if test=&quot;name != null and name != &apos;&apos;&quot;&gt; name = #{name} &lt;/if&gt; &lt;if test=&quot;age != null and age != &apos;&apos;&quot;&gt; , age = #{age} &lt;/if&gt; &lt;if test=&quot;birthday != null and birthday != &apos;&apos;&quot;&gt; , birthday = #{birthday} &lt;/if&gt; &lt;/trim&gt; where id = ${id} &lt;/update&gt; 在看这段解析之前，请先了解dom相关的知识，xml dom知识, dom博文 parseDynamicTags方法的返回值是一个List，也就是一个Sql节点集合。SqlNode本文一开始已经介绍，分析完解析过程之后会说一下各个SqlNode类型的作用。 1 首先根据update节点(Node)得到所有的子节点，分别是3个子节点 (1)文本节点 \\n UPDATE users (2)trim子节点 … (3)文本节点 \\n where id = #{id} 2 遍历各个子节点 (1) 如果节点类型是文本或者CDATA，构造一个TextSqlNode或StaticTextSqlNode (2) 如果节点类型是元素，说明该update节点是个动态sql，然后会使用NodeHandler处理各个类型的子节点。这里的NodeHandler是XMLScriptBuilder的一个内部接口，其实现类包括TrimHandler、WhereHandler、SetHandler、IfHandler、ChooseHandler等。看类名也就明白了这个Handler的作用，比如我们分析的trim节点，对应的是TrimHandler；if节点，对应的是IfHandler… 这里子节点trim被TrimHandler处理，TrimHandler内部也使用parseDynamicTags方法解析节点 3 遇到子节点是元素的话，重复以上步骤 trim子节点内部有7个子节点，分别是文本节点、if节点、是文本节点、if节点、是文本节点、if节点、文本节点。文本节点跟之前一样处理，if节点使用IfHandler处理 遍历步骤如上所示，下面我们看下几个Handler的实现细节。 IfHandler处理方法也是使用parseDynamicTags方法，然后加上if标签必要的属性。 private class IfHandler implements NodeHandler { public void handleNode(XNode nodeToHandle, List&lt;SqlNode&gt; targetContents) { List&lt;SqlNode&gt; contents = parseDynamicTags(nodeToHandle); MixedSqlNode mixedSqlNode = new MixedSqlNode(contents); String test = nodeToHandle.getStringAttribute(&quot;test&quot;); IfSqlNode ifSqlNode = new IfSqlNode(mixedSqlNode, test); targetContents.add(ifSqlNode); } } TrimHandler处理方法也是使用parseDynamicTags方法，然后加上trim标签必要的属性。 private class TrimHandler implements NodeHandler { public void handleNode(XNode nodeToHandle, List&lt;SqlNode&gt; targetContents) { List&lt;SqlNode&gt; contents = parseDynamicTags(nodeToHandle); MixedSqlNode mixedSqlNode = new MixedSqlNode(contents); String prefix = nodeToHandle.getStringAttribute(&quot;prefix&quot;); String prefixOverrides = nodeToHandle.getStringAttribute(&quot;prefixOverrides&quot;); String suffix = nodeToHandle.getStringAttribute(&quot;suffix&quot;); String suffixOverrides = nodeToHandle.getStringAttribute(&quot;suffixOverrides&quot;); TrimSqlNode trim = new TrimSqlNode(configuration, mixedSqlNode, prefix, prefixOverrides, suffix, suffixOverrides); targetContents.add(trim); } } 以上update方法最终通过parseDynamicTags方法得到的SqlNode集合如下： trim节点： 由于这个update方法是个动态节点，因此构造出了DynamicSqlSource。 DynamicSqlSource内部就可以构造sql了: DynamicSqlSource内部的SqlNode属性是一个MixedSqlNode。 然后我们看看各个SqlNode实现类的apply方法 下面分析一下两个SqlNode实现类的apply方法实现： MixedSqlNode： public boolean apply(DynamicContext context) { for (SqlNode sqlNode : contents) { sqlNode.apply(context); } return true; } MixedSqlNode会遍历调用内部各个sqlNode的apply方法。 StaticTextSqlNode: public boolean apply(DynamicContext context) { context.appendSql(text); return true; } 直接append sql文本。 IfSqlNode: public boolean apply(DynamicContext context) { if (evaluator.evaluateBoolean(test, context.getBindings())) { contents.apply(context); return true; } return false; } 这里的evaluator是一个ExpressionEvaluator类型的实例，内部使用了OGNL处理表达式逻辑。 TrimSqlNode: public boolean apply(DynamicContext context) { FilteredDynamicContext filteredDynamicContext = new FilteredDynamicContext(context); boolean result = contents.apply(filteredDynamicContext); filteredDynamicContext.applyAll(); return result; } public void applyAll() { sqlBuffer = new StringBuilder(sqlBuffer.toString().trim()); String trimmedUppercaseSql = sqlBuffer.toString().toUpperCase(Locale.ENGLISH); if (trimmedUppercaseSql.length() &gt; 0) { applyPrefix(sqlBuffer, trimmedUppercaseSql); applySuffix(sqlBuffer, trimmedUppercaseSql); } delegate.appendSql(sqlBuffer.toString()); } private void applyPrefix(StringBuilder sql, String trimmedUppercaseSql) { if (!prefixApplied) { prefixApplied = true; if (prefixesToOverride != null) { for (String toRemove : prefixesToOverride) { if (trimmedUppercaseSql.startsWith(toRemove)) { sql.delete(0, toRemove.trim().length()); break; } } } if (prefix != null) { sql.insert(0, &quot; &quot;); sql.insert(0, prefix); } } } TrimSqlNode的apply方法也是调用属性contents(一般都是MixedSqlNode)的apply方法，按照实例也就是7个SqlNode，都是StaticTextSqlNode和IfSqlNode。 最后会使用FilteredDynamicContext过滤掉prefix和suffix。 总结大致讲解了一下mybatis对动态sql语句的解析过程，其实回过头来看看不算复杂，还算蛮简单的。 之前接触mybaits的时候遇到刚才分析的那一段动态sql的时候总是很费解。 &lt;update id=&quot;update&quot; parameterType=&quot;org.format.dynamicproxy.mybatis.bean.User&quot;&gt; UPDATE users &lt;trim prefix=&quot;SET&quot; prefixOverrides=&quot;,&quot;&gt; &lt;if test=&quot;name != null and name != &apos;&apos;&quot;&gt; name = #{name} &lt;/if&gt; &lt;if test=&quot;age != null and age != &apos;&apos;&quot;&gt; , age = #{age} &lt;/if&gt; &lt;if test=&quot;birthday != null and birthday != &apos;&apos;&quot;&gt; , birthday = #{birthday} &lt;/if&gt; &lt;/trim&gt; where id = ${id} &lt;/update&gt; 想搞明白这个trim节点的prefixOverrides到底是什么意思(从字面上理解就是前缀覆盖)，而且官方文档上也没这方面知识的说明。我将这段xml改成如下： &lt;update id=&quot;update&quot; parameterType=&quot;org.format.dynamicproxy.mybatis.bean.User&quot;&gt; UPDATE users &lt;trim prefix=&quot;SET&quot; prefixOverrides=&quot;,&quot;&gt; &lt;if test=&quot;name != null and name != &apos;&apos;&quot;&gt; , name = #{name} &lt;/if&gt; &lt;if test=&quot;age != null and age != &apos;&apos;&quot;&gt; , age = #{age} &lt;/if&gt; &lt;if test=&quot;birthday != null and birthday != &apos;&apos;&quot;&gt; , birthday = #{birthday} &lt;/if&gt; &lt;/trim&gt; where id = ${id} &lt;/update&gt; (第二段第一个if节点多了个逗号) 结果我发现这2段xml解析的结果是一样的，非常迫切地想知道这到底是为什么，然后这也促使了我去看源码的决心。最终还是看下来了。 文章有点长，而且讲的也不是非常直观，希望对有些人有帮助。","raw":null,"content":null,"categories":[{"name":"mybatis","slug":"mybatis","permalink":"http://fangjian0423.github.io/categories/mybatis/"}],"tags":[{"name":"mybatis","slug":"mybatis","permalink":"http://fangjian0423.github.io/tags/mybatis/"},{"name":"java","slug":"java","permalink":"http://fangjian0423.github.io/tags/java/"}]},{"title":"简单谈谈dom解析xml和html","slug":"dom-parse-xml-html","date":"2014-09-21T01:49:50.000Z","updated":"2016-05-10T14:59:25.000Z","comments":true,"path":"2014/09/21/dom-parse-xml-html/","link":"","permalink":"http://fangjian0423.github.io/2014/09/21/dom-parse-xml-html/","excerpt":"","text":"前言文件对象模型（Document Object Model，简称DOM），是W3C组织推荐的处理可扩展标志语言的标准编程接口。html，xml都是基于这个模型构造的。这也是一个W3C推出的标准。java，python，javascript等语言都提供了一套基于dom的编程接口。 java使用dom解析xml一段xml文档, note.xml： &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;note&gt; &lt;to id=&quot;1&quot;&gt;George&lt;/to&gt; &lt;from&gt;John&lt;/from&gt; &lt;heading&gt;Reminder&lt;/heading&gt; &lt;body&gt;Don&apos;t forget the meeting!&lt;/body&gt; &lt;/note&gt; 我们先使用w3c dom解析该xml： @Test public void test() { NodeList nodeList = doc.getChildNodes().item(0).getChildNodes(); System.out.println(&quot;xml size: &quot; + nodeList.getLength()); for(int i = 0; i &lt; nodeList.getLength(); i ++) { Node node = nodeList.item(i); System.out.println(node.getNodeType()); System.out.println(node.getNodeName()); } } 输出： xml size: 9 3 #text 1 to 3 #text 1 from 3 #text 1 heading 3 #text 1 body 3 #text 我们看到代码输出note节点的字节点的时候，有9个节点，但是xml文档中note节点实际上只有to、from、heading、body4个节点。 那为什么是9个呢，原因是这样的。选取几个w3c规范中关于节点类型的描述： 节点类型 描述 nodeName返回值 nodeValue返回值 子元素 类型常量值 Document 表示整个文档（DOM 树的根节点） #document null Element(max. one)，Comment，DocumentType 9 Element 表示 element（元素）元素 element name null Text，Comment，CDATASection 1 Attr 表示属性 属性名称 属性值 Text 2 Text 表示元素或属性中的文本内容。 #text 节点内容 None 3 CDATASection 表示文档中的 CDATA 区段（文本不会被解析器解析） #cdata-section 节点内容 None 4 Comment 表示注释 #comment 注释文本 None 8 更多细节请查看w3c DOM节点类型 下面解释一下文档节点的字节点的处理过程： 其中红色部分为Text节点，紫色部分是Element节点(只画了部分)。&lt;/body&gt;后面的也是一个Element节点，所有4个Element节点，5个Text节点。 所以输出的内容中3 #text表示该节点是个Text节点，1 节点name是个Element节点，这与表格中表述的是一样的。 测试代码： @Test public void test1() { NodeList nodeList = doc.getChildNodes().item(0).getChildNodes(); System.out.println(&quot;xml size: &quot; + nodeList.getLength()); for(int i = 0; i &lt; nodeList.getLength(); i ++) { Node node = nodeList.item(i); if(node.getNodeType() == Node.TEXT_NODE) { System.out.println(node.getNodeValue().replace(&quot;\\n&quot;,&quot;hr&quot;).replace(&apos; &apos;, &apos;-&apos;)); } } } 很明显，我们把空格和回车键替换打印后发现我们的结论是正确的。 测试代码： @Test public void test2() { System.out.println(&quot;doc type: &quot; + doc.getNodeType()); NodeList nodeList = doc.getChildNodes().item(0).getChildNodes(); Node secondNode = nodeList.item(1); System.out.println(&quot;element [to] node type: &quot; + secondNode.getNodeType()); System.out.println(&quot;element [to] node name: &quot; + secondNode.getNodeName()); System.out.println(&quot;element [to] node value: &quot; + secondNode.getNodeValue()); System.out.println(&quot;element [to] children len: &quot; + secondNode.getChildNodes().getLength()); System.out.println(&quot;element [to] children node type: &quot; + secondNode.getChildNodes().item(0).getNodeType()); System.out.println(&quot;element [to] children node value: &quot; + secondNode.getChildNodes().item(0).getNodeValue()); System.out.println(&quot;element [to] children node name: &quot; + secondNode.getChildNodes().item(0).getNodeName()); Node attNode = secondNode.getAttributes().item(0); System.out.println(&quot;attr type: &quot; + attNode.getNodeType()); } 输出结果跟表格中是一样的。 大家有兴趣的话其他类型的节点比如CDATA节点大家可以自行测试～ javascript使用dom解析htmlhtml代码： &lt;!DOCTYPE html&gt; &lt;html&gt; &lt;head&gt; &lt;meta charset=&quot;utf-8&quot;&gt; &lt;title&gt;JS Bin&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;div&gt; &lt;p&gt;gogogo&lt;/p&gt; &lt;/div&gt; &lt;/body&gt; &lt;/html&gt; js代码： console.log(document.nodeType); var div = document.getElementsByTagName(&quot;div&quot;)[0]; //9 console.log(div.nodeType); //1 for(var i = 0;i &lt; div.childNodes.length; i ++) { console.log(div.childNodes[i].nodeType); } 分别输出9，1，3，1，3跟我们在表格中对应～ 总结本次博客主要讲解了dom解析xml和html。 以前使用java解析xml的时候总是使用一些第三方库，比如jdom。 但是dom却是w3c的规范，不止java，包括javascript，python这些主流语言也都主持，有了规范，语言只是实现了这些规范而已。","raw":null,"content":null,"categories":[{"name":"java","slug":"java","permalink":"http://fangjian0423.github.io/categories/java/"}],"tags":[{"name":"javascript","slug":"javascript","permalink":"http://fangjian0423.github.io/tags/javascript/"},{"name":"java","slug":"java","permalink":"http://fangjian0423.github.io/tags/java/"},{"name":"dom","slug":"dom","permalink":"http://fangjian0423.github.io/tags/dom/"}]},{"title":"Spring与Mybatis整合的MapperScannerConfigurer处理过程源码分析","slug":"MapperScannerConfigurer-analysis","date":"2014-09-06T13:55:12.000Z","updated":"2016-05-10T14:59:25.000Z","comments":true,"path":"2014/09/06/MapperScannerConfigurer-analysis/","link":"","permalink":"http://fangjian0423.github.io/2014/09/06/MapperScannerConfigurer-analysis/","excerpt":"","text":"前言本文将分析mybatis与spring整合的MapperScannerConfigurer的底层原理，之前已经分析过java中实现动态，可以使用jdk自带api和cglib第三方库生成动态代理。本文分析的mybatis版本3.2.7，mybatis-spring版本1.2.2。 MapperScannerConfigurer介绍MapperScannerConfigurer是spring和mybatis整合的mybatis-spring jar包中提供的一个类。 想要了解该类的作用，就得先了解MapperFactoryBean。 MapperFactoryBean的出现为了代替手工使用SqlSessionDaoSupport或SqlSessionTemplate编写数据访问对象(DAO)的代码,使用动态代理实现。 比如下面这个官方文档中的配置： &lt;bean id=&quot;userMapper&quot; class=&quot;org.mybatis.spring.mapper.MapperFactoryBean&quot;&gt; &lt;property name=&quot;mapperInterface&quot; value=&quot;org.mybatis.spring.sample.mapper.UserMapper&quot; /&gt; &lt;property name=&quot;sqlSessionFactory&quot; ref=&quot;sqlSessionFactory&quot; /&gt; &lt;/bean&gt; org.mybatis.spring.sample.mapper.UserMapper是一个接口，我们创建一个MapperFactoryBean实例，然后注入这个接口和sqlSessionFactory（mybatis中提供的SqlSessionFactory接口，MapperFactoryBean会使用SqlSessionFactory创建SqlSession）这两个属性。 之后想使用这个UserMapper接口的话，直接通过spring注入这个bean，然后就可以直接使用了，spring内部会创建一个这个接口的动态代理。 当发现要使用多个MapperFactoryBean的时候，一个一个定义肯定非常麻烦，于是mybatis-spring提供了MapperScannerConfigurer这个类，它将会查找类路径下的映射器并自动将它们创建成MapperFactoryBean。 &lt;bean class=&quot;org.mybatis.spring.mapper.MapperScannerConfigurer&quot;&gt; &lt;property name=&quot;basePackage&quot; value=&quot;org.mybatis.spring.sample.mapper&quot; /&gt; &lt;/bean&gt; 这段配置会扫描org.mybatis.spring.sample.mapper下的所有接口，然后创建各自接口的动态代理类。 MapperScannerConfigurer底层代码分析以以下代码为示例进行讲解(部分代码，其他代码及配置省略)： package org.format.dynamicproxy.mybatis.dao; public interface UserDao { public User getById(int id); public int add(User user); public int update(User user); public int delete(User user); public List&lt;User&gt; getAll(); } &lt;bean class=&quot;org.mybatis.spring.mapper.MapperScannerConfigurer&quot;&gt; &lt;property name=&quot;basePackage&quot; value=&quot;org.format.dynamicproxy.mybatis.dao&quot;/&gt; &lt;property name=&quot;sqlSessionFactoryBeanName&quot; value=&quot;sqlSessionFactory&quot;/&gt; &lt;/bean&gt; 我们先通过测试用例debug查看userDao的实现类到底是什么。我们可以看到，userDao是1个MapperProxy类的实例。看下MapperProxy的源码，没错，实现了InvocationHandler，说明使用了jdk自带的动态代理。 public class MapperProxy&lt;T&gt; implements InvocationHandler, Serializable { private static final long serialVersionUID = -6424540398559729838L; private final SqlSession sqlSession; private final Class&lt;T&gt; mapperInterface; private final Map&lt;Method, MapperMethod&gt; methodCache; public MapperProxy(SqlSession sqlSession, Class&lt;T&gt; mapperInterface, Map&lt;Method, MapperMethod&gt; methodCache) { this.sqlSession = sqlSession; this.mapperInterface = mapperInterface; this.methodCache = methodCache; } public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { if (Object.class.equals(method.getDeclaringClass())) { try { return method.invoke(this, args); } catch (Throwable t) { throw ExceptionUtil.unwrapThrowable(t); } } final MapperMethod mapperMethod = cachedMapperMethod(method); return mapperMethod.execute(sqlSession, args); } private MapperMethod cachedMapperMethod(Method method) { MapperMethod mapperMethod = methodCache.get(method); if (mapperMethod == null) { mapperMethod = new MapperMethod(mapperInterface, method, sqlSession.getConfiguration()); methodCache.put(method, mapperMethod); } return mapperMethod; } } 下面开始分析MapperScannerConfigurer的源码MapperScannerConfigurer实现了BeanDefinitionRegistryPostProcessor接口，BeanDefinitionRegistryPostProcessor接口是一个可以修改spring工长中已定义的bean的接口，该接口有个postProcessBeanDefinitionRegistry方法。 然后我们看下ClassPathMapperScanner中的关键是如何扫描对应package下的接口的。 其实MapperScannerConfigurer的作用也就是将对应的接口的类型改造为MapperFactoryBean，而这个MapperFactoryBean的属性mapperInterface是原类型。MapperFactoryBean本文开头已分析过。 所以最终我们还是要分析MapperFactoryBean的实现原理！ MapperFactoryBean继承了SqlSessionDaoSupport类，SqlSessionDaoSupport类继承DaoSupport抽象类，DaoSupport抽象类实现了InitializingBean接口，因此实例个MapperFactoryBean的时候，都会调用InitializingBean接口的afterPropertiesSet方法。 DaoSupport的afterPropertiesSet方法：MapperFactoryBean重写了checkDaoConfig方法：然后通过spring工厂拿对应的bean的时候：这里的SqlSession是SqlSessionTemplate，SqlSessionTemplate的getMapper方法：Configuration的getMapper方法，会使用MapperRegistry的getMapper方法：MapperRegistry的getMapper方法：MapperProxyFactory构造MapperProxy：没错！ MapperProxyFactory就是使用了jdk组带的Proxy完成动态代理。MapperProxy本来一开始已经提到。MapperProxy内部使用了MapperMethod类完成方法的调用： 下面，我们以UserDao的getById方法来debug看看MapperMethod的execute方法是如何走的。 @Test public void testGet() { int id = 1; System.out.println(userDao.getById(id)); } &lt;select id=&quot;getById&quot; parameterType=&quot;int&quot; resultType=&quot;org.format.dynamicproxy.mybatis.bean.User&quot;&gt; SELECT * FROM users WHERE id = #{id} &lt;/select&gt; 示例代码：https://github.com/fangjian0423/dynamic-proxy-mybatis-study 总结来到了新公司，接触了Mybatis，以前接触过～ 但是接触的不深入，突然发现spring与mybatis整合之后可以只写个接口而不实现，spring默认会帮我们实现，然后觉得非常神奇，于是写了篇java动态代码浅析和本文。 参考资料https://mybatis.github.io/spring/zh/mappers.html","raw":null,"content":null,"categories":[{"name":"mybatis","slug":"mybatis","permalink":"http://fangjian0423.github.io/categories/mybatis/"}],"tags":[{"name":"mybatis","slug":"mybatis","permalink":"http://fangjian0423.github.io/tags/mybatis/"},{"name":"spring","slug":"spring","permalink":"http://fangjian0423.github.io/tags/spring/"}]},{"title":"java动态代理浅析","slug":"java-dynamic-proxy","date":"2014-08-16T07:49:50.000Z","updated":"2016-05-10T14:59:25.000Z","comments":true,"path":"2014/08/16/java-dynamic-proxy/","link":"","permalink":"http://fangjian0423.github.io/2014/08/16/java-dynamic-proxy/","excerpt":"","text":"最近在公司看到了mybatis与spring整合中MapperScannerConfigurer的使用，该类通过反向代理自动生成基于接口的动态代理类。 于是想起了java的动态代理，然后就有了这篇文章。 本文使用动态代理模拟处理事务的拦截器。 接口： public interface UserService { public void addUser(); public void removeUser(); public void searchUser(); } 实现类： public class UserServiceImpl implements UserService { public void addUser() { System.out.println(&quot;add user&quot;); } public void removeUser() { System.out.println(&quot;remove user&quot;); } public void searchUser() { System.out.println(&quot;search user&quot;); } } java动态代理的实现有2种方式1.jdk自带的动态代理使用jdk自带的动态代理需要了解InvocationHandler接口和Proxy类，他们都是在java.lang.reflect包下。 InvocationHandler介绍： InvocationHandler是代理实例的调用处理程序实现的接口。 每个代理实例都具有一个关联的InvocationHandler。对代理实例调用方法时，这个方法会调用InvocationHandler的invoke方法。 Proxy介绍： Proxy 提供静态方法用于创建动态代理类和实例。 实例(模拟AOP处理事务)： public class TransactionInterceptor implements InvocationHandler { private Object target; public void setTarget(Object target) { this.target = target; } @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { System.out.println(&quot;start Transaction&quot;); method.invoke(target, args); System.out.println(&quot;end Transaction&quot;); return null; } } 测试代码： public class TestDynamicProxy { @Test public void testJDK() { TransactionInterceptor transactionInterceptor = new TransactionInterceptor(); UserService userService = new UserServiceImpl(); transactionInterceptor.setTarget(userService); UserService userServiceProxy = (UserService) Proxy.newProxyInstance( userService.getClass().getClassLoader(), userService.getClass().getInterfaces(), transactionInterceptor); userServiceProxy.addUser(); } } 测试结果： start Transaction add user end Transaction 很明显，我们通过userServiceProxy这个代理类进行方法调用的时候，会在方法调用前后进行事务的开启和关闭。 2. 第三方库cglibCGLIB是一个功能强大的，高性能、高质量的代码生成库，用于在运行期扩展Java类和实现Java接口。 它与JDK的动态代理的之间最大的区别就是： JDK动态代理是针对接口的，而cglib是针对类来实现代理的，cglib的原理是对指定的目标类生成一个子类，并覆盖其中方法实现增强，但因为采用的是继承，所以不能对final修饰的类进行代理。 实例： public class UserServiceCallBack implements MethodInterceptor { @Override public Object intercept(Object o, Method method, Object[] args, MethodProxy methodProxy) throws Throwable { System.out.println(&quot;start Transaction by cglib&quot;); methodProxy.invokeSuper(o, args); System.out.println(&quot;end Transaction by cglib&quot;); return null; } } 测试代码： public class TestDynamicProxy { @Test public void testCGLIB() { Enhancer enhancer = new Enhancer(); enhancer.setSuperclass(UserServiceImpl.class); enhancer.setCallback(new UserServiceCallBack()); UserServiceImpl proxy = (UserServiceImpl)enhancer.create(); proxy.addUser(); } } 测试结果： start Transaction by cglib add user end Transaction by cglib 结束语简单讲解了JDK和cglib这2个动态代理，之后会再写篇文章讲讲MapperScannerConfigurer的原理实现。","raw":null,"content":null,"categories":[{"name":"java","slug":"java","permalink":"http://fangjian0423.github.io/categories/java/"}],"tags":[{"name":"java","slug":"java","permalink":"http://fangjian0423.github.io/tags/java/"},{"name":"proxy","slug":"proxy","permalink":"http://fangjian0423.github.io/tags/proxy/"},{"name":"cglib","slug":"cglib","permalink":"http://fangjian0423.github.io/tags/cglib/"}]},{"title":"Redis简介-安装-入门","slug":"redis","date":"2014-08-16T01:49:50.000Z","updated":"2016-05-10T14:59:25.000Z","comments":true,"path":"2014/08/16/redis/","link":"","permalink":"http://fangjian0423.github.io/2014/08/16/redis/","excerpt":"","text":"前言我们team马上要用Redis了。 leader要求学习一下这东西。 Redis大名很早以前就听过了，以前在的公司都没有用到。 现在有机会终于接触到了，果断学习起来。 什么是redisRedis是完全开源免费的，遵守BSD协议，先进的key - value持久化产品。它通常被称为数据结构服务器，因为值（value）可以是 字符串(String), 哈希(Map), 列表(list), 集合(sets)和有序集合(sorted sets)等类型。 当然，我们是通过命令行操作这些数据的。 具体的一些关于命令的东西小伙伴们可以去http://try.redis.io/感受一下。 redis的安装Redis在linux下安装比较简单。 略过….. 下面讲下windows下安装Redis。 首先进入redis下载页面 进入之后 下载的zip解压到指定的目录。 /redis/bin/release目录下结构有个压缩包，直接解压。 目录内文件如下： redis-server.exe 表示服务端程序。redis-cli.exe 表示客户端程序。 先启动redis服务器： 这里注意一下，启动服务器的时候需要配置文件，直接在命令行后面加上配置文件的路径即可。 命令行最后 “The server is now ready to accept connections on port 6397” 也说明了服务器启动成功。 接下来启动客户端： ok, 安装成功。 Java操作Redismaven加入redis依赖。 &lt;dependency&gt; &lt;groupId&gt;redis.clients&lt;/groupId&gt; &lt;artifactId&gt;jedis&lt;/artifactId&gt; &lt;version&gt;2.5.1&lt;/version&gt; &lt;/dependency&gt; Java： import org.junit.Before; import org.junit.Test; import redis.clients.jedis.Jedis; import redis.clients.jedis.JedisPool; import redis.clients.jedis.JedisPoolConfig; import java.util.Set; public class RedisTest { private JedisPool pool; private Jedis jedis; @Before public void setUp() { this.pool = new JedisPool(new JedisPoolConfig(), &quot;127.0.0.1&quot;); this.jedis = pool.getResource(); } @Test public void testGetName() { System.out.println(jedis.get(&quot;name&quot;)); } @Test public void testDel() { jedis.set(&quot;age&quot;, &quot;99&quot;); System.out.println(jedis.get(&quot;age&quot;)); jedis.del(&quot;age&quot;); System.out.println(jedis.get(&quot;age&quot;)); } @Test public void testKeys() { Set&lt;String&gt; keys = jedis.keys(&quot;*&quot;); System.out.println(keys); } } 简单地测试了几个方法。 其他方法名跟redis命令基本类似，所以还是得熟悉redis命令。 总结简单地安装了一下redis，然后用Java访问了Redis服务器，并操作了一些数据。 接下来就是熟悉redis的各种命令了。 go go go!~","raw":null,"content":null,"categories":[{"name":"cache","slug":"cache","permalink":"http://fangjian0423.github.io/categories/cache/"}],"tags":[{"name":"cache","slug":"cache","permalink":"http://fangjian0423.github.io/tags/cache/"},{"name":"redis","slug":"redis","permalink":"http://fangjian0423.github.io/tags/redis/"}]},{"title":"Idea使用svn的问题","slug":"Idea_SVN","date":"2014-08-11T15:49:50.000Z","updated":"2016-05-10T14:59:25.000Z","comments":true,"path":"2014/08/11/Idea_SVN/","link":"","permalink":"http://fangjian0423.github.io/2014/08/11/Idea_SVN/","excerpt":"","text":"Idea使用svn出现svn: E204899: Cannot run program “svn” 解决方法安装TortoiseSVN的时候command line client tools(命令行客户端工具)这个选项是默认不选的 然后idea使用svn的时候就是使用svn命令来完成一系列操作的。 没有安装命令行工具，然后就不行啦。 安装的时候这个命令行选项选择安装就行。 安装完成之后idea的Subversion这些设置全部不勾选。 PS：一开始自己也是没有选择安装命令行工具，但是之后安装svn命令行工具SlikSVN，然后在idea中设置使用，但是好像没有起作用～。","raw":null,"content":null,"categories":[],"tags":[{"name":"idea","slug":"idea","permalink":"http://fangjian0423.github.io/tags/idea/"},{"name":"svn","slug":"svn","permalink":"http://fangjian0423.github.io/tags/svn/"}]},{"title":"Backbone小记录","slug":"Backbone","date":"2014-08-06T01:49:50.000Z","updated":"2016-05-10T14:59:25.000Z","comments":true,"path":"2014/08/06/Backbone/","link":"","permalink":"http://fangjian0423.github.io/2014/08/06/Backbone/","excerpt":"","text":"前言这两天看了下Backbone.js的知识，大概了解了这个框架的一些知识。 写篇博客总结一下。 Backbone.js是一个web端javascript的轻量级MVC框架。为什么说是轻量级呢？因为它基于underscore(基于jQuery)和jQuery这2个框架，熟悉jQuery的小伙伴们可以快速入门Backbone.js。 为何使用Backbone使用Backbone.js可以让你像写Java代码一样对js代码进行组织，比如定义类，类的属性、方法，这点非常重要。比如传统的js开发是这样的： &lt;input type=&quot;button&quot; value=&quot;save&quot; onclick=&quot;save()&quot;/&gt; &lt;input type=&quot;button&quot; value=&quot;add&quot; onclick=&quot;add()&quot;/&gt; ... &lt;script type=&quot;text/javascript&quot;&gt; function save() { ... } function add() { ... } &lt;/script&gt; 页面内容一多，js代码会分布在页面的各个角落，维护、开发都很麻烦。 使用Backbone处理： var AppView = Backbone.View.extend({ el: &quot;body&quot;, // 事件 events: { &apos;click input[type=button][value=add]&apos;: &apos;add&apos;, &apos;click input[type=button][value=save]&apos;: &apos;save&apos; }, add: function(event) { ... }, save: function(event) { ... } }); var appView = new AppView; 使用Backbone之后，所有js的代码都会在同一个地方，也就是对js代码进行了组织，我只需要管理这段js代码即可，前面button里的onclick事件就不需要单独写了。 Backbone中的几个重要概念Backbone中有4个重要的概念，分别是Model，Collection，View，Router。 ModelModel是Javascript应用程序的核心，包括基础的数据以及围绕着这些数据的逻辑：数据转换、数据验证、访问控制。自定义的Model可以继承Backbone.Model。1个Person实体： var Person = Backbone.Model.extend({ initialize: function() { // 构造实例的时候会调用这里 }, //默认属性 defaults: { name: &apos;unknow&apos;, age: 0 }, validate: function(attributes) { if(!attributes.age || attributes.age &lt; 0) { return &quot;年龄小于0，出问题了&quot;; } }, customMethod: function() { console.log(&quot;自定义方法&quot;); } }); // 实例化1个Person对象 var person = new Person; console.log(person.get(&apos;name&apos;)); // unknow console.log(person.get(&apos;age&apos;)); // 0 person.set(&apos;name&apos;, &apos;format&apos;); console.log(person.get(&apos;name&apos;)); // format person.set({name: &apos;james&apos;, age: 33}); console.log(person.get(&apos;age&apos;) + &apos;, &apos; + person.get(&apos;age&apos;)); // james, 33 person.customMethod(); // 自定义方法 //设置其他属性 person.set({birth: &apos;1977-12-15&apos;}); console.log(person.get(&apos;birth&apos;)); // 1977-12-15 console.log(person.attributes); //{name: &quot;james&quot;, age: 33, birth: &quot;1977-12-15&quot;} if(!person.set({age: &apos;-1&apos;})) { //set属性默认不验证，这边set age 0，可以设置进去 console.log(&apos;设置age属性出错------11&apos;); //这里不会打印 } if(!person.set({age: &apos;-1&apos;}, {validate: true})) { //set 的时候加上validate: true， 让其验证，这里验证出错返回false console.log(&apos;设置age属性出错------22&apos;); //这里会打印 } 更加详细的资料请参考Model CollectionCollection, 集合， 也就是多个Model，很好理解。 可以绑定’change’, ‘add’, ‘remove’事件到集合上以监听集合的数据状态。 // 这里的代码依赖之前的Person这个Model var Family = Backbone.Collection.extend({ model: Person, initialize: function() { //构造集合会调用这里 } }); var father = new Person({name: &apos;father&apos;, age: 42}); var mother = new Person({name: &apos;mother&apos;, age: 40}); var family = new Family([father, mother]); family.bind(&apos;change&apos;, function(person) { console.log(&apos;family集合有数据改变了: &apos; + person.get(&apos;age&apos;)); }); family.bind(&apos;add&apos;, function(person) { console.log(&apos;family集合有数据进来了: &apos; + person.get(&apos;name&apos;)); }); family.bind(&apos;remove&apos;, function(person) { console.log(&apos;family集合有数据被删掉了: &apos; + person.get(&apos;name&apos;)); }); // 触发add事件 family.add(new Person({name: &apos;son&apos;, age: 0})); // 触发remove事件 family.remove(father); // 触发change事件 mother.set(&apos;age&apos;, 41); // 查询 console.log(family.where({name: &apos;son&apos;})); [son] // 触发add事件 family.add(father); // 比较器，排序使用 family.comparator = &apos;age&apos;; family.sort(); //根据age排序，并从对象里拿出name属性 console.log(family.pluck(&apos;name&apos;)); // [&quot;son&quot;, &quot;mother&quot;, &quot;father&quot;] 更加详细的资料请参考Collection ViewBackbone的View是用来显示你的model中的数据到页面的，同时它也可用来监听DOM上的事件然后做出响应。 html: &lt;ul id=&quot;persons&quot;&gt; &lt;/ul&gt; &lt;input type=&quot;button&quot; value=&quot;add&quot;/&gt; javascript: $(document).ready(function() { var AppView = Backbone.View.extend({ // 注意这里的el是body，不然的话下面的事件绑定不了。下面事件的element是基于el进行查找的 el: &apos;body&apos;, events: { &apos;click input[type=button][value=add]&apos;: &apos;add&apos;, &apos;click #persons li&apos;: &apos;remove&apos; }, add: function(event) { var name = window.prompt(&quot;请输入名字&quot;); this.$(&apos;#persons&apos;).append(&apos;&lt;li&gt;&apos; + name + &apos;&lt;/li&gt;&apos;); }, remove: function(event) { $(event.target).remove(); } }); var appView = new AppView; }); 更加详细的资料请参考View RouterRouter既控制器，用来监控一些#(锚)地址的访问情况。 var AppRouter = Backbone.Router.extend({ routes: { //* 代表全部， : 代表1个 &quot;all/*action&quot;: &quot;actionRoute&quot;, // all/*action 会截获all/后面全部的地址 &quot;posts/:id&quot;: &quot;posts&quot;, // :id 会截获 &quot;trigger&quot;: &quot;trigger&quot; }, actionRoute: function(actions) { console.log(&quot;all/*action生效了，具体的地址: &quot; + actions); }, posts: function(id) { console.log(&quot;posts/:id生效了，id: &quot; + id); }, trigger: function() { //手动触发其他route, 参数trigger表示触发事件，如果为false，则只是url变化，并不会触发事件，replace表示url替换，而不是前进到这个url，意味着启用该参数，浏览器的history不会记录这个变动。 appRouter.navigate(&quot;/posts/&quot; + 404, {trigger: true, replace: true}); } }); var appRouter = new AppRouter; // 使用Router必须要有以下这个代码 Backbone.history.start(); 更加详细的资料请参考Router 总结总结了一下Backbone中4大核心概念的知识，接下来准备写一个使用Backbone的小项目。 参考资料： https://github.com/the5fire/backbonejs-learning-note http://www.cnblogs.com/yexiaochai/archive/2013/07/27/3219402.html","raw":null,"content":null,"categories":[{"name":"javascript","slug":"javascript","permalink":"http://fangjian0423.github.io/categories/javascript/"}],"tags":[{"name":"javascript","slug":"javascript","permalink":"http://fangjian0423.github.io/tags/javascript/"},{"name":"backbone","slug":"backbone","permalink":"http://fangjian0423.github.io/tags/backbone/"}]},{"title":"Hexo框架下Tinny主题中评论框的使用","slug":"Hexo_Tinny_duoshuo","date":"2014-07-30T08:58:18.000Z","updated":"2016-05-10T14:59:25.000Z","comments":true,"path":"2014/07/30/Hexo_Tinny_duoshuo/","link":"","permalink":"http://fangjian0423.github.io/2014/07/30/Hexo_Tinny_duoshuo/","excerpt":"","text":"##Hexo及Tinny介绍 Hexo是一个快速、简单、功能强大的博客框架，基于Node.js。 Tinny主题基于Pacman主题，可以在Hexo中使用这些主题。 ##具体操作 在Tinny中使用多说评论框的通用代码的时候需要改掉 站点ID，标题，文章地址 这3个参数。 具体代码如下，站点ID对应 &lt;%= page.path %&gt;, 标题 &lt;%= page.title %&gt;, 文章地址 &lt;%= page.permalink %&gt;： &lt;div class=&quot;ds-thread&quot; data-thread-key=&quot;&lt;%= page.path %&gt;&quot; data-title=&quot;&lt;%= page.title %&gt;&quot; data-url=&quot;&lt;%= page.permalink %&gt;&quot;&gt;&lt;/div&gt; 这段代码(js代码已省略)放到Tinny主题的 layout/_partial/post/comment.ejs 文件中。 跟JSP好像 0 0. 但是为什么不用$呢，由于没有了解过Hexo，所以就不晓得了。 有时间看看Hexo的一些底层代码。","raw":null,"content":null,"categories":[],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"http://fangjian0423.github.io/tags/Hexo/"},{"name":"Tinny","slug":"Tinny","permalink":"http://fangjian0423.github.io/tags/Tinny/"},{"name":"duoshuo","slug":"duoshuo","permalink":"http://fangjian0423.github.io/tags/duoshuo/"}]}]}